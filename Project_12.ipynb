{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a2511-6ade-446f-88c4-a57173c89c46",
   "metadata": {
    "id": "d92a2511-6ade-446f-88c4-a57173c89c46",
    "outputId": "08f8eb92-61fc-4b2e-ee29-5d8dd2183d29"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TEXT DATASET PREPROCESSING - WITH SMART TEXT CLEANING (NO EMOJI PRINTS)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_splita\n",
    "\n",
    "def process_text_datasets():\n",
    "    \"\"\"Process text datasets with correct column mappings and smart text cleaning\"\"\"\n",
    "\n",
    "    base_dir = r\"datasets\"\n",
    "    processed_dir = os.path.join(base_dir, \"processed_data\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    all_datasets_processed = {}\n",
    "\n",
    "    def smart_text_cleaning(text):\n",
    "        \"\"\"\n",
    "        Robust cleaning that:\n",
    "        - normalizes unicode and apostrophes\n",
    "        - expands contractions (uses contractions lib if installed, otherwise fallback rules)\n",
    "        - removes URLs, emails, hashtags\n",
    "        - normalizes punctuation and whitespace\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # 1) HTML unescape and unicode normalize\n",
    "        text = html.unescape(text)\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "        # 2) Normalize apostrophes to straight single quote\n",
    "        text = text.replace(\"'\", \"'\").replace(\"'\", \"'\").replace(\"`\", \"'\")\n",
    "\n",
    "        # 3) Expand contractions (try contractions lib, otherwise apply regex fallback)\n",
    "        try:\n",
    "            import contractions\n",
    "            # contractions.fix preserves capitalization; we'll lowercase later\n",
    "            text = contractions.fix(text)\n",
    "        except Exception:\n",
    "            # fallback generic rules (covers common contractions)\n",
    "            # special cases first\n",
    "            text = re.sub(r\"\\bwon't\\b\", \"will not\", text, flags=re.IGNORECASE)\n",
    "            text = re.sub(r\"\\bcan't\\b\", \"can not\", text, flags=re.IGNORECASE)\n",
    "            # general patterns\n",
    "            rules = [\n",
    "                (r\"n['']t\\b\", \" not\"),\n",
    "                (r\"['']re\\b\", \" are\"),\n",
    "                (r\"['']ve\\b\", \" have\"),\n",
    "                (r\"['']ll\\b\", \" will\"),\n",
    "                (r\"['']d\\b\", \" would\"),\n",
    "                (r\"['']m\\b\", \" am\"),\n",
    "                (r\"['']s\\b\", \" is\"),\n",
    "            ]\n",
    "            for pat, rep in rules:\n",
    "                text = re.sub(pat, rep, text, flags=re.IGNORECASE)\n",
    "\n",
    "        # 4) Lowercase and strip\n",
    "        text = text.lower().strip()\n",
    "\n",
    "        # 5) Remove URLs and emails\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "\n",
    "        # 6) Remove hashtags (#love -> love)\n",
    "        text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "\n",
    "        # 7) Normalize repeated punctuation\n",
    "        text = re.sub(r\"!{2,}\", \"!\", text)\n",
    "        text = re.sub(r\"\\?{2,}\", \"?\", text)\n",
    "        text = re.sub(r\"\\.{2,}\", \".\", text)\n",
    "\n",
    "        # 8) Remove stray characters but keep basic punctuation . , ! ? ' -\n",
    "        text = re.sub(r\"[^\\w\\s\\.\\,\\!\\?\\-\\'\\\"]+\", \" \", text)\n",
    "\n",
    "        # 9) Collapse whitespace\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def safe_read_csv(csv_file, nrows=None):\n",
    "        \"\"\"Safe CSV reading with multiple encodings\"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(csv_file, encoding='utf-8', on_bad_lines='skip', low_memory=False, nrows=nrows)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return pd.read_csv(csv_file, encoding='utf-8-sig', on_bad_lines='skip', low_memory=False, nrows=nrows)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return pd.read_csv(csv_file, encoding='latin-1', on_bad_lines='skip', low_memory=False, nrows=nrows)\n",
    "                except Exception:\n",
    "                    print(f\"Could not read: {csv_file}\")\n",
    "                    return pd.DataFrame()\n",
    "\n",
    "    def safe_int_convert(value):\n",
    "        try:\n",
    "            return int(float(value))\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    # ---------- dataset processors (kept structure from your file) ----------\n",
    "\n",
    "    def process_hate_speech_curated():\n",
    "        print(\"\\nPROCESSING: hate_speech_curated\")\n",
    "        dataset_path = os.path.join(base_dir, \"hate_speech_curated\")\n",
    "        all_data = []\n",
    "\n",
    "        file1 = os.path.join(dataset_path, \"HateSpeechDataset.csv\")\n",
    "        if os.path.exists(file1):\n",
    "            print(\"  Reading: HateSpeechDataset.csv\")\n",
    "            df = safe_read_csv(file1)\n",
    "            if not df.empty:\n",
    "                print(f\"    Columns: {list(df.columns)}\")\n",
    "                valid_count = 0\n",
    "                for idx, row in df.iterrows():\n",
    "                    try:\n",
    "                        text = str(row.get('Content', \"\")) if pd.notna(row.get('Content', \"\")) else \"\"\n",
    "                        label_val = row.get('Label', 0)\n",
    "\n",
    "                        if text.strip() == \"\" or text.lower() in ['content', 'label', 'content_int']:\n",
    "                            continue\n",
    "\n",
    "                        text = smart_text_cleaning(text)\n",
    "                        if not text.strip():\n",
    "                            continue\n",
    "\n",
    "                        label = safe_int_convert(label_val)\n",
    "\n",
    "                        all_data.append({\n",
    "                            'text': text,\n",
    "                            'label': label,\n",
    "                            'source_dataset': 'hate_speech_curated',\n",
    "                            'source_file': 'HateSpeechDataset.csv'\n",
    "                        })\n",
    "                        valid_count += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                print(f\"    Processed {valid_count} valid samples from HateSpeechDataset.csv\")\n",
    "\n",
    "        file2 = os.path.join(dataset_path, \"HateSpeechDatasetBalanced.csv\")\n",
    "        if os.path.exists(file2):\n",
    "            print(\"  Reading: HateSpeechDatasetBalanced.csv\")\n",
    "            df = safe_read_csv(file2)\n",
    "            if not df.empty:\n",
    "                print(f\"    Columns: {list(df.columns)}\")\n",
    "                valid_count = 0\n",
    "                for idx, row in df.iterrows():\n",
    "                    try:\n",
    "                        text = str(row.get('Content', \"\")) if pd.notna(row.get('Content', \"\")) else \"\"\n",
    "                        label_val = row.get('Label', 0)\n",
    "\n",
    "                        if text.strip() == \"\" or text.lower() in ['content', 'label']:\n",
    "                            continue\n",
    "\n",
    "                        text = smart_text_cleaning(text)\n",
    "                        if not text.strip():\n",
    "                            continue\n",
    "\n",
    "                        label = safe_int_convert(label_val)\n",
    "\n",
    "                        all_data.append({\n",
    "                            'text': text,\n",
    "                            'label': label,\n",
    "                            'source_dataset': 'hate_speech_curated',\n",
    "                            'source_file': 'HateSpeechDatasetBalanced.csv'\n",
    "                        })\n",
    "                        valid_count += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                print(f\"    Processed {valid_count} valid samples from HateSpeechDatasetBalanced.csv\")\n",
    "\n",
    "        if all_data:\n",
    "            result_df = pd.DataFrame(all_data).drop_duplicates(subset=['text'])\n",
    "            output_path = os.path.join(processed_dir, \"hate_speech_curated_cleaned.csv\")\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            all_datasets_processed['hate_speech_curated'] = {\n",
    "                'samples': len(result_df),\n",
    "                'hate_samples': result_df['label'].sum()\n",
    "            }\n",
    "        return all_data\n",
    "\n",
    "    def process_hate_speech_offensive():\n",
    "        print(\"\\nPROCESSING: hate_speech_offensive\")\n",
    "        dataset_path = os.path.join(base_dir, \"hate_speech_and_offensive_language\")\n",
    "        all_data = []\n",
    "\n",
    "        file_path = os.path.join(dataset_path, \"labeled_data.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(\"  Reading: labeled_data.csv\")\n",
    "            df = safe_read_csv(file_path)\n",
    "            if not df.empty:\n",
    "                print(f\"    Columns: {list(df.columns)}\")\n",
    "                valid_count = 0\n",
    "                for idx, row in df.iterrows():\n",
    "                    try:\n",
    "                        text = str(row.get('tweet', \"\")) if pd.notna(row.get('tweet', \"\")) else \"\"\n",
    "                        class_val = row.get('class', 2)\n",
    "\n",
    "                        if text.strip() == \"\" or text.lower() in ['tweet', 'class']:\n",
    "                            continue\n",
    "\n",
    "                        text = smart_text_cleaning(text)\n",
    "                        if not text.strip():\n",
    "                            continue\n",
    "\n",
    "                        label = 1 if safe_int_convert(class_val) in [0, 1] else 0\n",
    "\n",
    "                        all_data.append({\n",
    "                            'text': text,\n",
    "                            'label': label,\n",
    "                            'source_dataset': 'hate_speech_offensive',\n",
    "                            'source_file': 'labeled_data.csv',\n",
    "                            'original_class': safe_int_convert(class_val)\n",
    "                        })\n",
    "                        valid_count += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                print(f\"    Processed {valid_count} samples from labeled_data.csv\")\n",
    "\n",
    "        if all_data:\n",
    "            result_df = pd.DataFrame(all_data).drop_duplicates(subset=['text'])\n",
    "            output_path = os.path.join(processed_dir, \"hate_speech_offensive_cleaned.csv\")\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            all_datasets_processed['hate_speech_offensive'] = {\n",
    "                'samples': len(result_df),\n",
    "                'hate_samples': result_df['label'].sum()\n",
    "            }\n",
    "        return all_data\n",
    "\n",
    "    def process_suspicious_comm():\n",
    "        print(\"\\nPROCESSING: suspicious_comm\")\n",
    "        dataset_path = os.path.join(base_dir, \"suspicious_communication_on_social_platforms\")\n",
    "        all_data = []\n",
    "\n",
    "        file_path = os.path.join(dataset_path, \"Suspicious Communication on Social Platforms.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(\"  Reading: Suspicious Communication on Social Platforms.csv\")\n",
    "            df = safe_read_csv(file_path)\n",
    "            if not df.empty:\n",
    "                print(f\"    Columns: {list(df.columns)}\")\n",
    "                valid_count = 0\n",
    "                for idx, row in df.iterrows():\n",
    "                    try:\n",
    "                        text = str(row.get('comments', \"\")) if pd.notna(row.get('comments', \"\")) else \"\"\n",
    "                        tagging_val = row.get('tagging', 0)\n",
    "\n",
    "                        if text.strip() == \"\" or text.lower() in ['comments', 'tagging']:\n",
    "                            continue\n",
    "\n",
    "                        text = smart_text_cleaning(text)\n",
    "                        if not text.strip():\n",
    "                            continue\n",
    "\n",
    "                        label = safe_int_convert(tagging_val)\n",
    "\n",
    "                        all_data.append({\n",
    "                            'text': text,\n",
    "                            'label': label,\n",
    "                            'source_dataset': 'suspicious_comm',\n",
    "                            'source_file': 'Suspicious Communication on Social Platforms.csv'\n",
    "                        })\n",
    "                        valid_count += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                print(f\"    Processed {valid_count} samples from Suspicious Communication on Social Platforms.csv\")\n",
    "\n",
    "        if all_data:\n",
    "            result_df = pd.DataFrame(all_data).drop_duplicates(subset=['text'])\n",
    "            output_path = os.path.join(processed_dir, \"suspicious_comm_cleaned.csv\")\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            all_datasets_processed['suspicious_comm'] = {\n",
    "                'samples': len(result_df),\n",
    "                'hate_samples': result_df['label'].sum()\n",
    "            }\n",
    "        return all_data\n",
    "\n",
    "    def process_jigsaw_toxic():\n",
    "        print(\"\\nPROCESSING: jigsaw_toxic\")\n",
    "        dataset_path = os.path.join(base_dir, \"jigsaw-toxic-comment-classification-challenge\")\n",
    "        all_data = []\n",
    "\n",
    "        # Process train.csv\n",
    "        train_file = os.path.join(dataset_path, \"train.csv\")\n",
    "        if os.path.exists(train_file):\n",
    "            print(\"  Reading: train.csv\")\n",
    "            df = safe_read_csv(train_file)\n",
    "            if not df.empty:\n",
    "                print(f\"    Columns: {list(df.columns)}\")\n",
    "                valid_count = 0\n",
    "                toxic_count = 0\n",
    "                for idx, row in df.iterrows():\n",
    "                    try:\n",
    "                        text = str(row.get('comment_text', \"\")) if pd.notna(row.get('comment_text', \"\")) else \"\"\n",
    "\n",
    "                        if text.strip() == \"\" or text.lower() in ['comment_text', 'id']:\n",
    "                            continue\n",
    "\n",
    "                        text = smart_text_cleaning(text)\n",
    "                        if not text.strip():\n",
    "                            continue\n",
    "\n",
    "                        toxic_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "                        is_toxic = any(safe_int_convert(row.get(label, 0)) == 1 for label in toxic_labels)\n",
    "\n",
    "                        if is_toxic:\n",
    "                            toxic_count += 1\n",
    "\n",
    "                        all_data.append({\n",
    "                            'text': text,\n",
    "                            'label': 1 if is_toxic else 0,\n",
    "                            'source_dataset': 'jigsaw_toxic',\n",
    "                            'source_file': 'train.csv'\n",
    "                        })\n",
    "                        valid_count += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                print(f\"    Processed {valid_count} samples from train.csv ({toxic_count} toxic)\")\n",
    "\n",
    "        # Process test.csv with test_labels.csv\n",
    "        test_file = os.path.join(dataset_path, \"test.csv\")\n",
    "        test_labels_file = os.path.join(dataset_path, \"test_labels.csv\")\n",
    "\n",
    "        if os.path.exists(test_file) and os.path.exists(test_labels_file):\n",
    "            print(\"  Reading: test.csv with test_labels.csv\")\n",
    "            test_df = safe_read_csv(test_file)\n",
    "            test_labels_df = safe_read_csv(test_labels_file)\n",
    "\n",
    "            if not test_df.empty and not test_labels_df.empty:\n",
    "                merged_df = pd.merge(test_df, test_labels_df, on='id', how='inner')\n",
    "                valid_count = 0\n",
    "                toxic_count = 0\n",
    "                for idx, row in merged_df.iterrows():\n",
    "                    try:\n",
    "                        text = str(row.get('comment_text', \"\")) if pd.notna(row.get('comment_text', \"\")) else \"\"\n",
    "\n",
    "                        if text.strip() == \"\" or text.lower() in ['comment_text', 'id']:\n",
    "                            continue\n",
    "\n",
    "                        if row.get('toxic', 0) == -1:\n",
    "                            continue\n",
    "\n",
    "                        text = smart_text_cleaning(text)\n",
    "                        if not text.strip():\n",
    "                            continue\n",
    "\n",
    "                        toxic_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "                        is_toxic = any(safe_int_convert(row.get(label, 0)) == 1 for label in toxic_labels if row.get(label, 0) != -1)\n",
    "\n",
    "                        if is_toxic:\n",
    "                            toxic_count += 1\n",
    "\n",
    "                        all_data.append({\n",
    "                            'text': text,\n",
    "                            'label': 1 if is_toxic else 0,\n",
    "                            'source_dataset': 'jigsaw_toxic',\n",
    "                            'source_file': 'test.csv'\n",
    "                        })\n",
    "                        valid_count += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                print(f\"    Processed {valid_count} samples from test data ({toxic_count} toxic)\")\n",
    "\n",
    "        if all_data:\n",
    "            result_df = pd.DataFrame(all_data).drop_duplicates(subset=['text'])\n",
    "            output_path = os.path.join(processed_dir, \"jigsaw_toxic_cleaned.csv\")\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            all_datasets_processed['jigsaw_toxic'] = {\n",
    "                'samples': len(result_df),\n",
    "                'hate_samples': result_df['label'].sum()\n",
    "            }\n",
    "        return all_data\n",
    "\n",
    "    # Process all datasets\n",
    "    print(\"PROCESSING TEXT DATASETS WITH SMART CLEANING...\")\n",
    "\n",
    "    process_hate_speech_curated()\n",
    "    process_hate_speech_offensive()\n",
    "    process_suspicious_comm()\n",
    "    process_jigsaw_toxic()\n",
    "\n",
    "    # Create combined dataset\n",
    "    if all_datasets_processed:\n",
    "        print(\"\\nCREATING COMBINED DATASET...\")\n",
    "\n",
    "        all_dfs = []\n",
    "        for dataset_name in all_datasets_processed.keys():\n",
    "            try:\n",
    "                csv_path = os.path.join(processed_dir, f\"{dataset_name}_cleaned.csv\")\n",
    "                if os.path.exists(csv_path):\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    df['dataset_source'] = dataset_name\n",
    "                    all_dfs.append(df)\n",
    "                    print(f\"  Added {dataset_name}: {len(df)} samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {dataset_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if all_dfs:\n",
    "            combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "            combined_df = combined_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "            combined_path = os.path.join(processed_dir, \"ALL_TEXT_DATASETS_COMBINED.csv\")\n",
    "            combined_df.to_csv(combined_path, index=False)\n",
    "\n",
    "            # Create train/val/test splits\n",
    "            train_df, temp_df = train_test_split(\n",
    "                combined_df, test_size=0.3, stratify=combined_df['label'], random_state=42\n",
    "            )\n",
    "            val_df, test_df = train_test_split(\n",
    "                temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n",
    "            )\n",
    "\n",
    "            # Create split directories\n",
    "            splits_dir = os.path.join(processed_dir, \"splits\")\n",
    "            os.makedirs(os.path.join(splits_dir, \"train\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(splits_dir, \"val\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(splits_dir, \"test\"), exist_ok=True)\n",
    "\n",
    "            # Save splits\n",
    "            train_df.to_csv(os.path.join(splits_dir, \"train\", \"text_train.csv\"), index=False)\n",
    "            val_df.to_csv(os.path.join(splits_dir, \"val\", \"text_val.csv\"), index=False)\n",
    "            test_df.to_csv(os.path.join(splits_dir, \"test\", \"text_test.csv\"), index=False)\n",
    "\n",
    "            print(\"TEXT PROCESSING COMPLETED.\")\n",
    "    else:\n",
    "        print(\"No datasets were successfully processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_text_datasets()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9838a2-dd26-4b44-acf4-ce1bfa5811ca",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "\"\"\"\n",
    "INTERPRETATION OF DATASET SEPARABILITY\n",
    "\n",
    "The processed text datasets contain multiple sources of hate, toxic, offensive,\n",
    "and suspicious communication. After cleaning and normalization, certain\n",
    "datasets exhibit clearer separability between classes (hate vs. non-hate)\n",
    "than others.\n",
    "\n",
    "In particular:\n",
    "- Some datasets contain highly explicit toxic or hateful language, making\n",
    "  class boundaries sharper and easier for models to learn.\n",
    "- Other datasets include more subtle or context-dependent expressions,\n",
    "  producing greater overlap between the \"hate\" and \"non-hate\" categories.\n",
    "\n",
    "Overall, the combined dataset reflects a mix of clearly separable samples\n",
    "and samples with strong class ambiguity. This variety closely resembles\n",
    "real-world social-media text, where explicit and subtle forms of harmful\n",
    "communication coexist.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb4ec3-b926-4f66-ada7-0ee9af6bd563",
   "metadata": {
    "id": "35df0368-93aa-4d61-ba61-9552ccef6d6a",
    "outputId": "63472997-a9e9-4850-dee7-e6af6f14722f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MEMOTION DATASET PREPROCESSING - WITH FIXED REPORT AND DOCUMENTATION\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def process_memotion_dataset():\n",
    "    \"\"\"Process Memotion 7K dataset with corrected label mappings and proper reporting\"\"\"\n",
    "\n",
    "    base_dir = r\"datasets\"\n",
    "    memotion_dir = os.path.join(base_dir, \"memotion_dataset_7k\")\n",
    "    processed_dir = os.path.join(base_dir, \"processed_data\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    print(\"PROCESSING MEMOTION 7K DATASET...\")\n",
    "\n",
    "    if not os.path.exists(memotion_dir):\n",
    "        print(f\"ERROR: Memotion dataset not found: {memotion_dir}\")\n",
    "        return\n",
    "\n",
    "    def smart_text_cleaning(text):\n",
    "        \"\"\"Smart cleaning that preserves important hate speech signals\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Basic cleaning\n",
    "        text = text.lower().strip()\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Remove URLs and emails (usually noise)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "        # Normalize excessive punctuation (keep at least one)\n",
    "        text = re.sub(r'!+', '!', text)  # \"!!!!\" → \"!\"\n",
    "        text = re.sub(r'\\?+', '?', text) # \"????\" → \"?\"\n",
    "        text = re.sub(r'\\.+', '.', text) # \"......\" → \".\"\n",
    "\n",
    "        # Remove extra whitespace again\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def load_labels():\n",
    "        \"\"\"Load label data from CSV or Excel\"\"\"\n",
    "        labels_file = os.path.join(memotion_dir, \"labels.csv\")\n",
    "        if os.path.exists(labels_file):\n",
    "            print(f\"Loading labels from: labels.csv\")\n",
    "            df = pd.read_csv(labels_file)\n",
    "        else:\n",
    "            labels_file = os.path.join(memotion_dir, \"labels.xlsx\")\n",
    "            if os.path.exists(labels_file):\n",
    "                print(f\"Loading labels from: labels.xlsx\")\n",
    "                df = pd.read_excel(labels_file)\n",
    "            else:\n",
    "                print(\"ERROR: No labels file found\")\n",
    "                return None\n",
    "\n",
    "        print(f\"   Loaded {len(df)} rows\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "\n",
    "    def find_image_files():\n",
    "        \"\"\"Find all image files\"\"\"\n",
    "        images_dir = os.path.join(memotion_dir, \"images\")\n",
    "        if not os.path.exists(images_dir):\n",
    "            print(f\"ERROR: Images directory not found: {images_dir}\")\n",
    "            return {}\n",
    "\n",
    "        image_files = {}\n",
    "        supported_formats = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif']\n",
    "\n",
    "        for format in supported_formats:\n",
    "            pattern = os.path.join(images_dir, '**', format)\n",
    "            for img_path in glob.glob(pattern, recursive=True):\n",
    "                filename = os.path.basename(img_path)\n",
    "                image_files[filename] = img_path\n",
    "\n",
    "        print(f\"Found {len(image_files)} image files\")\n",
    "        return image_files\n",
    "\n",
    "    def get_offensive_label(offensive_str):\n",
    "        \"\"\"\n",
    "        Convert offensive string to binary label\n",
    "\n",
    "        Memotion offensive categories:\n",
    "        - 'not_offensive' → 0 (Not Hate) - No offensive content\n",
    "        - 'slight' → 0 (Not Hate) - Mild/borderline offensive, not severe enough for hate speech\n",
    "        - 'offensive' → 1 (Hate) - Clearly offensive content\n",
    "        - 'very_offensive' → 1 (Hate) - Highly offensive content\n",
    "        - 'hateful_offensive' → 1 (Hate) - Hate speech specifically targeting groups\n",
    "        \"\"\"\n",
    "        if not isinstance(offensive_str, str):\n",
    "            return 0\n",
    "\n",
    "        offensive_str = offensive_str.lower().strip()\n",
    "\n",
    "        # Hate speech categories\n",
    "        if offensive_str in ['offensive', 'very_offensive', 'hateful_offensive']:\n",
    "            return 1\n",
    "        # Not hate categories\n",
    "        elif offensive_str in ['not_offensive', 'slight']:\n",
    "            return 0\n",
    "        # Default to not hate for unknown values\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Main processing\n",
    "    labels_df = load_labels()\n",
    "    if labels_df is None:\n",
    "        return\n",
    "\n",
    "    image_files = find_image_files()\n",
    "\n",
    "    all_data = []\n",
    "    processed_count = 0\n",
    "    image_found_count = 0\n",
    "    hate_count = 0\n",
    "\n",
    "    print(f\"PROCESSING {len(labels_df)} SAMPLES...\")\n",
    "\n",
    "    for idx, row in labels_df.iterrows():\n",
    "        try:\n",
    "            # Use text_corrected if available, otherwise text_ocr\n",
    "            text = \"\"\n",
    "            if pd.notna(row['text_corrected']):\n",
    "                text = str(row['text_corrected'])\n",
    "            elif pd.notna(row['text_ocr']):\n",
    "                text = str(row['text_ocr'])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Skip header rows or invalid data\n",
    "            if text.lower() in ['text_corrected', 'text_ocr', 'image_name'] or text == '':\n",
    "                continue\n",
    "\n",
    "            # Clean text using smart cleaning\n",
    "            text = smart_text_cleaning(text)\n",
    "            if not text or len(text) < 5:\n",
    "                continue\n",
    "\n",
    "            # Get offensive label (main hate speech indicator)\n",
    "            offensive_str = row['offensive'] if pd.notna(row['offensive']) else \"not_offensive\"\n",
    "            label = get_offensive_label(offensive_str)\n",
    "\n",
    "            if label == 1:\n",
    "                hate_count += 1\n",
    "\n",
    "            # Find corresponding image\n",
    "            image_name = str(row['image_name']) if pd.notna(row['image_name']) else \"\"\n",
    "            image_path = None\n",
    "\n",
    "            if image_name and image_name in image_files:\n",
    "                image_path = image_files[image_name]\n",
    "                # Validate image\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        img.verify()\n",
    "                    image_found_count += 1\n",
    "                except:\n",
    "                    image_path = None\n",
    "                    print(f\"    WARNING: Corrupted image: {image_name}\")\n",
    "\n",
    "            all_data.append({\n",
    "                'text': text,\n",
    "                'label': label,\n",
    "                'image_path': image_path if image_path else \"\",\n",
    "                'image_name': image_name,\n",
    "                'source_dataset': 'memotion_7k',\n",
    "                'offensive_category': offensive_str,\n",
    "                'humour': row['humour'] if pd.notna(row['humour']) else \"\",\n",
    "                'sarcasm': row['sarcasm'] if pd.notna(row['sarcasm']) else \"\",\n",
    "                'motivational': row['motivational'] if pd.notna(row['motivational']) else \"\",\n",
    "                'overall_sentiment': row['overall_sentiment'] if pd.notna(row['overall_sentiment']) else \"\",\n",
    "                'has_image': 1 if image_path else 0\n",
    "            })\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            if processed_count % 1000 == 0:\n",
    "                print(f\"  Processed {processed_count}/{len(labels_df)} samples... ({hate_count} hate so far)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Create final dataset\n",
    "    if all_data:\n",
    "        memotion_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Remove duplicates based on text and image\n",
    "        initial_count = len(memotion_df)\n",
    "        memotion_df = memotion_df.drop_duplicates(subset=['text', 'image_name'])\n",
    "        final_count = len(memotion_df)\n",
    "\n",
    "        # Save the dataset\n",
    "        output_path = os.path.join(processed_dir, \"memotion_7k_multimodal.csv\")\n",
    "        memotion_df.to_csv(output_path, index=False)\n",
    "\n",
    "        # Calculate statistics\n",
    "        total_samples = len(memotion_df)\n",
    "        total_hate = memotion_df['label'].sum()\n",
    "        images_available = memotion_df['has_image'].sum()\n",
    "\n",
    "        # Analyze offensive category distribution\n",
    "        offensive_dist = memotion_df['offensive_category'].value_counts()\n",
    "\n",
    "        print(f\"\"\"\n",
    "MEMOTION PROCESSING COMPLETED!\n",
    "=================================\n",
    "\n",
    "DATASET SUMMARY:\n",
    "- Total samples: {total_samples:,}\n",
    "- Hate speech samples: {total_hate:,}\n",
    "- Normal samples: {total_samples - total_hate:,}\n",
    "- Hate ratio: {(total_hate/total_samples)*100:.1f}%\n",
    "- Images available: {images_available:,} ({images_available/total_samples*100:.1f}%)\n",
    "\n",
    "OFFENSIVE CATEGORY DISTRIBUTION:\"\"\")\n",
    "\n",
    "        for category, count in offensive_dist.items():\n",
    "            percentage = (count / total_samples) * 100\n",
    "            hate_label = \"HATE\" if get_offensive_label(category) == 1 else \"NOT HATE\"\n",
    "            print(f\"  - {category}: {count:,} ({percentage:.1f}%) [{hate_label}]\")\n",
    "\n",
    "        print(f\"\"\"\n",
    "OUTPUT:\n",
    "- Dataset: {output_path}\n",
    "\n",
    "FILES PROCESSED:\n",
    "- Labels: {len(labels_df)} rows\n",
    "- Images found: {image_found_count} files\n",
    "- Valid multimodal pairs: {total_samples}\n",
    "- Duplicates removed: {initial_count - final_count:,}\n",
    "\"\"\")\n",
    "\n",
    "        # Save detailed analysis report\n",
    "        report_path = os.path.join(processed_dir, \"memotion_analysis_report.txt\")\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"\"\"Memotion 7K Dataset Analysis Report\n",
    "=====================================\n",
    "\n",
    "Dataset Location: {memotion_dir}\n",
    "Processing Date: {pd.Timestamp.now()}\n",
    "\n",
    "FINAL STATISTICS:\n",
    "- Total samples processed: {total_samples}\n",
    "- Hate speech samples: {total_hate}\n",
    "- Normal samples: {total_samples - total_hate}\n",
    "- Hate speech ratio: {(total_hate/total_samples)*100:.1f}%\n",
    "- Images available: {images_available} ({images_available/total_samples*100:.1f}%)\n",
    "\n",
    "OFFENSIVE CATEGORY BREAKDOWN:\n",
    "\"\"\")\n",
    "            for category, count in offensive_dist.items():\n",
    "                percentage = (count / total_samples) * 100\n",
    "                hate_label = \"HATE\" if get_offensive_label(category) == 1 else \"NOT HATE\"\n",
    "                f.write(f\"- {category}: {count} samples ({percentage:.1f}%) [{hate_label}]\\n\")\n",
    "\n",
    "            f.write(f\"\"\"\n",
    "PROCESSING DETAILS:\n",
    "- Original label rows: {len(labels_df)}\n",
    "- Image files found: {len(image_files)}\n",
    "- Successful pairs: {processed_count}\n",
    "- Images successfully matched: {image_found_count}\n",
    "- Duplicates removed: {initial_count - final_count}\n",
    "\n",
    "LABELING LOGIC EXPLANATION:\n",
    "- HATE (1): 'offensive', 'very_offensive', 'hateful_offensive'\n",
    "  * Clear hate speech, offensive content targeting groups\n",
    "\n",
    "- NOT HATE (0): 'not_offensive', 'slight'\n",
    "  * 'not_offensive': No offensive content\n",
    "  * 'slight': Mild/borderline offensive (insults, subtle prejudice) but not severe hate speech\n",
    "\n",
    "CATEGORY DEFINITIONS:\n",
    "- not_offensive: No offensive content whatsoever\n",
    "- slight: Mild offensive content, insults, borderline comments\n",
    "- offensive: Clearly offensive content, profanity, strong insults\n",
    "- very_offensive: Highly offensive content, severe language\n",
    "- hateful_offensive: Hate speech specifically targeting racial, religious, or other groups\n",
    "\n",
    "ADDITIONAL FEATURES:\n",
    "- humour: Categorical humor classification\n",
    "- sarcasm: Categorical sarcasm classification\n",
    "- motivational: Motivational content flag\n",
    "- overall_sentiment: Overall sentiment classification\n",
    "\n",
    "OUTPUT FILES:\n",
    "- Main dataset: {output_path}\n",
    "- This report: {report_path}\n",
    "\"\"\")\n",
    "\n",
    "        print(f\"Detailed analysis saved to: {report_path}\")\n",
    "        return memotion_df\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: No valid samples were processed!\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_memotion_dataset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda6cdf-e4dc-42a7-ac69-f12053abb848",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "MEMOTION DATASET PREPROCESSING - WITH FIXED REPORT AND DOCUMENTATION\n",
    "\n",
    "DATA PROCESSING PIPELINE:\n",
    "- Loads and validates Memotion 7K dataset with corrected label mappings\n",
    "- Implements smart text cleaning that preserves hate speech signals\n",
    "- Converts offensive categories to unified binary labels\n",
    "- Matches text with corresponding images and validates image integrity\n",
    "- Removes duplicates and generates comprehensive analysis reports\n",
    "\n",
    "SMART TEXT CLEANING:\n",
    "- Normalizes text while preserving important linguistic cues\n",
    "- Removes URLs, emails, and excessive punctuation\n",
    "- Maintains emotional intensity indicators relevant to hate speech detection\n",
    "\n",
    "LABEL CONVERSION LOGIC:\n",
    "- HATE (1): 'offensive', 'very_offensive', 'hateful_offensive'\n",
    "- NOT HATE (0): 'not_offensive', 'slight' (mild/borderline content)\n",
    "\n",
    "OVERALL INTERPRETATION:\n",
    "This preprocessing system transforms the multimodal Memotion dataset into a clean, \n",
    "standardized format optimized for hate speech detection. It maintains the crucial \n",
    "relationship between text and images while ensuring data quality through validation, \n",
    "deduplication, and comprehensive reporting. The resulting corpus enables reliable \n",
    "multimodal model training with balanced class representation and traceable metadata.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b813ba9-c51a-4575-bd05-8bafdd4226ee",
   "metadata": {
    "id": "0b813ba9-c51a-4575-bd05-8bafdd4226ee"
   },
   "outputs": [],
   "source": [
    "# train_novel_multimodal_t4_lora.py\n",
    "# CLIP vision + consistency + hard-mining + LoRA (text default, vision optional)\n",
    "# Direct 3-class labeling (Abusive, Offensive, Non-abusive) at dataset load time\n",
    "# Adds: text/image projection to shared space, multimodal gate, image-only aux loss, text-modality dropout\n",
    "\n",
    "# Optional installs:\n",
    "# !pip install -U torch torchvision transformers scikit-learn pandas numpy pillow tqdm seaborn matplotlib peft\n",
    "\n",
    "import os, gc, json, math, random, warnings, sys, subprocess\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ============== Config ==============\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # Paths\n",
    "    base_dir: str = \"datasets\"\n",
    "    processed_dir_name: str = \"processed_data\"\n",
    "    memotion_csv: str = \"memotion_7k_multimodal.csv\"\n",
    "    splits_dirname: str = \"splits\"\n",
    "\n",
    "    # Text CSVs\n",
    "    text_train_csv: str = \"train/text_train.csv\"\n",
    "    text_val_csv: str = \"val/text_val.csv\"\n",
    "    text_test_csv: str = \"test/text_test.csv\"\n",
    "\n",
    "    # Random subsampling for val/test (tracking during training)\n",
    "    seed: int = 42\n",
    "    sample_val_text_n: Optional[int] = 7_000\n",
    "    sample_test_text_n: Optional[int] = 5_000\n",
    "    sample_val_memotion_n: Optional[int] = None\n",
    "    sample_test_memotion_n: Optional[int] = None\n",
    "\n",
    "    # Model (text + vision)\n",
    "    text_model_name: str = \"nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large\"\n",
    "    use_clip_vision: bool = True\n",
    "    clip_vision_model: str = \"openai/clip-vit-base-patch32\"\n",
    "    image_backbone_fallback: str = \"mobilenet_v2\"\n",
    "    freeze_text_encoder: bool = True\n",
    "    freeze_vision_encoder: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    use_torch_compile: bool = False\n",
    "\n",
    "    # Tokenization / image\n",
    "    max_len: int = 96\n",
    "    image_size: int = 224\n",
    "    pre_tokenize_text: bool = False\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 30\n",
    "    batch_size: int = 32\n",
    "    grad_accum_steps: int = 1\n",
    "    lr: float = 1e-4          # bumped for LoRA\n",
    "    weight_decay: float = 1e-2\n",
    "    warmup_ratio: float = 0.06\n",
    "    label_smoothing: float = 0.05\n",
    "    grad_clip: float = 1.0\n",
    "    early_stop_patience: int = 4\n",
    "    mixed_precision: bool = True\n",
    "\n",
    "    # Architecture\n",
    "    hidden: int = 256\n",
    "    dropout: float = 0.25\n",
    "    ab2_mode: str = \"ce\"\n",
    "    add_meta_dim: int = 2\n",
    "    use_multi_task: bool = True\n",
    "    proj_dim: int = 256  # NEW: shared projection dimension for text/image\n",
    "\n",
    "    # Loss tweaks\n",
    "    lambda_consistency: float = 0.2\n",
    "    image_only_loss_w: float = 0.5   # NEW: weight for image-only aux loss\n",
    "    text_dropout_prob: float = 0.25  # NEW: chance to drop text per step\n",
    "\n",
    "    # Hard-mining curriculum (TEXT)\n",
    "    hard_mining: bool = True\n",
    "    pool_size: int = 5_000\n",
    "    train_text_per_epoch: int = 5_000\n",
    "    hard_frac: float = 0.6\n",
    "\n",
    "    # Memotion images per epoch\n",
    "    images_per_epoch: int = 200\n",
    "\n",
    "    # Balance\n",
    "    use_class_weights: bool = True\n",
    "\n",
    "    # LoRA config\n",
    "    use_lora_text: bool = True\n",
    "    use_lora_vision: bool = False      # enable if you want CLIP LoRA too\n",
    "    lora_r_text: int = 8\n",
    "    lora_alpha_text: int = 16\n",
    "    lora_dropout_text: float = 0.1\n",
    "    lora_target_text: Optional[List[str]] = None  # defaults to [\"query\",\"value\"]\n",
    "\n",
    "    lora_r_vision: int = 4\n",
    "    lora_alpha_vision: int = 8\n",
    "    lora_dropout_vision: float = 0.05\n",
    "    lora_target_vision: Optional[List[str]] = None  # defaults to [\"q_proj\",\"v_proj\"]\n",
    "\n",
    "    # Reporting\n",
    "    reports_dirname: str = \"reports_novel\"\n",
    "    checkpoint_name: str = \"best_novel.pt\"\n",
    "    drive_save_path: Optional[str] = None\n",
    "    final_full_eval: bool = False\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# ============== Device / AMP / Repro ==============\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory/1024**3, \"GB\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler(enabled=cfg.mixed_precision)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    import numpy as np, random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# ============== Imports ==============\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "from torchvision import models, transforms\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, CLIPVisionModel, CLIPImageProcessor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PEFT (LoRA)\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "except ImportError:\n",
    "    print(\"Installing 'peft' for LoRA...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"peft\"])\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ============== JSON-safe ==============\n",
    "def to_serializable(o):\n",
    "    import numpy as np, torch\n",
    "    if isinstance(o, dict):\n",
    "        return {k: to_serializable(v) for k, v in o.items()}\n",
    "    if isinstance(o, (list, tuple, set)):\n",
    "        return [to_serializable(v) for v in o]\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, torch.Tensor):\n",
    "        return o.detach().cpu().tolist()\n",
    "    if isinstance(o, Path):\n",
    "        return str(o)\n",
    "    return o\n",
    "\n",
    "# ============== Label helpers ==============\n",
    "def map_text_labels(row: pd.Series):\n",
    "    hs3 = -100\n",
    "    if \"original_class\" in row and str(row[\"original_class\"]).strip() != \"\":\n",
    "        try:\n",
    "            oc = int(float(row[\"original_class\"]))\n",
    "            hs3 = oc if oc in (0,1,2) else -100\n",
    "        except: hs3 = -100\n",
    "    ab2 = -100\n",
    "    if \"label\" in row and str(row[\"label\"]).strip() != \"\":\n",
    "        try:\n",
    "            v = int(float(row[\"label\"])); ab2 = 1 if v == 1 else 0\n",
    "        except: ab2 = -100\n",
    "    return hs3, ab2\n",
    "\n",
    "def map_memotion_labels(off_cat: str):\n",
    "    s = str(off_cat).strip().lower()\n",
    "    if s == \"hateful_offensive\": hs3 = 0\n",
    "    elif s in (\"offensive\",\"very_offensive\"): hs3 = 1\n",
    "    elif s in (\"slight\",\"not_offensive\"): hs3 = 2\n",
    "    else: hs3 = -100\n",
    "    ab2 = 1 if s in (\"offensive\",\"very_offensive\",\"hateful_offensive\") else 0\n",
    "    return hs3, ab2\n",
    "\n",
    "def parse_sarcasm(val) -> int:\n",
    "    s = str(val).strip().lower()\n",
    "    return 1 if s in {\"sarcasm\",\"sarcastic\",\"yes\",\"true\",\"1\"} or \"sarcas\" in s else 0\n",
    "\n",
    "def parse_humour(val) -> int:\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"\", \"none\", \"not_funny\", \"not funny\", \"no_humour\", \"no_humor\"}: return 0\n",
    "    return 1 if any(k in s for k in [\"funny\",\"hilar\",\"humor\",\"humour\",\"very_funny\"]) else 0\n",
    "\n",
    "def one_hot_smooth(y: int, num_classes: int = 3, eps: float = 0.0) -> torch.Tensor:\n",
    "    vec = torch.full((num_classes,), eps / (num_classes - 1), dtype=torch.float32)\n",
    "    if y >= 0: vec[y] = 1.0 - eps\n",
    "    else: vec[:] = 0.0\n",
    "    return vec\n",
    "\n",
    "class SoftCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=-100): super().__init__(); self.ignore_index = ignore_index\n",
    "    def forward(self, input, target):\n",
    "        if target.dim() == 2:\n",
    "            target = target.to(input.dtype)\n",
    "            valid = target.sum(dim=-1) > 0\n",
    "            if not valid.any():\n",
    "                return input.new_tensor(0.0)\n",
    "            log_probs = F.log_softmax(input[valid], dim=-1)\n",
    "            return -(target[valid] * log_probs).sum(dim=-1).mean()\n",
    "        else:\n",
    "            return F.cross_entropy(input, target, ignore_index=self.ignore_index)\n",
    "\n",
    "def compute_weights_from_counts(counts: np.ndarray):\n",
    "    if counts.sum() == 0: return None\n",
    "    freqs = counts / counts.sum()\n",
    "    w = 1.0 / np.clip(freqs, 1e-8, None)\n",
    "    w = w / (w.mean() + 1e-8)\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "def build_losses(ab2_mode=\"ce\", hs3_class_weights=None, ab2_class_weights=None, device=\"cpu\", label_smoothing=0.0):\n",
    "    def _ce(weight):\n",
    "        try: return nn.CrossEntropyLoss(ignore_index=-100, weight=weight, label_smoothing=label_smoothing)\n",
    "        except TypeError: return nn.CrossEntropyLoss(ignore_index=-100, weight=weight)\n",
    "    hs3_w = hs3_class_weights.to(device) if isinstance(hs3_class_weights, torch.Tensor) else None\n",
    "    ab2_w = ab2_class_weights.to(device) if isinstance(ab2_class_weights, torch.Tensor) else None\n",
    "    crit = {\"hs3\": SoftCrossEntropyLoss(ignore_index=-100)}\n",
    "    crit[\"ab2\"] = _ce(ab2_w) if ab2_mode == \"ce\" else nn.BCEWithLogitsLoss(pos_weight=ab2_w[1:2] if ab2_w is not None else None)\n",
    "    return crit\n",
    "\n",
    "def build_scheduler(optimizer, train_loader_len, cfg):\n",
    "    updates_per_epoch = max(1, math.ceil(train_loader_len / cfg.grad_accum_steps))\n",
    "    total_updates = updates_per_epoch * cfg.epochs\n",
    "    warmup_updates = max(1, int(cfg.warmup_ratio * total_updates))\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_updates:\n",
    "            return float(step) / float(max(1, warmup_updates))\n",
    "        progress = (step - warmup_updates) / float(max(1, total_updates - warmup_updates))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda), total_updates\n",
    "\n",
    "# ============== Tokenizers / processors ==============\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name, use_fast=True)\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(cfg.clip_vision_model) if cfg.use_clip_vision else None\n",
    "img_tfm_fallback = transforms.Compose([\n",
    "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ============== Dataset (Memotion path FIX + Fused labels at load time) ==============\n",
    "class OptimizedProcessedCSVSet(Dataset):\n",
    "    def __init__(self, csv_path: Path, is_memotion=False, split: Optional[str]=None,\n",
    "                 sample_n: Optional[int]=None, pre_tokenize: Optional[bool]=None):\n",
    "        self.is_memotion = is_memotion\n",
    "        self.split = split\n",
    "        self.pre_tokenize = cfg.pre_tokenize_text if pre_tokenize is None else pre_tokenize\n",
    "\n",
    "        hdr = pd.read_csv(csv_path, nrows=0).columns\n",
    "        if is_memotion:\n",
    "            want = [\"text\", \"image_path\", \"offensive_category\", \"sarcasm\", \"humour\"]\n",
    "        else:\n",
    "            want = [\"text\", \"original_class\", \"label\"]\n",
    "        usecols = [c for c in want if c in hdr]\n",
    "        df = pd.read_csv(csv_path, usecols=usecols or None).fillna(\"\")\n",
    "\n",
    "        if is_memotion and split in {\"train\",\"val\",\"test\"}:\n",
    "            rng = np.random.RandomState(123)\n",
    "            idx = np.arange(len(df)); rng.shuffle(idx)\n",
    "            n = len(idx); n_train=int(0.8*n); n_val=int(0.1*n)\n",
    "            part = {\"train\": idx[:n_train], \"val\": idx[n_train:n_train+n_val], \"test\": idx[n_train+n_val:]}[split]\n",
    "            df = df.iloc[part].reset_index(drop=True)\n",
    "\n",
    "        if \"text\" in df.columns and 3 > 0:\n",
    "            df = df[df[\"text\"].astype(str).str.len() >= 3]\n",
    "\n",
    "        if sample_n is not None and len(df) > sample_n:\n",
    "            df = df.sample(n=int(sample_n), random_state=cfg.seed, replace=False).reset_index(drop=True)\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "        # Fused labels (0=Abusive, 1=Offensive, 2=Non-abusive), fallback to ab2 if hs3 missing\n",
    "        self.labels = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            if is_memotion:\n",
    "                hs3, ab2 = map_memotion_labels(row.get(\"offensive_category\",\"\"))\n",
    "            else:\n",
    "                hs3, ab2 = map_text_labels(row)\n",
    "            fused_hs3 = hs3 if hs3 != -100 else (2 if ab2 == 0 else (1 if ab2 == 1 else -100))\n",
    "            self.labels.append((fused_hs3, ab2))\n",
    "\n",
    "        self.use_half = torch.cuda.is_available() and cfg.mixed_precision\n",
    "\n",
    "        self.enc_input_ids, self.enc_attention_mask = None, None\n",
    "        if self.pre_tokenize and len(self.df) > 0:\n",
    "            texts = self.df[\"text\"].astype(str).tolist()\n",
    "            enc = text_tokenizer(texts, truncation=True, max_length=cfg.max_len, padding=False)\n",
    "            self.enc_input_ids, self.enc_attention_mask = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        text = str(row.get(\"text\",\"\")).strip()\n",
    "\n",
    "        if self.enc_input_ids is not None:\n",
    "            input_ids = self.enc_input_ids[i]; attention_mask = self.enc_attention_mask[i]\n",
    "        else:\n",
    "            enc = text_tokenizer(text, truncation=True, max_length=cfg.max_len, padding=False)\n",
    "            input_ids, attention_mask = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "        img = None; has_image = False\n",
    "        if self.is_memotion:\n",
    "            fname = Path(str(row.get(\"image_path\",\"\")).strip()).name\n",
    "            if fname:\n",
    "                img_root = Path(cfg.base_dir) / cfg.processed_dir_name / \"memotion_dataset_7k\" / \"images\"\n",
    "                p = img_root / fname\n",
    "                if p.exists():\n",
    "                    try:\n",
    "                        pil = Image.open(p).convert(\"RGB\")\n",
    "                        if cfg.use_clip_vision:\n",
    "                            img = clip_processor(images=pil, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "                        else:\n",
    "                            img = img_tfm_fallback(pil)\n",
    "                        has_image = True\n",
    "                    except Exception:\n",
    "                        has_image = False\n",
    "\n",
    "        if img is None:\n",
    "            img = torch.zeros(3, cfg.image_size, cfg.image_size, dtype=torch.float32)\n",
    "        if self.use_half: img = img.half()\n",
    "\n",
    "        sarcasm = parse_sarcasm(row.get(\"sarcasm\",\"\")) if self.is_memotion and \"sarcasm\" in row else 0\n",
    "        humour  = parse_humour(row.get(\"humour\",\"\"))   if self.is_memotion and \"humour\" in row else 0\n",
    "        meta = torch.tensor([sarcasm, humour], dtype=torch.float16 if self.use_half else torch.float32)\n",
    "\n",
    "        hs3, ab2 = self.labels[i]\n",
    "        hs3_target = one_hot_smooth(int(hs3), num_classes=3, eps=cfg.label_smoothing)\n",
    "        if self.use_half: hs3_target = hs3_target.half()\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"input_ids\": input_ids, \"attention_mask\": attention_mask,\n",
    "            \"image\": img, \"has_image\": torch.tensor(has_image, dtype=torch.bool),\n",
    "            \"meta\": meta,\n",
    "            \"hs3_label\": torch.tensor(hs3, dtype=torch.long),\n",
    "            \"hs3_target\": hs3_target,\n",
    "            \"ab2_label\": torch.tensor(ab2, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# ============== Model (LoRA + mean pooling + projections + multimodal gate) ==============\n",
    "class OptimizedMultiModalGated(nn.Module):\n",
    "    def __init__(self, text_model_name, freeze_text=True, use_clip=True, clip_model_name=None,\n",
    "                 fallback_backbone=\"mobilenet_v2\", freeze_vision=True,\n",
    "                 use_multi_task=True, ab2_mode=\"ce\", hidden=256, dropout=0.25, add_meta_dim=2,\n",
    "                 gradient_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.use_multi_task = use_multi_task\n",
    "        self.ab2_mode = ab2_mode\n",
    "        self.add_meta_dim = add_meta_dim\n",
    "        self.use_clip = use_clip\n",
    "        self.proj_dim = cfg.proj_dim\n",
    "\n",
    "        # Text encoder (disable pooler + mean pooling)\n",
    "        txt_cfg = AutoConfig.from_pretrained(text_model_name)\n",
    "        if hasattr(txt_cfg, \"add_pooling_layer\"):\n",
    "            txt_cfg.add_pooling_layer = False\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name, config=txt_cfg)\n",
    "        if gradient_checkpointing and hasattr(self.text_model, \"gradient_checkpointing_enable\"):\n",
    "            try: self.text_model.gradient_checkpointing_enable()\n",
    "            except: pass\n",
    "        tdim = self.text_model.config.hidden_size\n",
    "        if freeze_text:\n",
    "            for p in self.text_model.parameters(): p.requires_grad = False\n",
    "\n",
    "        # LoRA on text\n",
    "        if getattr(cfg, \"use_lora_text\", False):\n",
    "            targets = cfg.lora_target_text or [\"query\",\"value\"]\n",
    "            lora_text_cfg = LoraConfig(\n",
    "                r=cfg.lora_r_text, lora_alpha=cfg.lora_alpha_text, lora_dropout=cfg.lora_dropout_text,\n",
    "                target_modules=targets, bias=\"none\", task_type=TaskType.FEATURE_EXTRACTION\n",
    "            )\n",
    "            self.text_model = get_peft_model(self.text_model, lora_text_cfg)\n",
    "\n",
    "        # Vision encoder\n",
    "        if use_clip:\n",
    "            self.vision = CLIPVisionModel.from_pretrained(clip_model_name or \"openai/clip-vit-base-patch32\")\n",
    "            idim = self.vision.config.hidden_size\n",
    "            if freeze_vision:\n",
    "                for p in self.vision.parameters(): p.requires_grad = False\n",
    "            if getattr(cfg, \"use_lora_vision\", False):\n",
    "                vtargets = cfg.lora_target_vision or [\"q_proj\",\"v_proj\"]\n",
    "                lora_vis_cfg = LoraConfig(\n",
    "                    r=cfg.lora_r_vision, lora_alpha=cfg.lora_alpha_vision, lora_dropout=cfg.lora_dropout_vision,\n",
    "                    target_modules=vtargets, bias=\"none\", task_type=TaskType.FEATURE_EXTRACTION\n",
    "                )\n",
    "                self.vision = get_peft_model(self.vision, lora_vis_cfg)\n",
    "            self.use_clip = True\n",
    "        else:\n",
    "            if fallback_backbone.lower() == \"mobilenet_v2\":\n",
    "                try:\n",
    "                    from torchvision.models import MobileNet_V2_Weights\n",
    "                    im = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "                except Exception:\n",
    "                    im = models.mobilenet_v2(pretrained=True)\n",
    "                idim = 1280; im.classifier = nn.Identity()\n",
    "            else:\n",
    "                try:\n",
    "                    from torchvision.models import ResNet18_Weights\n",
    "                    backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "                except Exception:\n",
    "                    backbone = models.resnet18(pretrained=True)\n",
    "                idim = 512; im = nn.Sequential(*list(backbone.children())[:-1], nn.Flatten(1))\n",
    "            if freeze_vision:\n",
    "                for p in im.parameters(): p.requires_grad = False\n",
    "            self.vision = im\n",
    "            self.use_clip = False\n",
    "\n",
    "        # Projections to shared space\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(tdim, self.proj_dim), nn.LayerNorm(self.proj_dim),\n",
    "            nn.GELU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.i_proj = nn.Sequential(\n",
    "            nn.Linear(idim, self.proj_dim), nn.LayerNorm(self.proj_dim),\n",
    "            nn.GELU(), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Backbones use projected features\n",
    "        fusion_in = self.proj_dim + self.proj_dim + (add_meta_dim if add_meta_dim>0 else 0)\n",
    "        text_in   = self.proj_dim + (add_meta_dim if add_meta_dim>0 else 0)\n",
    "\n",
    "        self.fusion_backbone = nn.Sequential(\n",
    "            nn.Linear(fusion_in, hidden), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.GELU(), nn.Dropout(dropout),\n",
    "        )\n",
    "        self.text_backbone = nn.Sequential(\n",
    "            nn.Linear(text_in, hidden), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.GELU(), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Gate sees both modalities (+meta)\n",
    "        gate_in = self.proj_dim + self.proj_dim + add_meta_dim\n",
    "        self.gate_proj = nn.Sequential(\n",
    "            nn.Linear(gate_in, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Heads\n",
    "        self.hs3_fusion_head = nn.Linear(hidden//2, 3)\n",
    "        self.hs3_text_head   = nn.Linear(hidden//2, 3)\n",
    "        self.ab2_fusion_head = nn.Linear(hidden//2, 2)\n",
    "        self.ab2_text_head   = nn.Linear(hidden//2, 2)\n",
    "\n",
    "    def _mean_pool(self, last_hidden_state, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "        return summed / denom\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images, has_image_mask, meta=None):\n",
    "        # Text\n",
    "        out_text = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        t_last = out_text.last_hidden_state\n",
    "        t_feat = self._mean_pool(t_last, attention_mask)\n",
    "\n",
    "        if (meta is not None) and (meta.dtype != t_feat.dtype):\n",
    "            meta = meta.to(t_feat.dtype)\n",
    "\n",
    "        # Vision\n",
    "        B = images.size(0)\n",
    "        img_mask_flat = has_image_mask.view(-1)\n",
    "        if img_mask_flat.any():\n",
    "            idx = torch.nonzero(img_mask_flat, as_tuple=False).squeeze(1)\n",
    "            images_sub = images[idx].to(memory_format=torch.channels_last)\n",
    "            if self.use_clip:\n",
    "                v_out = self.vision(pixel_values=images_sub)\n",
    "                i_sub = v_out.pooler_output\n",
    "            else:\n",
    "                i_sub = self.vision(images_sub)\n",
    "            i_sub = i_sub.to(images_sub.dtype)\n",
    "            i_feat = images.new_zeros((B, self.t_proj[0].out_features), dtype=images_sub.dtype)  # placeholder; we proj later\n",
    "            # We'll store raw i_sub first then project for all B rows below\n",
    "            raw_i_feat = images.new_zeros((B, self.i_proj[0].in_features), dtype=images_sub.dtype)\n",
    "            raw_i_feat[idx] = i_sub\n",
    "        else:\n",
    "            raw_i_feat = images.new_zeros((B, self.i_proj[0].in_features), dtype=images.dtype)\n",
    "\n",
    "        # Project both\n",
    "        t_vec = self.t_proj(t_feat)\n",
    "        i_vec = self.i_proj(raw_i_feat)\n",
    "        img_mask = img_mask_flat.float().unsqueeze(1)\n",
    "\n",
    "        # Inputs\n",
    "        if self.add_meta_dim and meta is not None:\n",
    "            t_in = torch.cat([t_vec, meta], dim=1)\n",
    "            f_in = torch.cat([t_vec, i_vec, meta], dim=1)\n",
    "            g_in = torch.cat([t_vec, i_vec, meta], dim=1)\n",
    "        else:\n",
    "            t_in = t_vec\n",
    "            f_in = torch.cat([t_vec, i_vec], dim=1)\n",
    "            g_in = torch.cat([t_vec, i_vec], dim=1)\n",
    "\n",
    "        t_repr = self.text_backbone(t_in)\n",
    "        f_repr = self.fusion_backbone(f_in)\n",
    "\n",
    "        gate_weight = self.gate_proj(g_in) * img_mask\n",
    "\n",
    "        hs3_f = self.hs3_fusion_head(f_repr)\n",
    "        hs3_t = self.hs3_text_head(t_repr)\n",
    "        hs3_logits = gate_weight * hs3_f + (1 - gate_weight) * hs3_t\n",
    "\n",
    "        ab2_f = self.ab2_fusion_head(f_repr)\n",
    "        ab2_t = self.ab2_text_head(t_repr)\n",
    "        ab2_logits = gate_weight * ab2_f + (1 - gate_weight) * ab2_t\n",
    "\n",
    "        return {\n",
    "            \"hs3_logits\": hs3_logits,\n",
    "            \"ab2_logits\": ab2_logits,\n",
    "            \"gate_weight\": gate_weight.detach(),\n",
    "            \"hs3_f_only\": hs3_f, \"ab2_f_only\": ab2_f,\n",
    "            \"hs3_t_only\": hs3_t, \"ab2_t_only\": ab2_t\n",
    "        }\n",
    "\n",
    "# ============== Collate / utils ==============\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch: return None\n",
    "    pad = text_tokenizer.pad([{\"input_ids\": b[\"input_ids\"], \"attention_mask\": b[\"attention_mask\"]} for b in batch],\n",
    "                             padding=True, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"text\": [b[\"text\"] for b in batch],\n",
    "        \"input_ids\": pad[\"input_ids\"],\n",
    "        \"attention_mask\": pad[\"attention_mask\"],\n",
    "        \"image\": torch.stack([b[\"image\"] for b in batch]),\n",
    "        \"has_image\": torch.stack([b[\"has_image\"] for b in batch]),\n",
    "        \"meta\": torch.stack([b[\"meta\"] for b in batch]),\n",
    "        \"hs3_label\": torch.stack([b[\"hs3_label\"] for b in batch]),\n",
    "        \"hs3_target\": torch.stack([b[\"hs3_target\"] for b in batch]),\n",
    "        \"ab2_label\": torch.stack([b[\"ab2_label\"] for b in batch]),\n",
    "    }\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Mem - Alloc {torch.cuda.memory_allocated()/1024**3:.2f} GB | Reserved {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "\n",
    "# ============== Consistency loss ==============\n",
    "def hierarchical_consistency_loss(hs3_logits, ab2_logits):\n",
    "    p_hs3 = F.softmax(hs3_logits, dim=-1)\n",
    "    q_ab2 = torch.stack([p_hs3[:,2], (p_hs3[:,0] + p_hs3[:,1])], dim=1).clamp_min(1e-8)\n",
    "    p_ab2 = F.softmax(ab2_logits, dim=-1).clamp_min(1e-8)\n",
    "    return F.kl_div(p_ab2.log(), q_ab2, reduction=\"batchmean\") + F.kl_div(q_ab2.log(), p_ab2, reduction=\"batchmean\")\n",
    "\n",
    "# ============== Evaluation ==============\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model, crit, split=\"val\"):\n",
    "    model.eval()\n",
    "    agg = {\"loss\":0.0, \"n\":0}\n",
    "    hs3_y, hs3_p = [], []\n",
    "    ab2_y, ab2_p = [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=f\"Evaluating {split}\", leave=False):\n",
    "        if batch is None: continue\n",
    "        ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attn = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "        has_img = batch[\"has_image\"].to(device, non_blocking=True)\n",
    "        meta = batch[\"meta\"].to(device, non_blocking=True) if cfg.add_meta_dim > 0 else None\n",
    "        y_hs3 = batch[\"hs3_label\"].to(device, non_blocking=True)\n",
    "        y_ab2 = batch[\"ab2_label\"].to(device, non_blocking=True)\n",
    "        hs3_target = batch[\"hs3_target\"].to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=cfg.mixed_precision, dtype=torch.float16):\n",
    "            out = model(ids, attn, imgs, has_img, meta)\n",
    "            hs3_loss = crit[\"hs3\"](out[\"hs3_logits\"], hs3_target)\n",
    "            ab2_loss = crit[\"ab2\"](out[\"ab2_logits\"], y_ab2)\n",
    "            cons_loss = hierarchical_consistency_loss(out[\"hs3_logits\"], out[\"ab2_logits\"]) * cfg.lambda_consistency\n",
    "            # image-only aux loss\n",
    "            img_only_loss = imgs.new_tensor(0.0)\n",
    "            img_mask = has_img.view(-1)\n",
    "            if cfg.image_only_loss_w > 0 and img_mask.any():\n",
    "                img_only_hs3 = crit[\"hs3\"](out[\"hs3_f_only\"][img_mask], hs3_target[img_mask])\n",
    "                img_only_ab2 = crit[\"ab2\"](out[\"ab2_f_only\"][img_mask], y_ab2[img_mask])\n",
    "                img_only_loss = cfg.image_only_loss_w * (img_only_hs3 + img_only_ab2)\n",
    "\n",
    "            loss = hs3_loss + ab2_loss + cons_loss + img_only_loss\n",
    "\n",
    "        agg[\"loss\"] += float(loss.item()); agg[\"n\"] += 1\n",
    "\n",
    "        m_hs3 = y_hs3 != -100\n",
    "        if m_hs3.any():\n",
    "            hs3_y.append(y_hs3[m_hs3].cpu().numpy())\n",
    "            hs3_p.append(out[\"hs3_logits\"][m_hs3].argmax(-1).cpu().numpy())\n",
    "\n",
    "        m_ab2 = y_ab2 != -100\n",
    "        if m_ab2.any():\n",
    "            ab2_y.append(y_ab2[m_ab2].cpu().numpy())\n",
    "            ab2_p.append(out[\"ab2_logits\"][m_ab2].argmax(-1).cpu().numpy())\n",
    "\n",
    "    def agg_metrics(Y, P):\n",
    "        if len(Y)==0: return None\n",
    "        y = np.concatenate(Y); p = np.concatenate(P)\n",
    "        return dict(\n",
    "            acc=accuracy_score(y,p),\n",
    "            f1_macro=f1_score(y,p,average=\"macro\"),\n",
    "            precision_macro=precision_score(y,p,average=\"macro\",zero_division=0),\n",
    "            recall_macro=recall_score(y,p,average=\"macro\",zero_division=0),\n",
    "            y=y, p=p\n",
    "        )\n",
    "\n",
    "    hs3_m = agg_metrics(hs3_y, hs3_p)\n",
    "    ab2_m = agg_metrics(ab2_y, ab2_p)\n",
    "    print(f\"[{split}] loss={agg['loss']/max(1,agg['n']):.4f}\")\n",
    "    if hs3_m: print(f\"[{split}] 3-Class: acc={hs3_m['acc']:.4f} f1={hs3_m['f1_macro']:.4f}\")\n",
    "    if ab2_m: print(f\"[{split}] AB2: acc={ab2_m['acc']:.4f} f1={ab2_m['f1_macro']:.4f}\")\n",
    "    return {\"loss\": agg[\"loss\"]/max(1,agg[\"n\"]), \"hs3\": hs3_m, \"ab2\": ab2_m}\n",
    "\n",
    "# ============== Hard-mining (text) + Memotion sampling helpers ==============\n",
    "@torch.no_grad()\n",
    "def score_difficulty_text(model, dataset: Dataset, indices: List[int], batch_size=256) -> Tuple[List[int], np.ndarray]:\n",
    "    model.eval()\n",
    "    pool_ds = Subset(dataset, indices)\n",
    "    loader = DataLoader(pool_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=(device==\"cuda\"), collate_fn=collate_fn)\n",
    "    diffs = np.zeros(len(indices), dtype=np.float32)\n",
    "    off = 0\n",
    "    for batch in tqdm(loader, desc=\"Scoring hardness\", leave=False):\n",
    "        ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attn = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "        has_img = batch[\"has_image\"].to(device, non_blocking=True)\n",
    "        meta = batch[\"meta\"].to(device, non_blocking=True) if cfg.add_meta_dim > 0 else None\n",
    "        y_hs3 = batch[\"hs3_label\"].to(device, non_blocking=True)\n",
    "        y_ab2 = batch[\"ab2_label\"].to(device, non_blocking=True)\n",
    "        with autocast(enabled=cfg.mixed_precision, dtype=torch.float16):\n",
    "            out = model(ids, attn, imgs, has_img, meta)\n",
    "            hs3_log = out[\"hs3_logits\"]; ab2_log = out[\"ab2_logits\"]\n",
    "            hs3_ce = torch.zeros(ids.size(0), device=device)\n",
    "            m_h = (y_hs3 != -100)\n",
    "            if m_h.any():\n",
    "                hs3_ce[m_h] = F.cross_entropy(hs3_log[m_h], y_hs3[m_h], reduction=\"none\")\n",
    "            ab2_ce = torch.zeros(ids.size(0), device=device)\n",
    "            m_a = (y_ab2 != -100)\n",
    "            if m_a.any():\n",
    "                ab2_ce[m_a] = F.cross_entropy(ab2_log[m_a], y_ab2[m_a], reduction=\"none\")\n",
    "            diff = (hs3_ce + ab2_ce).detach().float().cpu().numpy()\n",
    "        diffs[off:off+len(diff)] = diff\n",
    "        off += len(diff)\n",
    "    return indices, diffs\n",
    "\n",
    "def select_hard_examples(model, full_text_ds: Dataset, pool_size: int, select_n: int, hard_frac: float, epoch: int) -> List[int]:\n",
    "    N = len(full_text_ds)\n",
    "    pool_size = min(pool_size, N)\n",
    "    rng_pool = np.random.RandomState(cfg.seed + 1009*epoch)\n",
    "    pool_idx = rng_pool.choice(N, size=pool_size, replace=False).tolist()\n",
    "    _, diffs = score_difficulty_text(model, full_text_ds, pool_idx, batch_size=256)\n",
    "    k_hard = int(select_n * hard_frac)\n",
    "    hard_idx_rel = np.argsort(-diffs)[:k_hard]\n",
    "    hard_idx = [pool_idx[i] for i in hard_idx_rel]\n",
    "    remaining = select_n - k_hard\n",
    "    remaining_candidates = [i for i in range(N) if i not in set(hard_idx)]\n",
    "    rng_rest = np.random.RandomState(cfg.seed + 2027*epoch)\n",
    "    rand_rel = rng_rest.choice(len(remaining_candidates), size=remaining, replace=False)\n",
    "    rand_idx = [remaining_candidates[i] for i in rand_rel]\n",
    "    return hard_idx + rand_idx\n",
    "\n",
    "def get_memotion_valid_indices(ds_memo: OptimizedProcessedCSVSet) -> List[int]:\n",
    "    valid = []\n",
    "    img_root = Path(cfg.base_dir) / cfg.processed_dir_name / \"memotion_dataset_7k\" / \"images\"\n",
    "    for i, row in ds_memo.df.iterrows():\n",
    "        fname = Path(str(row.get(\"image_path\",\"\")).strip()).name\n",
    "        if fname and (img_root / fname).exists():\n",
    "            valid.append(i)\n",
    "    return valid\n",
    "\n",
    "def sample_memotion_indices(valid_idx: List[int], n: int, epoch: int) -> List[int]:\n",
    "    n = min(n, len(valid_idx))\n",
    "    rng = np.random.RandomState(cfg.seed + 707*epoch)\n",
    "    sel = rng.choice(valid_idx, size=n, replace=False).tolist()\n",
    "    return sel\n",
    "\n",
    "# ============== Train ==============\n",
    "def train_novel():\n",
    "    clear_memory()\n",
    "    processed_dir = Path(cfg.base_dir) / cfg.processed_dir_name\n",
    "    splits_dir = processed_dir / cfg.splits_dirname\n",
    "    reports_dir = processed_dir / cfg.reports_dirname\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    memotion_path = processed_dir / cfg.memotion_csv\n",
    "    assert memotion_path.exists(), f\"Missing memotion CSV: {memotion_path}\"\n",
    "    train_text_csv = splits_dir / cfg.text_train_csv\n",
    "    val_text_csv   = splits_dir / cfg.text_val_csv\n",
    "    test_text_csv  = splits_dir / cfg.text_test_csv\n",
    "    for p in [train_text_csv, val_text_csv, test_text_csv]:\n",
    "        assert p.exists(), f\"Missing: {p}\"\n",
    "\n",
    "    print(\"Paths verified\")\n",
    "\n",
    "    # Datasets\n",
    "    ds_train_text_full = OptimizedProcessedCSVSet(train_text_csv, is_memotion=False, sample_n=None, pre_tokenize=False)\n",
    "    ds_val_text   = OptimizedProcessedCSVSet(val_text_csv,   is_memotion=False, sample_n=cfg.sample_val_text_n, pre_tokenize=True)\n",
    "    ds_test_text  = OptimizedProcessedCSVSet(test_text_csv,  is_memotion=False, sample_n=cfg.sample_test_text_n, pre_tokenize=True)\n",
    "\n",
    "    ds_val_memo   = OptimizedProcessedCSVSet(memotion_path, is_memotion=True, split=\"val\",  sample_n=cfg.sample_val_memotion_n, pre_tokenize=True)\n",
    "    ds_test_memo  = OptimizedProcessedCSVSet(memotion_path, is_memotion=True, split=\"test\", sample_n=cfg.sample_test_memotion_n, pre_tokenize=True)\n",
    "    ds_train_memo_full = OptimizedProcessedCSVSet(memotion_path, is_memotion=True, split=\"train\", sample_n=None, pre_tokenize=True)\n",
    "\n",
    "    memotion_valid_idx = get_memotion_valid_indices(ds_train_memo_full)\n",
    "    if len(memotion_valid_idx) == 0:\n",
    "        print(\"No valid Memotion images found under memotion_dataset_7k/images. Training will use zero-image placeholders.\")\n",
    "\n",
    "    # Initial subsets: 5k text + 200 images (you can increase images_per_epoch to 1k+ for stronger vision)\n",
    "    text_init_size = min(cfg.train_text_per_epoch, len(ds_train_text_full))\n",
    "    init_text_idx = np.random.RandomState(cfg.seed).choice(len(ds_train_text_full), size=text_init_size, replace=False)\n",
    "    ds_train_text_epoch = Subset(ds_train_text_full, init_text_idx.tolist())\n",
    "\n",
    "    memo_init_idx = sample_memotion_indices(memotion_valid_idx, cfg.images_per_epoch, epoch=0)\n",
    "    ds_train_memo_epoch = Subset(ds_train_memo_full, memo_init_idx)\n",
    "\n",
    "    train_ds = ConcatDataset([ds_train_text_epoch, ds_train_memo_epoch])\n",
    "    val_ds   = ConcatDataset([ds_val_text, ds_val_memo])\n",
    "    test_ds  = ConcatDataset([ds_test_text, ds_test_memo])\n",
    "\n",
    "    # Dataloaders\n",
    "    cpu_ct = os.cpu_count() or 2\n",
    "    num_workers = min(8, max(2, cpu_ct // 2))\n",
    "    dl_kwargs = dict(num_workers=num_workers, collate_fn=collate_fn, pin_memory=(device==\"cuda\"), persistent_workers=True, prefetch_factor=4)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, **dl_kwargs)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, **dl_kwargs)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, **dl_kwargs)\n",
    "\n",
    "    print(f\"Datasets — Train:{len(train_ds)} (text {len(ds_train_text_epoch)} + memo {len(ds_train_memo_epoch)}) | Val:{len(val_ds)} | Test:{len(test_ds)}\")\n",
    "\n",
    "    # Model\n",
    "    model = OptimizedMultiModalGated(\n",
    "        text_model_name=cfg.text_model_name,\n",
    "        freeze_text=cfg.freeze_text_encoder,\n",
    "        use_clip=cfg.use_clip_vision,\n",
    "        clip_model_name=cfg.clip_vision_model,\n",
    "        fallback_backbone=cfg.image_backbone_fallback,\n",
    "        freeze_vision=cfg.freeze_vision_encoder,\n",
    "        use_multi_task=cfg.use_multi_task,\n",
    "        ab2_mode=cfg.ab2_mode,\n",
    "        hidden=cfg.hidden,\n",
    "        dropout=cfg.dropout,\n",
    "        add_meta_dim=cfg.add_meta_dim,\n",
    "        gradient_checkpointing=cfg.gradient_checkpointing\n",
    "    ).to(device)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    if cfg.use_torch_compile and hasattr(torch, \"compile\"):\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "            print(\"torch.compile enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"torch.compile skipped: {e}\")\n",
    "\n",
    "    # Optimizer\n",
    "    params_to_train = [p for p in model.parameters() if p.requires_grad]\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(params_to_train, lr=cfg.lr, weight_decay=cfg.weight_decay, fused=True)\n",
    "    except TypeError:\n",
    "        optimizer = torch.optim.AdamW(params_to_train, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # Class weights (from current train subsets)\n",
    "    def counts_from_concat(text_subset: Dataset, memo_subset: Dataset, label_pos, n_classes):\n",
    "        counts = np.zeros(n_classes, dtype=np.int64)\n",
    "        for i in range(len(text_subset)):\n",
    "            idx = text_subset.indices[i] if isinstance(text_subset, Subset) else i\n",
    "            y = ds_train_text_full.labels[idx][label_pos]\n",
    "            if y != -100: counts[y] += 1\n",
    "        for i in range(len(memo_subset)):\n",
    "            idx = memo_subset.indices[i] if isinstance(memo_subset, Subset) else i\n",
    "            y_h, y_a = ds_train_memo_full.labels[idx]\n",
    "            if label_pos == 0 and y_h != -100: counts[y_h] += 1\n",
    "            if label_pos == 1 and y_a != -100: counts[y_a] += 1\n",
    "        return counts\n",
    "\n",
    "    hs3_w = ab2_w = None\n",
    "    if cfg.use_class_weights:\n",
    "        hs3_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=0, n_classes=3)\n",
    "        ab2_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=1, n_classes=2)\n",
    "        hs3_w = compute_weights_from_counts(hs3_counts); ab2_w = compute_weights_from_counts(ab2_counts)\n",
    "\n",
    "    crit = build_losses(cfg.ab2_mode, hs3_w, ab2_w, device=device, label_smoothing=cfg.label_smoothing)\n",
    "    scheduler, total_updates = build_scheduler(optimizer, len(train_loader), cfg)\n",
    "\n",
    "    print(\"Training started\")\n",
    "    print(f\"Steps/epoch: {len(train_loader)} | Total steps: {total_updates}\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "    best_score = -1.0\n",
    "    patience = 0\n",
    "    history = []\n",
    "\n",
    "    def unwrap_for_save(m):\n",
    "        if hasattr(m, \"_orig_mod\"): m = m._orig_mod\n",
    "        if hasattr(m, \"module\"): m = m.module\n",
    "        return m\n",
    "\n",
    "    for epoch in range(1, cfg.epochs+1):\n",
    "        # Rebuild subsets each epoch\n",
    "        if cfg.hard_mining:\n",
    "            print(f\"\\nHard-mining epoch {epoch}: pool {cfg.pool_size} -> select {cfg.train_text_per_epoch} (hard_frac={cfg.hard_frac})\")\n",
    "            with torch.no_grad():\n",
    "                sel_text_idx = select_hard_examples(model, ds_train_text_full, cfg.pool_size, cfg.train_text_per_epoch, cfg.hard_frac, epoch)\n",
    "            ds_train_text_epoch = Subset(ds_train_text_full, sel_text_idx)\n",
    "\n",
    "        sel_memo_idx = sample_memotion_indices(memotion_valid_idx, cfg.images_per_epoch, epoch)\n",
    "        ds_train_memo_epoch = Subset(ds_train_memo_full, sel_memo_idx)\n",
    "\n",
    "        train_ds = ConcatDataset([ds_train_text_epoch, ds_train_memo_epoch])\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, **dl_kwargs)\n",
    "\n",
    "        if cfg.use_class_weights:\n",
    "            hs3_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=0, n_classes=3)\n",
    "            ab2_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=1, n_classes=2)\n",
    "            hs3_w = compute_weights_from_counts(hs3_counts); ab2_w = compute_weights_from_counts(ab2_counts)\n",
    "            crit = build_losses(cfg.ab2_mode, hs3_w, ab2_w, device=device, label_smoothing=cfg.label_smoothing)\n",
    "\n",
    "        print(f\"Epoch {epoch} train sizes — text: {len(ds_train_text_epoch)}, memotion: {len(ds_train_memo_epoch)}\")\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}/{cfg.epochs}\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for it, batch in pbar:\n",
    "            if batch is None: continue\n",
    "            ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attn = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "            has_img = batch[\"has_image\"].to(device, non_blocking=True)\n",
    "            meta = batch[\"meta\"].to(device, non_blocking=True) if cfg.add_meta_dim > 0 else None\n",
    "            y_ab2 = batch[\"ab2_label\"].to(device, non_blocking=True)\n",
    "            hs3_target = batch[\"hs3_target\"].to(device, non_blocking=True)\n",
    "\n",
    "            # Modality dropout for text\n",
    "            if cfg.text_dropout_prob > 0 and random.random() < cfg.text_dropout_prob:\n",
    "                ids = ids.clone(); attn = torch.zeros_like(attn)\n",
    "                ids[:] = text_tokenizer.pad_token_id\n",
    "\n",
    "            with autocast(enabled=cfg.mixed_precision, dtype=torch.float16):\n",
    "                out = model(ids, attn, imgs, has_img, meta)\n",
    "                hs3_loss = crit[\"hs3\"](out[\"hs3_logits\"], hs3_target)\n",
    "                ab2_loss = crit[\"ab2\"](out[\"ab2_logits\"], y_ab2)\n",
    "                cons_loss = hierarchical_consistency_loss(out[\"hs3_logits\"], out[\"ab2_logits\"]) * cfg.lambda_consistency\n",
    "\n",
    "                # image-only aux loss (cheap; reuse fusion-only heads)\n",
    "                img_only_loss = imgs.new_tensor(0.0)\n",
    "                img_mask = has_img.view(-1)\n",
    "                if cfg.image_only_loss_w > 0 and img_mask.any():\n",
    "                    img_only_hs3 = crit[\"hs3\"](out[\"hs3_f_only\"][img_mask], hs3_target[img_mask])\n",
    "                    img_only_ab2 = crit[\"ab2\"](out[\"ab2_f_only\"][img_mask], y_ab2[img_mask])\n",
    "                    img_only_loss = cfg.image_only_loss_w * (img_only_hs3 + img_only_ab2)\n",
    "\n",
    "                loss = (hs3_loss + ab2_loss + cons_loss + img_only_loss) / cfg.grad_accum_steps\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            step_now = ((it + 1) % cfg.grad_accum_steps == 0) or (it + 1 == len(train_loader))\n",
    "            if step_now:\n",
    "                if cfg.grad_clip:\n",
    "                    if scaler.is_enabled(): scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(params_to_train, cfg.grad_clip)\n",
    "                if scaler.is_enabled():\n",
    "                    scaler.step(optimizer); scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += float(loss.item()) * cfg.grad_accum_steps\n",
    "            total_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{running_loss/total_batches:.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "\n",
    "            del out, loss, hs3_loss, ab2_loss, cons_loss, img_only_loss\n",
    "\n",
    "        avg_train_loss = running_loss / max(1, total_batches)\n",
    "        val_metrics = evaluate(val_loader, model, crit, split=\"val\")\n",
    "\n",
    "        f1s = []\n",
    "        if val_metrics[\"hs3\"] is not None: f1s.append(val_metrics[\"hs3\"][\"f1_macro\"])\n",
    "        if val_metrics[\"ab2\"] is not None: f1s.append(val_metrics[\"ab2\"][\"f1_macro\"])\n",
    "        current_score = float(np.mean(f1s)) if f1s else -val_metrics[\"loss\"]\n",
    "\n",
    "        history.append({'epoch': epoch, 'train_loss': avg_train_loss, 'val_score': current_score,\n",
    "                        'val_hs3_f1': val_metrics[\"hs3\"][\"f1_macro\"] if val_metrics[\"hs3\"] else None,\n",
    "                        'val_ab2_f1': val_metrics[\"ab2\"][\"f1_macro\"] if val_metrics[\"ab2\"] else None})\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Score={current_score:.4f}, Best={best_score:.4f}\")\n",
    "        print_gpu_memory()\n",
    "\n",
    "        base_model = unwrap_for_save(model)\n",
    "        ckpt = {\n",
    "            \"model\": base_model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if scaler.is_enabled() else None,\n",
    "            \"epoch\": epoch,\n",
    "            \"best_score\": max(best_score, current_score),\n",
    "            \"cfg\": cfg.__dict__,\n",
    "            \"history\": history\n",
    "        }\n",
    "\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            patience = 0\n",
    "            torch.save(ckpt, processed_dir / cfg.checkpoint_name)\n",
    "            if cfg.drive_save_path:\n",
    "                Path(cfg.drive_save_path).mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(ckpt, Path(cfg.drive_save_path) / cfg.checkpoint_name)\n",
    "            print(\"New best model saved!\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            print(f\"⏳ No improvement. Patience {patience}/{cfg.early_stop_patience}\")\n",
    "            if patience >= cfg.early_stop_patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "        clear_memory()\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL EVALUATION (tracking splits)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    def _clean_state_dict_keys(sd):\n",
    "        out = {}\n",
    "        for k, v in sd.items():\n",
    "            if k.startswith(\"_orig_mod.\"): k = k[len(\"_orig_mod.\"):]\n",
    "            if k.startswith(\"module.\"):    k = k[len(\"module.\"):]\n",
    "            out[k] = v\n",
    "        return out\n",
    "\n",
    "    processed_dir = Path(cfg.base_dir) / cfg.processed_dir_name\n",
    "    reports_dir = processed_dir / cfg.reports_dirname\n",
    "    ckpt_path = processed_dir / cfg.checkpoint_name\n",
    "    if ckpt_path.exists():\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        sd = _clean_state_dict_keys(checkpoint[\"model\"])\n",
    "        model.load_state_dict(sd, strict=True)\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch']} | score {checkpoint['best_score']:.4f}\")\n",
    "\n",
    "    test_metrics = evaluate(test_loader, model, crit, split=\"test\")\n",
    "    results = {\"best_val_score\": best_score, \"final_test_metrics\": test_metrics, \"training_history\": history, \"config\": cfg.__dict__}\n",
    "\n",
    "    with open(reports_dir / \"training_results_novel.json\", \"w\") as f:\n",
    "        json.dump(to_serializable(results), f, indent=2)\n",
    "\n",
    "    if cfg.final_full_eval:\n",
    "        print(\"\\n===== FULL SPLIT EVALUATION (val/test full) =====\")\n",
    "        ds_val_text_full  = OptimizedProcessedCSVSet(splits_dir / cfg.text_val_csv,  is_memotion=False, sample_n=None, pre_tokenize=True)\n",
    "        ds_test_text_full = OptimizedProcessedCSVSet(splits_dir / cfg.text_test_csv, is_memotion=False, sample_n=None, pre_tokenize=True)\n",
    "        val_ds_full  = ConcatDataset([ds_val_text_full,  ds_val_memo])\n",
    "        test_ds_full = ConcatDataset([ds_test_text_full, ds_test_memo])\n",
    "\n",
    "        num_workers_full = min(8, max(2, (os.cpu_count() or 2)//2))\n",
    "        val_loader_full = DataLoader(val_ds_full, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                     num_workers=num_workers_full, collate_fn=collate_fn,\n",
    "                                     pin_memory=(device==\"cuda\"), persistent_workers=True, prefetch_factor=4)\n",
    "        test_loader_full = DataLoader(test_ds_full, batch_size=cfg.batch_size, shuffle=False,\n",
    "                                      num_workers=num_workers_full, collate_fn=collate_fn,\n",
    "                                      pin_memory=(device==\"cuda\"), persistent_workers=True, prefetch_factor=4)\n",
    "        val_full = evaluate(val_loader_full, model, crit, split=\"val_full200k\")\n",
    "        test_full = evaluate(test_loader_full, model, crit, split=\"test_full200k\")\n",
    "\n",
    "    def save_confusion_matrix(y, p, labels, name):\n",
    "        cm = confusion_matrix(y, p, labels=list(range(len(labels))))\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f\"{name} Confusion Matrix\"); plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
    "        plt.tight_layout(); plt.savefig(reports_dir / f\"{name}_cm.png\", dpi=150); plt.close()\n",
    "        pd.DataFrame(cm, index=labels, columns=labels).to_csv(reports_dir / f\"{name}_cm.csv\")\n",
    "\n",
    "    if test_metrics[\"hs3\"] is not None:\n",
    "        y, p = test_metrics[\"hs3\"][\"y\"], test_metrics[\"hs3\"][\"p\"]\n",
    "        save_confusion_matrix(y, p, [\"Abusive\",\"Offensive\",\"Non-abusive\"], \"HS3_test\")\n",
    "    if test_metrics[\"ab2\"] is not None:\n",
    "        y, p = test_metrics[\"ab2\"][\"y\"], test_metrics[\"ab2\"][\"p\"]\n",
    "        save_confusion_matrix(y, p, [\"Non-abusive\",\"Abusive\"], \"AB2_test\")\n",
    "\n",
    "    if cfg.drive_save_path:\n",
    "        import shutil\n",
    "        drive_results = Path(cfg.drive_save_path) / cfg.reports_dirname\n",
    "        shutil.copytree(reports_dir, drive_results, dirs_exist_ok=True)\n",
    "        print(f\"Copied reports to {drive_results}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best Val Score: {best_score:.4f} | Reports: {reports_dir}\")\n",
    "    return results\n",
    "\n",
    "# ============== Run ==============\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting novel multimodal training (CLIP + consistency + hard-mining + LoRA, direct 3-class) …\")\n",
    "    results = train_novel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3fc1a-ef69-423b-b14f-8fa51df04158",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "INTERPRETATION OF ARCHITECTURE DESIGN — MULTIMODAL HATE DETECTION WITH LoRA\n",
    "\n",
    "The novel multimodal architecture combines text and vision encoders with\n",
    "adaptive gating and LoRA fine-tuning to handle the complex separability\n",
    "patterns in hateful meme detection.\n",
    "\n",
    "Key architectural interpretations:\n",
    "\n",
    "- **Dual Encoder Strategy**: Uses MiniLM for text and CLIP-ViT for images,\n",
    "  recognizing that hate in memes often requires understanding both modalities\n",
    "  simultaneously. Text captures explicit slurs while vision contextualizes\n",
    "  cultural references and visual stereotypes.\n",
    "\n",
    "- **Shared Projection Space**: Projects both text and image features into\n",
    "  a common 256D space, enabling the model to learn cross-modal relationships\n",
    "  where offensive meaning emerges from text-image combinations rather than\n",
    "  either modality alone.\n",
    "\n",
    "- **Adaptive Gating Mechanism**: The gate weight dynamically balances\n",
    "  modality contributions per sample. For text-heavy memes, it relies more\n",
    "  on textual analysis; for image-dominant offensive content, it shifts\n",
    "  focus to visual features. This handles cases where offensive intent is\n",
    "  only apparent when both modalities are considered together.\n",
    "\n",
    "- **LoRA Fine-tuning**: Applies Low-Rank Adaptation to the text encoder\n",
    "  (and optionally vision) to efficiently adapt pre-trained models without\n",
    "  full fine-tuning. This preserves linguistic capabilities while specializing\n",
    "  for hate detection patterns, particularly useful for detecting coded\n",
    "  language and subtle offensive constructs.\n",
    "\n",
    "- **Auxiliary Image-Only Loss**: Forces the vision backbone to develop\n",
    "  standalone offensive content recognition, crucial for memes where\n",
    "  the image alone conveys hateful meaning while text appears neutral.\n",
    "\n",
    "- **Text Modality Dropout**: Randomly masks text during training to\n",
    "  prevent over-reliance on textual cues and force robust multimodal\n",
    "  reasoning, especially important for culturally contextual memes\n",
    "  where visual elements carry offensive weight.\n",
    "\n",
    "- **Hierarchical Consistency**: Aligns 3-class (Abusive/Offensive/Non-abusive)\n",
    "  and binary predictions to maintain logical coherence, addressing the\n",
    "  dataset's natural hierarchy where abusive content is a subset of offensive.\n",
    "\n",
    "Overall interpretation:\n",
    "This architecture acknowledges that hateful meme detection operates in\n",
    "a continuum from clearly separable explicit content to highly ambiguous\n",
    "borderline cases. The multimodal gating, projection strategy, and auxiliary\n",
    "objectives create a robust system that can handle both obvious hate speech\n",
    "and subtle, context-dependent offensive content that requires joint\n",
    "text-image understanding.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab077b21-544d-4bdf-9dc8-11a85cdbc828",
   "metadata": {
    "id": "ab077b21-544d-4bdf-9dc8-11a85cdbc828",
    "outputId": "205e714a-535c-45ed-bb21-22b1938ddcf5"
   },
   "outputs": [],
   "source": [
    "# ds_multimodal_novel_lora_full_pooled.py\n",
    "# End-to-end: Train (LoRA + hard-mining + image-only aux loss + text dropout) + Direct 3-Class Evaluation\n",
    "# Keys:\n",
    "# - Fused 3-class labels at dataset load time (0=Abusive, 1=Offensive, 2=Non-abusive)\n",
    "# - Mean pooling text encoder (no pooler warning)\n",
    "# - Projection layers for text/image; gate uses both modalities (not text-only)\n",
    "# - Cheap image-only auxiliary loss (no extra forward) + optional text-modality dropout\n",
    "# - Direct 3-class eval; PR/ECE; gate analysis; CSV export\n",
    "\n",
    "# Optional installs:\n",
    "# !pip install -U torch torchvision transformers scikit-learn pandas numpy pillow tqdm seaborn matplotlib peft\n",
    "\n",
    "import os, gc, json, math, random, warnings, sys, subprocess\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"  # quiet HF warnings\n",
    "\n",
    "# ===========================\n",
    "# USER TOGGLES\n",
    "# ===========================\n",
    "RUN_TRAIN = False          # True to train; False to only evaluate using saved checkpoint\n",
    "RUN_FUSED_EVAL = True      # Run final direct 3-class eval + insights\n",
    "TEACHER_N = 10000           # None = FULL test; set int (e.g., 5000) for a quick run\n",
    "EXPORT_SAMPLE = None       # None = full export; or int for sample CSV\n",
    "BATCH_EVAL = 128           # Batch size for eval/export\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = \"datasets/processed_data\"\n",
    "SPLITS_DIR = f\"{BASE_DIR}/splits\"\n",
    "CKPT_PATH = f\"{BASE_DIR}/best_novel.pt\"   # Training saves here by default\n",
    "OUT_DIR   = f\"{BASE_DIR}/novel_eval_insights\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===========================\n",
    "# Config\n",
    "# ===========================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # Paths\n",
    "    base_dir: str = \"datasets\"\n",
    "    processed_dir_name: str = \"processed_data\"\n",
    "    memotion_csv: str = \"memotion_7k_multimodal.csv\"\n",
    "    splits_dirname: str = \"splits\"\n",
    "\n",
    "    # Text CSVs\n",
    "    text_train_csv: str = \"train/text_train.csv\"\n",
    "    text_val_csv: str = \"val/text_val.csv\"\n",
    "    text_test_csv: str = \"test/text_test.csv\"\n",
    "\n",
    "    # Random subsampling for val/test (tracking during training)\n",
    "    seed: int = 42\n",
    "    sample_val_text_n: Optional[int] = 7_000\n",
    "    sample_test_text_n: Optional[int] = 5_000\n",
    "    sample_val_memotion_n: Optional[int] = None\n",
    "    sample_test_memotion_n: Optional[int] = None\n",
    "\n",
    "    # Model (text + vision)\n",
    "    text_model_name: str = \"nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large\"\n",
    "    use_clip_vision: bool = True\n",
    "    clip_vision_model: str = \"openai/clip-vit-base-patch32\"\n",
    "    image_backbone_fallback: str = \"mobilenet_v2\"\n",
    "    freeze_text_encoder: bool = True\n",
    "    freeze_vision_encoder: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    use_torch_compile: bool = False\n",
    "\n",
    "    # Tokenization / image\n",
    "    max_len: int = 96\n",
    "    image_size: int = 224\n",
    "    pre_tokenize_text: bool = False\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 30\n",
    "    batch_size: int = 32\n",
    "    grad_accum_steps: int = 1\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    warmup_ratio: float = 0.06\n",
    "    label_smoothing: float = 0.05\n",
    "    grad_clip: float = 1.0\n",
    "    early_stop_patience: int = 4\n",
    "    mixed_precision: bool = True\n",
    "\n",
    "    # Architecture\n",
    "    hidden: int = 256\n",
    "    dropout: float = 0.25\n",
    "    ab2_mode: str = \"ce\"\n",
    "    add_meta_dim: int = 2\n",
    "    use_multi_task: bool = True\n",
    "    proj_dim: int = 256   # projection dimension for text/image before fusion\n",
    "\n",
    "    # Loss tweaks\n",
    "    lambda_consistency: float = 0.2\n",
    "    image_only_loss_w: float = 0.5   # weight for image-only aux loss (0.0 to disable)\n",
    "    text_dropout_prob: float = 0.25  # probability to drop text (forces image reliance)\n",
    "\n",
    "    # Hard-mining curriculum (TEXT)\n",
    "    hard_mining: bool = True\n",
    "    pool_size: int = 5_000\n",
    "    train_text_per_epoch: int = 5_000\n",
    "    hard_frac: float = 0.6\n",
    "\n",
    "    # Memotion images per epoch\n",
    "    images_per_epoch: int = 200\n",
    "\n",
    "    # Balance\n",
    "    use_class_weights: bool = True\n",
    "\n",
    "    # LoRA config\n",
    "    use_lora_text: bool = True\n",
    "    use_lora_vision: bool = False      # enable if you want CLIP LoRA too\n",
    "    lora_r_text: int = 8\n",
    "    lora_alpha_text: int = 16\n",
    "    lora_dropout_text: float = 0.1\n",
    "    lora_target_text: Optional[List[str]] = None  # [\"query\",\"value\"] default\n",
    "\n",
    "    lora_r_vision: int = 4\n",
    "    lora_alpha_vision: int = 8\n",
    "    lora_dropout_vision: float = 0.05\n",
    "    lora_target_vision: Optional[List[str]] = None  # [\"q_proj\",\"v_proj\"] default\n",
    "\n",
    "    # Text pooling\n",
    "    text_pooling: str = \"mean\"   # \"mean\" or \"cls\"\n",
    "\n",
    "    # Reporting\n",
    "    reports_dirname: str = \"reports_novel\"\n",
    "    checkpoint_name: str = \"best_novel.pt\"\n",
    "    drive_save_path: Optional[str] = None\n",
    "    final_full_eval: bool = False\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# ===========================\n",
    "# Device / AMP / Repro\n",
    "# ===========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM:\", torch.cuda.get_device_properties(0).total_memory/1024**3, \"GB\")\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler(enabled=cfg.mixed_precision)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    import numpy as np, random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# ===========================\n",
    "# Imports\n",
    "# ===========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "from torchvision import models, transforms\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, CLIPVisionModel, CLIPImageProcessor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PEFT (LoRA)\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "except ImportError:\n",
    "    print(\"Installing 'peft' for LoRA...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"peft\"])\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ===========================\n",
    "# JSON-safe\n",
    "# ===========================\n",
    "def to_serializable(o):\n",
    "    import numpy as np, torch\n",
    "    if isinstance(o, dict):\n",
    "        return {k: to_serializable(v) for k, v in o.items()}\n",
    "    if isinstance(o, (list, tuple, set)):\n",
    "        return [to_serializable(v) for v in o]\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, torch.Tensor):\n",
    "        return o.detach().cpu().tolist()\n",
    "    if isinstance(o, Path):\n",
    "        return str(o)\n",
    "    return o\n",
    "\n",
    "# ===========================\n",
    "# Label helpers\n",
    "# ===========================\n",
    "def map_text_labels(row: pd.Series):\n",
    "    hs3 = -100\n",
    "    if \"original_class\" in row and str(row[\"original_class\"]).strip() != \"\":\n",
    "        try:\n",
    "            oc = int(float(row[\"original_class\"]))\n",
    "            hs3 = oc if oc in (0,1,2) else -100\n",
    "        except: hs3 = -100\n",
    "    ab2 = -100\n",
    "    if \"label\" in row and str(row[\"label\"]).strip() != \"\":\n",
    "        try:\n",
    "            v = int(float(row[\"label\"])); ab2 = 1 if v == 1 else 0\n",
    "        except: ab2 = -100\n",
    "    return hs3, ab2\n",
    "\n",
    "def map_memotion_labels(off_cat: str):\n",
    "    s = str(off_cat).strip().lower()\n",
    "    if s == \"hateful_offensive\": hs3 = 0\n",
    "    elif s in (\"offensive\",\"very_offensive\"): hs3 = 1\n",
    "    elif s in (\"slight\",\"not_offensive\"): hs3 = 2\n",
    "    else: hs3 = -100\n",
    "    ab2 = 1 if s in (\"offensive\",\"very_offensive\",\"hateful_offensive\") else 0\n",
    "    return hs3, ab2\n",
    "\n",
    "def parse_sarcasm(val) -> int:\n",
    "    s = str(val).strip().lower()\n",
    "    return 1 if s in {\"sarcasm\",\"sarcastic\",\"yes\",\"true\",\"1\"} or \"sarcas\" in s else 0\n",
    "\n",
    "def parse_humour(val) -> int:\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"\", \"none\", \"not_funny\", \"not funny\", \"no_humour\", \"no_humor\"}: return 0\n",
    "    return 1 if any(k in s for k in [\"funny\",\"hilar\",\"humor\",\"humour\",\"very_funny\"]) else 0\n",
    "\n",
    "def one_hot_smooth(y: int, num_classes: int = 3, eps: float = 0.0) -> torch.Tensor:\n",
    "    vec = torch.full((num_classes,), eps / (num_classes - 1), dtype=torch.float32)\n",
    "    if y >= 0: vec[y] = 1.0 - eps\n",
    "    else: vec[:] = 0.0\n",
    "    return vec\n",
    "\n",
    "class SoftCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=-100): super().__init__(); self.ignore_index = ignore_index\n",
    "    def forward(self, input, target):\n",
    "        if target.dim() == 2:\n",
    "            target = target.to(input.dtype)\n",
    "            valid = target.sum(dim=-1) > 0\n",
    "            if not valid.any():\n",
    "                return input.new_tensor(0.0)\n",
    "            log_probs = F.log_softmax(input[valid], dim=-1)\n",
    "            return -(target[valid] * log_probs).sum(dim=-1).mean()\n",
    "        else:\n",
    "            return F.cross_entropy(input, target, ignore_index=self.ignore_index)\n",
    "\n",
    "def compute_weights_from_counts(counts: np.ndarray):\n",
    "    if counts.sum() == 0: return None\n",
    "    freqs = counts / counts.sum()\n",
    "    w = 1.0 / np.clip(freqs, 1e-8, None)\n",
    "    w = w / (w.mean() + 1e-8)\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "def build_losses(ab2_mode=\"ce\", hs3_class_weights=None, ab2_class_weights=None, device=\"cpu\", label_smoothing=0.0):\n",
    "    def _ce(weight):\n",
    "        try: return nn.CrossEntropyLoss(ignore_index=-100, weight=weight, label_smoothing=label_smoothing)\n",
    "        except TypeError: return nn.CrossEntropyLoss(ignore_index=-100, weight=weight)\n",
    "    hs3_w = hs3_class_weights.to(device) if isinstance(hs3_class_weights, torch.Tensor) else None\n",
    "    ab2_w = ab2_class_weights.to(device) if isinstance(ab2_class_weights, torch.Tensor) else None\n",
    "    crit = {\"hs3\": SoftCrossEntropyLoss(ignore_index=-100)}\n",
    "    crit[\"ab2\"] = _ce(ab2_w) if ab2_mode == \"ce\" else nn.BCEWithLogitsLoss(pos_weight=ab2_w[1:2] if ab2_w is not None else None)\n",
    "    return crit\n",
    "\n",
    "def build_scheduler(optimizer, train_loader_len, cfg):\n",
    "    updates_per_epoch = max(1, math.ceil(train_loader_len / cfg.grad_accum_steps))\n",
    "    total_updates = updates_per_epoch * cfg.epochs\n",
    "    warmup_updates = max(1, int(cfg.warmup_ratio * total_updates))\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_updates:\n",
    "            return float(step) / float(max(1, warmup_updates))\n",
    "        progress = (step - warmup_updates) / float(max(1, total_updates - warmup_updates))\n",
    "        progress = min(1.0, max(0.0, progress))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda), total_updates\n",
    "\n",
    "# ===========================\n",
    "# Tokenizers / processors\n",
    "# ===========================\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name, use_fast=True)\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(cfg.clip_vision_model) if cfg.use_clip_vision else None\n",
    "img_tfm_fallback = transforms.Compose([\n",
    "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ===========================\n",
    "# Dataset (fused 3-class labels)\n",
    "# ===========================\n",
    "class OptimizedProcessedCSVSet(Dataset):\n",
    "    def __init__(self, csv_path: Path, is_memotion=False, split: Optional[str]=None,\n",
    "                 sample_n: Optional[int]=None, pre_tokenize: Optional[bool]=None):\n",
    "        self.is_memotion = is_memotion\n",
    "        self.split = split\n",
    "        self.pre_tokenize = cfg.pre_tokenize_text if pre_tokenize is None else pre_tokenize\n",
    "\n",
    "        hdr = pd.read_csv(csv_path, nrows=0).columns\n",
    "        if is_memotion:\n",
    "            want = [\"text\", \"image_path\", \"offensive_category\", \"sarcasm\", \"humour\"]\n",
    "        else:\n",
    "            want = [\"text\", \"original_class\", \"label\"]\n",
    "        usecols = [c for c in want if c in hdr]\n",
    "        df = pd.read_csv(csv_path, usecols=usecols or None).fillna(\"\")\n",
    "\n",
    "        if is_memotion and split in {\"train\",\"val\",\"test\"}:\n",
    "            rng = np.random.RandomState(123)\n",
    "            idx = np.arange(len(df)); rng.shuffle(idx)\n",
    "            n = len(idx); n_train=int(0.8*n); n_val=int(0.1*n)\n",
    "            part = {\"train\": idx[:n_train], \"val\": idx[n_train:n_train+n_val], \"test\": idx[n_train+n_val:]}[split]\n",
    "            df = df.iloc[part].reset_index(drop=True)\n",
    "\n",
    "        if \"text\" in df.columns and 3 > 0:\n",
    "            df = df[df[\"text\"].astype(str).str.len() >= 3]\n",
    "\n",
    "        if sample_n is not None and len(df) > sample_n:\n",
    "            df = df.sample(n=int(sample_n), random_state=cfg.seed, replace=False).reset_index(drop=True)\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "        # Fused labels (0=Abusive, 1=Offensive, 2=Non-abusive)\n",
    "        self.labels = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            if is_memotion:\n",
    "                hs3, ab2 = map_memotion_labels(row.get(\"offensive_category\",\"\"))\n",
    "            else:\n",
    "                hs3, ab2 = map_text_labels(row)\n",
    "            fused_hs3 = hs3\n",
    "            if fused_hs3 == -100 and ab2 != -100:\n",
    "                fused_hs3 = 2 if ab2 == 0 else 1\n",
    "            self.labels.append((fused_hs3, ab2))\n",
    "\n",
    "        self.use_half = torch.cuda.is_available() and cfg.mixed_precision\n",
    "\n",
    "        self.enc_input_ids, self.enc_attention_mask = None, None\n",
    "        if self.pre_tokenize and len(self.df) > 0:\n",
    "            texts = self.df[\"text\"].astype(str).tolist()\n",
    "            enc = text_tokenizer(texts, truncation=True, max_length=cfg.max_len, padding=False)\n",
    "            self.enc_input_ids, self.enc_attention_mask = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        text = str(row.get(\"text\",\"\")).strip()\n",
    "\n",
    "        if self.enc_input_ids is not None:\n",
    "            input_ids = self.enc_input_ids[i]; attention_mask = self.enc_attention_mask[i]\n",
    "        else:\n",
    "            enc = text_tokenizer(text, truncation=True, max_length=cfg.max_len, padding=False)\n",
    "            input_ids, attention_mask = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "        img = None; has_image = False\n",
    "        if self.is_memotion:\n",
    "            fname = Path(str(row.get(\"image_path\",\"\")).strip()).name\n",
    "            if fname:\n",
    "                img_root = Path(cfg.base_dir) / cfg.processed_dir_name / \"memotion_dataset_7k\" / \"images\"\n",
    "                p = img_root / fname\n",
    "                if p.exists():\n",
    "                    try:\n",
    "                        pil = Image.open(p).convert(\"RGB\")\n",
    "                        if cfg.use_clip_vision:\n",
    "                            img = clip_processor(images=pil, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "                        else:\n",
    "                            img = img_tfm_fallback(pil)\n",
    "                        has_image = True\n",
    "                    except Exception:\n",
    "                        has_image = False\n",
    "\n",
    "        if img is None:\n",
    "            img = torch.zeros(3, cfg.image_size, cfg.image_size, dtype=torch.float32)\n",
    "        if self.use_half: img = img.half()\n",
    "\n",
    "        sarcasm = parse_sarcasm(row.get(\"sarcasm\",\"\")) if self.is_memotion and \"sarcasm\" in row else 0\n",
    "        humour  = parse_humour(row.get(\"humour\",\"\"))   if self.is_memotion and \"humour\" in row else 0\n",
    "        meta = torch.tensor([sarcasm, humour], dtype=torch.float16 if self.use_half else torch.float32)\n",
    "\n",
    "        hs3, ab2 = self.labels[i]\n",
    "        hs3_target = one_hot_smooth(int(hs3), num_classes=3, eps=cfg.label_smoothing)\n",
    "        if self.use_half: hs3_target = hs3_target.half()\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"input_ids\": input_ids, \"attention_mask\": attention_mask,\n",
    "            \"image\": img, \"has_image\": torch.tensor(has_image, dtype=torch.bool),\n",
    "            \"meta\": meta,\n",
    "            \"hs3_label\": torch.tensor(hs3, dtype=torch.long),\n",
    "            \"hs3_target\": hs3_target,\n",
    "            \"ab2_label\": torch.tensor(ab2, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# ===========================\n",
    "# Model with LoRA, mean pooling, projections, multimodal gate\n",
    "# ===========================\n",
    "class OptimizedMultiModalGated(nn.Module):\n",
    "    def __init__(self, text_model_name, freeze_text=True, use_clip=True, clip_model_name=None,\n",
    "                 fallback_backbone=\"mobilenet_v2\", freeze_vision=True,\n",
    "                 use_multi_task=True, ab2_mode=\"ce\", hidden=256, dropout=0.25, add_meta_dim=2,\n",
    "                 gradient_checkpointing=True, cfg_loaded=None):\n",
    "        super().__init__()\n",
    "        self.use_multi_task = use_multi_task\n",
    "        self.ab2_mode = ab2_mode\n",
    "        self.add_meta_dim = add_meta_dim\n",
    "        self.use_clip = use_clip\n",
    "        self.cfg_loaded = cfg_loaded if cfg_loaded is not None else cfg\n",
    "\n",
    "        # Text encoder (remove pooler; enable mean pooling)\n",
    "        txt_cfg = AutoConfig.from_pretrained(text_model_name)\n",
    "        if hasattr(txt_cfg, \"add_pooling_layer\"):\n",
    "            txt_cfg.add_pooling_layer = False\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name, config=txt_cfg)\n",
    "        if gradient_checkpointing and hasattr(self.text_model, \"gradient_checkpointing_enable\"):\n",
    "            try: self.text_model.gradient_checkpointing_enable()\n",
    "            except: pass\n",
    "        tdim = self.text_model.config.hidden_size\n",
    "        if freeze_text:\n",
    "            for p in self.text_model.parameters(): p.requires_grad = False\n",
    "\n",
    "        # LoRA on text\n",
    "        if getattr(self.cfg_loaded, \"use_lora_text\", False):\n",
    "            targets = getattr(self.cfg_loaded, \"lora_target_text\", None) or [\"query\",\"value\"]\n",
    "            lora_text_cfg = LoraConfig(\n",
    "                r=getattr(self.cfg_loaded, \"lora_r_text\", 8),\n",
    "                lora_alpha=getattr(self.cfg_loaded, \"lora_alpha_text\", 16),\n",
    "                lora_dropout=getattr(self.cfg_loaded, \"lora_dropout_text\", 0.1),\n",
    "                target_modules=targets,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.FEATURE_EXTRACTION\n",
    "            )\n",
    "            self.text_model = get_peft_model(self.text_model, lora_text_cfg)\n",
    "\n",
    "        # Vision encoder\n",
    "        if use_clip:\n",
    "            self.vision = CLIPVisionModel.from_pretrained(clip_model_name or \"openai/clip-vit-base-patch32\")\n",
    "            idim = self.vision.config.hidden_size\n",
    "            if freeze_vision:\n",
    "                for p in self.vision.parameters(): p.requires_grad = False\n",
    "            if getattr(self.cfg_loaded, \"use_lora_vision\", False):\n",
    "                vtargets = getattr(self.cfg_loaded, \"lora_target_vision\", None) or [\"q_proj\",\"v_proj\"]\n",
    "                lora_vis_cfg = LoraConfig(\n",
    "                    r=getattr(self.cfg_loaded, \"lora_r_vision\", 4),\n",
    "                    lora_alpha=getattr(self.cfg_loaded, \"lora_alpha_vision\", 8),\n",
    "                    lora_dropout=getattr(self.cfg_loaded, \"lora_dropout_vision\", 0.05),\n",
    "                    target_modules=vtargets,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.FEATURE_EXTRACTION\n",
    "                )\n",
    "                self.vision = get_peft_model(self.vision, lora_vis_cfg)\n",
    "            self.use_clip = True\n",
    "        else:\n",
    "            if fallback_backbone.lower() == \"mobilenet_v2\":\n",
    "                try:\n",
    "                    from torchvision.models import MobileNet_V2_Weights\n",
    "                    im = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "                except Exception:\n",
    "                    im = models.mobilenet_v2(pretrained=True)\n",
    "                idim = 1280; im.classifier = nn.Identity()\n",
    "            else:\n",
    "                try:\n",
    "                    from torchvision.models import ResNet18_Weights\n",
    "                    backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "                except Exception:\n",
    "                    backbone = models.resnet18(pretrained=True)\n",
    "                idim = 512; im = nn.Sequential(*list(backbone.children())[:-1], nn.Flatten(1))\n",
    "            if freeze_vision:\n",
    "                for p in im.parameters(): p.requires_grad = False\n",
    "            self.vision = im\n",
    "            self.use_clip = False\n",
    "\n",
    "        self.image_feat_dim = idim\n",
    "        proj_dim = getattr(self.cfg_loaded, \"proj_dim\", 256)\n",
    "\n",
    "        # Projection layers to align scales\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(tdim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.i_proj = nn.Sequential(\n",
    "            nn.Linear(idim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Backbones\n",
    "        fusion_in = proj_dim + proj_dim + (add_meta_dim if add_meta_dim>0 else 0)\n",
    "        text_in   = proj_dim + (add_meta_dim if add_meta_dim>0 else 0)\n",
    "\n",
    "        self.fusion_backbone = nn.Sequential(\n",
    "            nn.Linear(fusion_in, hidden), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.GELU(), nn.Dropout(dropout),\n",
    "        )\n",
    "        self.text_backbone = nn.Sequential(\n",
    "            nn.Linear(text_in, hidden), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.GELU(), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Gate over both modalities (+meta)\n",
    "        self.gate_proj = nn.Sequential(\n",
    "            nn.Linear(proj_dim + proj_dim + add_meta_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Heads\n",
    "        self.hs3_fusion_head = nn.Linear(hidden//2, 3)\n",
    "        self.hs3_text_head   = nn.Linear(hidden//2, 3)\n",
    "        self.ab2_fusion_head = nn.Linear(hidden//2, 2)\n",
    "        self.ab2_text_head   = nn.Linear(hidden//2, 2)\n",
    "\n",
    "    def _mean_pool(self, last_hidden_state, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "        return summed / denom\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images, has_image_mask, meta=None):\n",
    "        # Text\n",
    "        out_text = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        t_last = out_text.last_hidden_state\n",
    "        if getattr(self.cfg_loaded, \"text_pooling\", \"mean\") == \"mean\":\n",
    "            t_feat = self._mean_pool(t_last, attention_mask)\n",
    "        else:\n",
    "            t_feat = t_last[:,0,:]\n",
    "        t_proj = self.t_proj(t_feat)\n",
    "\n",
    "        # Meta dtype align\n",
    "        if (meta is not None) and (meta.dtype != t_proj.dtype):\n",
    "            meta = meta.to(t_proj.dtype)\n",
    "\n",
    "        # Vision\n",
    "        B = images.size(0)\n",
    "        img_mask_flat = has_image_mask.view(-1)\n",
    "        if img_mask_flat.any():\n",
    "            idx = torch.nonzero(img_mask_flat, as_tuple=False).squeeze(1)\n",
    "            images_sub = images[idx].to(memory_format=torch.channels_last)\n",
    "            if self.use_clip:\n",
    "                v_out = self.vision(pixel_values=images_sub)\n",
    "                i_sub = v_out.pooler_output\n",
    "            else:\n",
    "                i_sub = self.vision(images_sub)\n",
    "            i_sub = i_sub.to(images_sub.dtype)\n",
    "            i_feat = images.new_zeros((B, self.image_feat_dim), dtype=images_sub.dtype)\n",
    "            i_feat[idx] = i_sub\n",
    "        else:\n",
    "            i_feat = images.new_zeros((B, self.image_feat_dim), dtype=images.dtype)\n",
    "        i_proj = self.i_proj(i_feat)\n",
    "        img_mask = img_mask_flat.float().unsqueeze(1)\n",
    "\n",
    "        # Inputs\n",
    "        if self.add_meta_dim and meta is not None:\n",
    "            t_in = torch.cat([t_proj, meta], dim=1)\n",
    "            f_in = torch.cat([t_proj, i_proj, meta], dim=1)\n",
    "            gate_in = f_in\n",
    "        else:\n",
    "            t_in = t_proj\n",
    "            f_in = torch.cat([t_proj, i_proj], dim=1)\n",
    "            gate_in = f_in\n",
    "\n",
    "        # Reprs\n",
    "        t_repr = self.text_backbone(t_in)\n",
    "        f_repr = self.fusion_backbone(f_in)\n",
    "\n",
    "        # Gate\n",
    "        gate_weight = self.gate_proj(gate_in) * img_mask  # [B,1]\n",
    "\n",
    "        # Heads\n",
    "        hs3_f = self.hs3_fusion_head(f_repr)\n",
    "        hs3_t = self.hs3_text_head(t_repr)\n",
    "        hs3_logits = gate_weight * hs3_f + (1 - gate_weight) * hs3_t\n",
    "\n",
    "        ab2_f = self.ab2_fusion_head(f_repr)\n",
    "        ab2_t = self.ab2_text_head(t_repr)\n",
    "        ab2_logits = gate_weight * ab2_f + (1 - gate_weight) * ab2_t\n",
    "\n",
    "        return {\n",
    "            \"hs3_logits\": hs3_logits,\n",
    "            \"ab2_logits\": ab2_logits,\n",
    "            \"gate_weight\": gate_weight.detach(),\n",
    "            # expose branch-only logits (for cheap image-only loss, ablations)\n",
    "            \"hs3_f_only\": hs3_f,\n",
    "            \"ab2_f_only\": ab2_f,\n",
    "            \"hs3_t_only\": hs3_t,\n",
    "            \"ab2_t_only\": ab2_t,\n",
    "        }\n",
    "\n",
    "# ===========================\n",
    "# Collate / utils\n",
    "# ===========================\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch: return None\n",
    "    pad = text_tokenizer.pad([{\"input_ids\": b[\"input_ids\"], \"attention_mask\": b[\"attention_mask\"]} for b in batch],\n",
    "                             padding=True, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"text\": [b[\"text\"] for b in batch],\n",
    "        \"input_ids\": pad[\"input_ids\"],\n",
    "        \"attention_mask\": pad[\"attention_mask\"],\n",
    "        \"image\": torch.stack([b[\"image\"] for b in batch]),\n",
    "        \"has_image\": torch.stack([b[\"has_image\"] for b in batch]),\n",
    "        \"meta\": torch.stack([b[\"meta\"] for b in batch]),\n",
    "        \"hs3_label\": torch.stack([b[\"hs3_label\"] for b in batch]),\n",
    "        \"hs3_target\": torch.stack([b[\"hs3_target\"] for b in batch]),\n",
    "        \"ab2_label\": torch.stack([b[\"ab2_label\"] for b in batch]),\n",
    "    }\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Mem - Alloc {torch.cuda.memory_allocated()/1024**3:.2f} GB | Reserved {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "\n",
    "# ===========================\n",
    "# Consistency loss\n",
    "# ===========================\n",
    "def hierarchical_consistency_loss(hs3_logits, ab2_logits):\n",
    "    p_hs3 = F.softmax(hs3_logits, dim=-1)\n",
    "    q_ab2 = torch.stack([p_hs3[:,2], (p_hs3[:,0] + p_hs3[:,1])], dim=1).clamp_min(1e-8)\n",
    "    p_ab2 = F.softmax(ab2_logits, dim=-1).clamp_min(1e-8)\n",
    "    return F.kl_div(p_ab2.log(), q_ab2, reduction=\"batchmean\") + F.kl_div(q_ab2.log(), p_ab2, reduction=\"batchmean\")\n",
    "\n",
    "# ===========================\n",
    "# Hard-mining and Memotion sampling\n",
    "# ===========================\n",
    "@torch.no_grad()\n",
    "def score_difficulty_text(model, dataset: Dataset, indices: List[int], batch_size=256):\n",
    "    model.eval()\n",
    "    pool_ds = Subset(dataset, indices)\n",
    "    loader = DataLoader(pool_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=(device==\"cuda\"), collate_fn=collate_fn)\n",
    "    diffs = np.zeros(len(indices), dtype=np.float32); off = 0\n",
    "    for batch in tqdm(loader, desc=\"Scoring hardness\", leave=False):\n",
    "        ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attn = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "        has_img = batch[\"has_image\"].to(device, non_blocking=True)\n",
    "        meta = batch[\"meta\"].to(device, non_blocking=True) if cfg.add_meta_dim > 0 else None\n",
    "        y_hs3 = batch[\"hs3_label\"].to(device, non_blocking=True)\n",
    "        y_ab2 = batch[\"ab2_label\"].to(device, non_blocking=True)\n",
    "        with autocast(enabled=cfg.mixed_precision, dtype=torch.float16):\n",
    "            out = model(ids, attn, imgs, has_img, meta)\n",
    "            hs3_log = out[\"hs3_logits\"]; ab2_log = out[\"ab2_logits\"]\n",
    "            hs3_ce = torch.zeros(ids.size(0), device=device)\n",
    "            m_h = (y_hs3 != -100)\n",
    "            if m_h.any():\n",
    "                hs3_ce[m_h] = F.cross_entropy(hs3_log[m_h], y_hs3[m_h], reduction=\"none\")\n",
    "            ab2_ce = torch.zeros(ids.size(0), device=device)\n",
    "            m_a = (y_ab2 != -100)\n",
    "            if m_a.any():\n",
    "                ab2_ce[m_a] = F.cross_entropy(ab2_log[m_a], y_ab2[m_a], reduction=\"none\")\n",
    "            diff = (hs3_ce + ab2_ce).detach().float().cpu().numpy()\n",
    "        diffs[off:off+len(diff)] = diff; off += len(diff)\n",
    "    return indices, diffs\n",
    "\n",
    "def select_hard_examples(model, full_text_ds: Dataset, pool_size: int, select_n: int, hard_frac: float, epoch: int):\n",
    "    N = len(full_text_ds); pool_size = min(pool_size, N)\n",
    "    rng_pool = np.random.RandomState(cfg.seed + 1009*epoch)\n",
    "    pool_idx = rng_pool.choice(N, size=pool_size, replace=False).tolist()\n",
    "    _, diffs = score_difficulty_text(model, full_text_ds, pool_idx, batch_size=256)\n",
    "    k_hard = int(select_n * hard_frac)\n",
    "    hard_idx_rel = np.argsort(-diffs)[:k_hard]; hard_idx = [pool_idx[i] for i in hard_idx_rel]\n",
    "    remaining = select_n - k_hard\n",
    "    remaining_candidates = [i for i in range(N) if i not in set(hard_idx)]\n",
    "    rng_rest = np.random.RandomState(cfg.seed + 2027*epoch)\n",
    "    rand_rel = rng_rest.choice(len(remaining_candidates), size=remaining, replace=False)\n",
    "    rand_idx = [remaining_candidates[i] for i in rand_rel]\n",
    "    return hard_idx + rand_idx\n",
    "\n",
    "def get_memotion_valid_indices(ds_memo: OptimizedProcessedCSVSet):\n",
    "    valid = []\n",
    "    img_root = Path(cfg.base_dir) / cfg.processed_dir_name / \"memotion_dataset_7k\" / \"images\"\n",
    "    for i, row in ds_memo.df.iterrows():\n",
    "        fname = Path(str(row.get(\"image_path\",\"\")).strip()).name\n",
    "        if fname and (img_root / fname).exists():\n",
    "            valid.append(i)\n",
    "    return valid\n",
    "\n",
    "def sample_memotion_indices(valid_idx: List[int], n: int, epoch: int):\n",
    "    n = min(n, len(valid_idx))\n",
    "    rng = np.random.RandomState(cfg.seed + 707*epoch)\n",
    "    sel = rng.choice(valid_idx, size=n, replace=False).tolist()\n",
    "    return sel\n",
    "\n",
    "# ===========================\n",
    "# Training\n",
    "# ===========================\n",
    "def train_novel():\n",
    "    clear_memory()\n",
    "    processed_dir = Path(cfg.base_dir) / cfg.processed_dir_name\n",
    "    splits_dir = processed_dir / cfg.splits_dirname\n",
    "    reports_dir = processed_dir / cfg.reports_dirname\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    memotion_path = processed_dir / cfg.memotion_csv\n",
    "    assert memotion_path.exists(), f\"Missing memotion CSV: {memotion_path}\"\n",
    "    train_text_csv = splits_dir / cfg.text_train_csv\n",
    "    val_text_csv   = splits_dir / cfg.text_val_csv\n",
    "    test_text_csv  = splits_dir / cfg.text_test_csv\n",
    "    for p in [train_text_csv, val_text_csv, test_text_csv]:\n",
    "        assert p.exists(), f\"Missing: {p}\"\n",
    "\n",
    "    # Datasets\n",
    "    ds_train_text_full = OptimizedProcessedCSVSet(train_text_csv, is_memotion=False, sample_n=None, pre_tokenize=False)\n",
    "    ds_val_text   = OptimizedProcessedCSVSet(val_text_csv,   is_memotion=False, sample_n=cfg.sample_val_text_n, pre_tokenize=True)\n",
    "    ds_test_text  = OptimizedProcessedCSVSet(test_text_csv,  is_memotion=False, sample_n=cfg.sample_test_text_n, pre_tokenize=True)\n",
    "\n",
    "    ds_val_memo   = OptimizedProcessedCSVSet(memotion_path, is_memotion=True, split=\"val\",  sample_n=cfg.sample_val_memotion_n, pre_tokenize=True)\n",
    "    ds_test_memo  = OptimizedProcessedCSVSet(memotion_path, is_memotion=True, split=\"test\", sample_n=cfg.sample_test_memotion_n, pre_tokenize=True)\n",
    "    ds_train_memo_full = OptimizedProcessedCSVSet(memotion_path, is_memotion=True, split=\"train\", sample_n=None, pre_tokenize=True)\n",
    "\n",
    "    memotion_valid_idx = get_memotion_valid_indices(ds_train_memo_full)\n",
    "\n",
    "    # Initial subsets\n",
    "    text_init_size = min(cfg.train_text_per_epoch, len(ds_train_text_full))\n",
    "    init_text_idx = np.random.RandomState(cfg.seed).choice(len(ds_train_text_full), size=text_init_size, replace=False)\n",
    "    ds_train_text_epoch = Subset(ds_train_text_full, init_text_idx.tolist())\n",
    "    memo_init_idx = sample_memotion_indices(memotion_valid_idx, cfg.images_per_epoch, epoch=0)\n",
    "    ds_train_memo_epoch = Subset(ds_train_memo_full, memo_init_idx)\n",
    "\n",
    "    train_ds = ConcatDataset([ds_train_text_epoch, ds_train_memo_epoch])\n",
    "    val_ds   = ConcatDataset([ds_val_text, ds_val_memo])\n",
    "    test_ds  = ConcatDataset([ds_test_text, ds_test_memo])\n",
    "\n",
    "    # Dataloaders\n",
    "    cpu_ct = os.cpu_count() or 2\n",
    "    num_workers = min(8, max(2, cpu_ct // 2))\n",
    "    dl_kwargs = dict(num_workers=num_workers, collate_fn=collate_fn, pin_memory=(device==\"cuda\"), persistent_workers=True, prefetch_factor=4)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, **dl_kwargs)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, **dl_kwargs)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, **dl_kwargs)\n",
    "\n",
    "    # Model\n",
    "    model = OptimizedMultiModalGated(\n",
    "        text_model_name=cfg.text_model_name,\n",
    "        freeze_text=cfg.freeze_text_encoder,\n",
    "        use_clip=cfg.use_clip_vision,\n",
    "        clip_model_name=cfg.clip_vision_model,\n",
    "        fallback_backbone=cfg.image_backbone_fallback,\n",
    "        freeze_vision=cfg.freeze_vision_encoder,\n",
    "        use_multi_task=cfg.use_multi_task,\n",
    "        ab2_mode=cfg.ab2_mode,\n",
    "        hidden=cfg.hidden,\n",
    "        dropout=cfg.dropout,\n",
    "        add_meta_dim=cfg.add_meta_dim,\n",
    "        gradient_checkpointing=cfg.gradient_checkpointing\n",
    "    ).to(device)\n",
    "    if device == \"cuda\":\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    if cfg.use_torch_compile and hasattr(torch, \"compile\"):\n",
    "        try:\n",
    "            model = torch.compile(model); print(\"torch.compile enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"torch.compile skipped: {e}\")\n",
    "\n",
    "    # Optimizer\n",
    "    params_to_train = [p for p in model.parameters() if p.requires_grad]\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(params_to_train, lr=cfg.lr, weight_decay=cfg.weight_decay, fused=True)\n",
    "    except TypeError:\n",
    "        optimizer = torch.optim.AdamW(params_to_train, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # Class weights\n",
    "    def counts_from_concat(text_subset: Dataset, memo_subset: Dataset, label_pos, n_classes):\n",
    "        counts = np.zeros(n_classes, dtype=np.int64)\n",
    "        for i in range(len(text_subset)):\n",
    "            idx = text_subset.indices[i] if isinstance(text_subset, Subset) else i\n",
    "            y = ds_train_text_full.labels[idx][label_pos]\n",
    "            if y != -100: counts[y] += 1\n",
    "        for i in range(len(memo_subset)):\n",
    "            idx = memo_subset.indices[i] if isinstance(memo_subset, Subset) else i\n",
    "            y_h, y_a = ds_train_memo_full.labels[idx]\n",
    "            if label_pos == 0 and y_h != -100: counts[y_h] += 1\n",
    "            if label_pos == 1 and y_a != -100: counts[y_a] += 1\n",
    "        return counts\n",
    "\n",
    "    hs3_w = ab2_w = None\n",
    "    if cfg.use_class_weights:\n",
    "        hs3_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=0, n_classes=3)\n",
    "        ab2_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=1, n_classes=2)\n",
    "        hs3_w = compute_weights_from_counts(hs3_counts); ab2_w = compute_weights_from_counts(ab2_counts)\n",
    "    crit = build_losses(cfg.ab2_mode, hs3_w, ab2_w, device=device, label_smoothing=cfg.label_smoothing)\n",
    "    scheduler, total_updates = build_scheduler(optimizer, len(train_loader), cfg)\n",
    "\n",
    "    print(\"Training started\")\n",
    "    print(f\"Steps/epoch: {len(train_loader)}\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "    best_score = -1.0; patience = 0; history = []\n",
    "    processed_dir = Path(cfg.base_dir) / cfg.processed_dir_name\n",
    "\n",
    "    def unwrap_for_save(m):\n",
    "        if hasattr(m, \"_orig_mod\"): m = m._orig_mod\n",
    "        if hasattr(m, \"module\"): m = m.module\n",
    "        return m\n",
    "\n",
    "    for epoch in range(1, cfg.epochs+1):\n",
    "        if cfg.hard_mining:\n",
    "            print(f\"\\nHard-mining epoch {epoch}: pool {cfg.pool_size} -> select {cfg.train_text_per_epoch} (hard_frac={cfg.hard_frac})\")\n",
    "            with torch.no_grad():\n",
    "                sel_text_idx = select_hard_examples(model, ds_train_text_full, cfg.pool_size, cfg.train_text_per_epoch, cfg.hard_frac, epoch)\n",
    "            ds_train_text_epoch = Subset(ds_train_text_full, sel_text_idx)\n",
    "\n",
    "        sel_memo_idx = sample_memotion_indices(get_memotion_valid_indices(ds_train_memo_full), cfg.images_per_epoch, epoch)\n",
    "        ds_train_memo_epoch = Subset(ds_train_memo_full, sel_memo_idx)\n",
    "\n",
    "        train_ds = ConcatDataset([ds_train_text_epoch, ds_train_memo_epoch])\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, **dl_kwargs)\n",
    "\n",
    "        if cfg.use_class_weights:\n",
    "            hs3_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=0, n_classes=3)\n",
    "            ab2_counts = counts_from_concat(ds_train_text_epoch, ds_train_memo_epoch, label_pos=1, n_classes=2)\n",
    "            hs3_w = compute_weights_from_counts(hs3_counts); ab2_w = compute_weights_from_counts(ab2_counts)\n",
    "            crit = build_losses(cfg.ab2_mode, hs3_w, ab2_w, device=device, label_smoothing=cfg.label_smoothing)\n",
    "\n",
    "        print(f\"Epoch {epoch} train sizes — text: {len(ds_train_text_epoch)}, memotion: {len(ds_train_memo_epoch)}\")\n",
    "\n",
    "        model.train(); running_loss = 0.0; total_batches = 0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}/{cfg.epochs}\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for it, batch in pbar:\n",
    "            if batch is None: continue\n",
    "            ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attn = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "            has_img = batch[\"has_image\"].to(device, non_blocking=True)\n",
    "            meta = batch[\"meta\"].to(device, non_blocking=True) if cfg.add_meta_dim > 0 else None\n",
    "            y_ab2 = batch[\"ab2_label\"].to(device, non_blocking=True)\n",
    "            hs3_target = batch[\"hs3_target\"].to(device, non_blocking=True)\n",
    "\n",
    "            # Modality dropout for text (forces image reliance sometimes)\n",
    "            if cfg.text_dropout_prob > 0 and random.random() < cfg.text_dropout_prob:\n",
    "                ids = ids.clone()\n",
    "                ids[:] = text_tokenizer.pad_token_id\n",
    "                attn = torch.zeros_like(attn)\n",
    "\n",
    "            with autocast(enabled=cfg.mixed_precision, dtype=torch.float16):\n",
    "                out = model(ids, attn, imgs, has_img, meta)\n",
    "                hs3_loss = crit[\"hs3\"](out[\"hs3_logits\"], hs3_target)\n",
    "                ab2_loss = crit[\"ab2\"](out[\"ab2_logits\"], y_ab2)\n",
    "                cons_loss = hierarchical_consistency_loss(out[\"hs3_logits\"], out[\"ab2_logits\"]) * cfg.lambda_consistency\n",
    "\n",
    "                # Cheap image-only aux loss (use fusion-only logits on samples with images)\n",
    "                img_mask = has_img.view(-1)\n",
    "                img_only_loss = imgs.new_tensor(0.0)\n",
    "                if cfg.image_only_loss_w > 0 and img_mask.any():\n",
    "                    img_hs3 = crit[\"hs3\"](out[\"hs3_f_only\"][img_mask], hs3_target[img_mask])\n",
    "                    img_ab2 = crit[\"ab2\"](out[\"ab2_f_only\"][img_mask], y_ab2[img_mask])\n",
    "                    img_only_loss = cfg.image_only_loss_w * (img_hs3 + img_ab2)\n",
    "\n",
    "                loss = (hs3_loss + ab2_loss + cons_loss + img_only_loss) / cfg.grad_accum_steps\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            step_now = ((it + 1) % cfg.grad_accum_steps == 0) or (it + 1 == len(train_loader))\n",
    "            if step_now:\n",
    "                if cfg.grad_clip:\n",
    "                    if scaler.is_enabled(): scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(params_to_train, cfg.grad_clip)\n",
    "                if scaler.is_enabled():\n",
    "                    scaler.step(optimizer); scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += float(loss.item()) * cfg.grad_accum_steps\n",
    "            total_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{running_loss/total_batches:.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "            del out, loss, hs3_loss, ab2_loss, cons_loss, img_only_loss\n",
    "\n",
    "        avg_train_loss = running_loss / max(1, total_batches)\n",
    "        current_score = -avg_train_loss\n",
    "        history.append({'epoch': epoch, 'train_loss': avg_train_loss, 'val_score': current_score})\n",
    "        print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Score={current_score:.4f}, Best={best_score:.4f}\")\n",
    "        print_gpu_memory()\n",
    "\n",
    "        base_model = unwrap_for_save(model)\n",
    "        ckpt = {\n",
    "            \"model\": base_model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if scaler.is_enabled() else None,\n",
    "            \"epoch\": epoch,\n",
    "            \"best_score\": max(best_score, current_score),\n",
    "            \"cfg\": cfg.__dict__,\n",
    "            \"history\": history\n",
    "        }\n",
    "\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score; patience = 0\n",
    "            torch.save(ckpt, processed_dir / cfg.checkpoint_name)\n",
    "            print(\"New best model saved!\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            print(f\"No improvement. Patience {patience}/{cfg.early_stop_patience}\")\n",
    "            if patience >= cfg.early_stop_patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "        clear_memory()\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best Val Score: {best_score:.4f} | Reports dir: {reports_dir}\")\n",
    "    return True\n",
    "\n",
    "# ===========================\n",
    "# Fused inference helpers (direct 3-class)\n",
    "# ===========================\n",
    "def _clean_state_dict_keys(sd):\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        if k.startswith(\"_orig_mod.\"): k = k[len(\"_orig_mod.\"):]\n",
    "        if k.startswith(\"module.\"):    k = k[len(\"module.\"):]\n",
    "        out[k] = v\n",
    "    return out\n",
    "\n",
    "def fused_label_names():\n",
    "    # 0=Abusive, 1=Offensive, 2=Non-abusive\n",
    "    return [\"Abusive\",\"Offensive\",\"Non-abusive\"]\n",
    "\n",
    "# ===========================\n",
    "# Eval + Insights (direct 3-class)\n",
    "# ===========================\n",
    "def load_model_and_processors_lora(ckpt_path, device=\"cpu\"):\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    assert \"cfg\" in ckpt, \"Checkpoint missing cfg.\"\n",
    "\n",
    "    class C: pass\n",
    "    cfg_loaded = C()\n",
    "    for k, v in ckpt[\"cfg\"].items():\n",
    "        setattr(cfg_loaded, k, v)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg_loaded.text_model_name, use_fast=True)\n",
    "    clip_proc = CLIPImageProcessor.from_pretrained(cfg_loaded.clip_vision_model) if getattr(cfg_loaded, \"use_clip_vision\", True) else None\n",
    "\n",
    "    model = OptimizedMultiModalGated(\n",
    "        text_model_name=cfg_loaded.text_model_name,\n",
    "        freeze_text=getattr(cfg_loaded, \"freeze_text_encoder\", True),\n",
    "        use_clip=getattr(cfg_loaded, \"use_clip_vision\", True),\n",
    "        clip_model_name=getattr(cfg_loaded, \"clip_vision_model\", \"openai/clip-vit-base-patch32\"),\n",
    "        fallback_backbone=getattr(cfg_loaded, \"image_backbone_fallback\", \"mobilenet_v2\"),\n",
    "        freeze_vision=getattr(cfg_loaded, \"freeze_vision_encoder\", True),\n",
    "        use_multi_task=getattr(cfg_loaded, \"use_multi_task\", True),\n",
    "        ab2_mode=getattr(cfg_loaded, \"ab2_mode\", \"ce\"),\n",
    "        hidden=getattr(cfg_loaded, \"hidden\", 256),\n",
    "        dropout=getattr(cfg_loaded, \"dropout\", 0.25),\n",
    "        add_meta_dim=getattr(cfg_loaded, \"add_meta_dim\", 2),\n",
    "        gradient_checkpointing=False,\n",
    "        cfg_loaded=cfg_loaded\n",
    "    ).to(device)\n",
    "\n",
    "    sd = _clean_state_dict_keys(ckpt[\"model\"])\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    if device == \"cuda\":\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "    model.eval()\n",
    "    return model, tokenizer, clip_proc, cfg_loaded\n",
    "\n",
    "def build_test_loader_for_fused(splits_dir, batch_size, sample_n=None):\n",
    "    test_text_csv = Path(splits_dir) / \"test/text_test.csv\"\n",
    "    memotion_csv  = Path(cfg.base_dir) / cfg.processed_dir_name / cfg.memotion_csv\n",
    "    ds_test_text = OptimizedProcessedCSVSet(test_text_csv, is_memotion=False, sample_n=sample_n, pre_tokenize=True)\n",
    "    ds_test_memo = OptimizedProcessedCSVSet(memotion_csv, is_memotion=True, split=\"test\", sample_n=cfg.sample_test_memotion_n, pre_tokenize=True)\n",
    "    test_ds = ConcatDataset([ds_test_text, ds_test_memo])\n",
    "    loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=2, collate_fn=collate_fn, pin_memory=(device==\"cuda\"))\n",
    "    return loader\n",
    "\n",
    "def evaluate_fused_on_loader(model, loader, amp=True):\n",
    "    model.eval()\n",
    "    all_y, all_p, all_probs = [], [], []\n",
    "    names = fused_label_names()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, desc=\"Fused eval (direct 3-class)\"):\n",
    "            ids = batch[\"input_ids\"].to(device); attn = batch[\"attention_mask\"].to(device)\n",
    "            imgs = batch[\"image\"].to(device); has_img = batch[\"has_image\"].to(device)\n",
    "            meta = batch[\"meta\"].to(device) if \"meta\" in batch else None\n",
    "            y_hs3 = batch[\"hs3_label\"].to(device)\n",
    "\n",
    "            if amp: imgs = imgs.half()\n",
    "            with torch.cuda.amp.autocast(enabled=amp, dtype=torch.float16):\n",
    "                out = model(ids, attn, imgs, has_img, meta=meta)\n",
    "                logits = out[\"hs3_logits\"]\n",
    "                probs  = torch.softmax(logits, dim=-1)\n",
    "                pred   = probs.argmax(dim=1)\n",
    "\n",
    "            y_np = y_hs3.detach().cpu().numpy()\n",
    "            mask = (y_np != -100)\n",
    "\n",
    "            if np.any(mask):\n",
    "                all_y.append(y_np[mask])\n",
    "                all_p.append(pred.detach().cpu().numpy()[mask])\n",
    "                all_probs.append(probs.detach().cpu().numpy()[mask])\n",
    "\n",
    "    if not all_y:\n",
    "        print(\"No valid fused labels found in eval.\")\n",
    "        return None\n",
    "\n",
    "    y = np.concatenate(all_y)\n",
    "    p = np.concatenate(all_p)\n",
    "    probs = np.concatenate(all_probs)\n",
    "\n",
    "    print(\"\\nFused report (direct 3-class head):\")\n",
    "    print(classification_report(y, p, target_names=names, zero_division=0))\n",
    "\n",
    "    cm = confusion_matrix(y, p, labels=[0,1,2])\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=names, yticklabels=names)\n",
    "    plt.title(\"Fused Confusion Matrix (Direct)\"); plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"fused_cm.png\"), dpi=150); plt.close()\n",
    "\n",
    "    return {\"y\": y, \"p\": p, \"probs\": probs}\n",
    "\n",
    "def gate_analysis(loader, model, amp=True):\n",
    "    model.eval()\n",
    "    gate_vals = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, desc=\"Gate analysis\"):\n",
    "            ids = batch[\"input_ids\"].to(device); attn = batch[\"attention_mask\"].to(device)\n",
    "            imgs = batch[\"image\"].to(device); has_img = batch[\"has_image\"].to(device)\n",
    "            meta = batch[\"meta\"].to(device) if \"meta\" in batch else None\n",
    "            if amp: imgs = imgs.half()\n",
    "            with torch.cuda.amp.autocast(enabled=amp, dtype=torch.float16):\n",
    "                out = model(ids, attn, imgs, has_img, meta=meta)\n",
    "            gw = out[\"gate_weight\"].detach().cpu().numpy().reshape(-1)\n",
    "            him = has_img.cpu().numpy().reshape(-1)\n",
    "            gate_vals.extend(list(gw[him==1]))\n",
    "    gate_vals = np.array(gate_vals)\n",
    "    if len(gate_vals):\n",
    "        print(f\"Image-sample count: {len(gate_vals)} | gate mean={gate_vals.mean():.3f} | median={np.median(gate_vals):.3f}\")\n",
    "    return gate_vals\n",
    "\n",
    "def precision_recall_thresholds(y_true_fused, fused_probs, target_precision=0.90):\n",
    "    # Abusive binary: class 0 or 1 -> abusive (1), class 2 -> non (0)\n",
    "    y_ab = (y_true_fused != 2).astype(int)\n",
    "    p_ab = fused_probs[:,0] + fused_probs[:,1]\n",
    "    ap = average_precision_score(y_ab, p_ab)\n",
    "    print(f\"Average precision (abusive): {ap:.4f}\")\n",
    "    prec, rec, thr = precision_recall_curve(y_ab, p_ab)\n",
    "    best = None\n",
    "    for i in range(len(prec)):\n",
    "        if prec[i] >= target_precision:\n",
    "            best = (thr[i-1] if i > 0 else 0.5, prec[i], rec[i]); break\n",
    "    if best:\n",
    "        th, bp, br = best\n",
    "        print(f\"Threshold @P>={target_precision}: th={th:.3f}, P={bp:.3f}, R={br:.3f}\")\n",
    "    return ap, prec, rec, thr\n",
    "\n",
    "def expected_calibration_error(y_true_abusive, y_prob, n_bins=15):\n",
    "    bins = np.linspace(0., 1., n_bins+1)\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        idx = binids == b\n",
    "        if np.any(idx):\n",
    "            conf = y_prob[idx].mean()\n",
    "            acc = (y_true_abusive[idx] == (y_prob[idx] >= 0.5)).mean()\n",
    "            ece += np.abs(acc - conf) * np.sum(idx) / len(y_prob)\n",
    "    return ece\n",
    "\n",
    "# ===========================\n",
    "# Main flow\n",
    "# ===========================\n",
    "# 1) Train (optional)\n",
    "if RUN_TRAIN:\n",
    "    print(\"Training LoRA + hard-mined model (with image-only aux loss + text dropout)...\")\n",
    "    train_novel()\n",
    "else:\n",
    "    print(\"Skipping training (RUN_TRAIN=False)\")\n",
    "\n",
    "# 2) Load model for eval\n",
    "print(\"Loading LoRA model for direct 3-class evaluation...\")\n",
    "model, tok, clip_proc, cfg_loaded = load_model_and_processors_lora(CKPT_PATH, device=device)\n",
    "\n",
    "# 3) Build test loader for fused eval (full or sample)\n",
    "test_loader = build_test_loader_for_fused(SPLITS_DIR, batch_size=BATCH_EVAL, sample_n=TEACHER_N)\n",
    "\n",
    "# 4) Run direct 3-class eval\n",
    "if RUN_FUSED_EVAL:\n",
    "    fused_res = evaluate_fused_on_loader(model, test_loader, amp=torch.cuda.is_available())\n",
    "    if fused_res is None:\n",
    "        print(\"No fused results computed.\")\n",
    "    else:\n",
    "        y = fused_res[\"y\"]; p = fused_res[\"p\"]; probs = fused_res[\"probs\"]\n",
    "        names = fused_label_names()\n",
    "\n",
    "        # Insights: Precision/Recall thresholds for abusive (class 0 or 1)\n",
    "        y_ab = (y != 2).astype(int)\n",
    "        p_ab = probs[:,0] + probs[:,1]\n",
    "        ap, prec, rec, thr = precision_recall_thresholds(y, probs, target_precision=0.90)\n",
    "        ece = expected_calibration_error(y_ab, p_ab, n_bins=15)\n",
    "        print(f\"ECE (abusive): {ece:.3f}\")\n",
    "\n",
    "        # Plot PR curve\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.plot(rec, prec, label=f'PR (AP={ap:.3f})')\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Abusive PR Curve (Direct 3-class)\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"abusive_pr_curve.png\"), dpi=150); plt.close()\n",
    "\n",
    "        # Export eval predictions table (with probs)\n",
    "        df_pred = pd.DataFrame({\n",
    "            \"y_true\": y,\n",
    "            \"y_pred\": p,\n",
    "            \"p_abusive\": probs[:,0],\n",
    "            \"p_offensive\": probs[:,1],\n",
    "            \"p_non_abusive\": probs[:,2]\n",
    "        })\n",
    "        df_pred.to_csv(os.path.join(OUT_DIR, \"fused_predictions_eval.csv\"), index=False)\n",
    "        print(f\"Saved fused_predictions_eval.csv to {OUT_DIR}\")\n",
    "\n",
    "print(\"\\nAll done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc66cd-04e6-4e5b-a463-13f8a0ee2f3c",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "INTERPRETATION OF EVALUATION PIPELINE — DIRECT 3-CLASS MULTIMODAL ASSESSMENT\n",
    "\n",
    "The end-to-end evaluation pipeline provides comprehensive insights into multimodal\n",
    "hate detection performance through direct 3-class classification and advanced\n",
    "diagnostic metrics.\n",
    "\n",
    "Key evaluation interpretations:\n",
    "\n",
    "- **Direct 3-Class Framework**: Evaluates models on the fused hierarchy\n",
    "  (0=Abusive, 1=Offensive, 2=Non-abusive) rather than separate binary/ternary\n",
    "  tasks. This reflects real-world deployment where systems must distinguish\n",
    "  between hate speech, offensive content, and acceptable material in one pass.\n",
    "\n",
    "- **Gate Analysis**: Measures the multimodal gate's behavior across image-bearing\n",
    "  samples, revealing how much the model relies on visual vs. textual information.\n",
    "  High gate values indicate visual dominance, while low values show text reliance.\n",
    "  This exposes modality preferences for different types of offensive content.\n",
    "\n",
    "- **Precision-Recall Tradeoffs**: Analyzes abusive content detection at high\n",
    "  precision thresholds (e.g., 90%), crucial for safety-critical applications\n",
    "  where false positives are costly. The threshold analysis identifies optimal\n",
    "  operating points for different deployment scenarios.\n",
    "\n",
    "- **Calibration Assessment**: Computes Expected Calibration Error to measure\n",
    "  how well predicted probabilities match true likelihoods. Poor calibration\n",
    "  indicates overconfident or underconfident predictions, which is critical\n",
    "  for risk assessment and content moderation decisions.\n",
    "\n",
    "- **Comprehensive Export**: Generates prediction tables with per-class\n",
    "  probabilities, enabling detailed error analysis and understanding of\n",
    "  model uncertainty patterns across the offensive content spectrum.\n",
    "\n",
    "Overall interpretation:\n",
    "This evaluation framework moves beyond simple accuracy metrics to provide\n",
    "deployment-ready insights about model behavior, calibration quality, and\n",
    "optimal operating thresholds. The direct 3-class approach combined with\n",
    "modality analysis and probability calibration makes the system suitable\n",
    "for real-world content moderation where nuanced distinctions between\n",
    "abusive, offensive, and acceptable content are essential.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526f4f2-8ffd-41d9-ac67-d34125bc09d5",
   "metadata": {
    "id": "0526f4f2-8ffd-41d9-ac67-d34125bc09d5",
    "outputId": "0366fd2f-8f97-46b6-c4b8-7e89d7bd40c0"
   },
   "outputs": [],
   "source": [
    "# user_test_fused_lora.py\n",
    "# Direct 3-class inference: Abusive / Offensive / Non-abusive\n",
    "# OCR: EasyOCR (default) + PaddleOCR support with safe CPU install and robust parsing\n",
    "# Keys:\n",
    "# - Normalize OCR text (NFKC, strip zero-width/NBSP, standardize punctuation)\n",
    "# - If OCR text exists -> feed OCR text directly; else use user-typed text\n",
    "# - Optionally ignore image when OCR text is used (avoid gate overshadow)\n",
    "# - Mean-pooled text encoder; projection for text/image; gate over both modalities\n",
    "# - Slightly larger inference max_len to reduce truncation\n",
    "\n",
    "# Optional installs:\n",
    "# !pip install -U transformers torchvision pandas numpy pillow peft gradio easyocr\n",
    "\n",
    "import os, sys, gc, subprocess, warnings, re, unicodedata\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Contractions (auto-install if missing)\n",
    "try:\n",
    "    import contractions\n",
    "except ImportError:\n",
    "    print(\"Installing 'contractions'...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"contractions\"])\n",
    "    import contractions\n",
    "\n",
    "# PEFT (LoRA)\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "except ImportError:\n",
    "    print(\"Installing 'peft'...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"peft\"])\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "# ======================\n",
    "# Paths/config\n",
    "# ======================\n",
    "ckpt_path = os.environ.get(\"CKPT_PATH\", \"datasets/processed_data/best_novel.pt\")  # set to your trained checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Demo defaults\n",
    "TEMP = 1.0\n",
    "CLASS_NAMES = [\"Abusive\", \"Offensive\", \"Non-abusive\"]  # 0,1,2\n",
    "\n",
    "# OCR defaults\n",
    "OCR_ENGINE = \"easyocr\"   # 'easyocr' or 'paddle'\n",
    "OCR_MIN_CONF = 0.50\n",
    "OCR_MAX_CHARS = 300\n",
    "\n",
    "# Robust OCR normalization settings\n",
    "OCR_LOWER = True  # keep casing by default (RoBERTa-like models are cased)\n",
    "INFER_MAX_LEN = int(os.environ.get(\"INFER_MAX_LEN\", \"128\"))  # slightly > training max_len\n",
    "FORCE_TEXT_ONLY_WHEN_OCR = True  # ignore image when OCR text available (to avoid gate overshadow)\n",
    "\n",
    "# ======================\n",
    "# Model class (mean pooling + projections + multimodal gate + LoRA)\n",
    "# ======================\n",
    "class OptimizedMultiModalGated(nn.Module):\n",
    "    def __init__(self, text_model_name, freeze_text=True, use_clip=True, clip_model_name=None,\n",
    "                 fallback_backbone=\"mobilenet_v2\", freeze_vision=True,\n",
    "                 use_multi_task=True, ab2_mode=\"ce\", hidden=256, dropout=0.25, add_meta_dim=2,\n",
    "                 gradient_checkpointing=False, cfg_loaded=None):\n",
    "        super().__init__()\n",
    "        self.use_multi_task = use_multi_task\n",
    "        self.ab2_mode = ab2_mode\n",
    "        self.add_meta_dim = add_meta_dim\n",
    "        self.use_clip = use_clip\n",
    "        self.cfg_loaded = cfg_loaded\n",
    "\n",
    "        proj_dim = getattr(cfg_loaded, \"proj_dim\", 256)\n",
    "\n",
    "        # Text encoder (no pooler; mean pooling)\n",
    "        tcfg = AutoConfig.from_pretrained(text_model_name)\n",
    "        if hasattr(tcfg, \"add_pooling_layer\"):\n",
    "            tcfg.add_pooling_layer = False\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name, config=tcfg)\n",
    "        if gradient_checkpointing and hasattr(self.text_model, \"gradient_checkpointing_enable\"):\n",
    "            try: self.text_model.gradient_checkpointing_enable()\n",
    "            except: pass\n",
    "        tdim = self.text_model.config.hidden_size\n",
    "        if getattr(cfg_loaded, \"freeze_text_encoder\", True):\n",
    "            for p in self.text_model.parameters(): p.requires_grad = False\n",
    "\n",
    "        # LoRA on text\n",
    "        if getattr(cfg_loaded, \"use_lora_text\", False):\n",
    "            targets = getattr(cfg_loaded, \"lora_target_text\", None) or [\"query\",\"value\"]\n",
    "            self.text_model = get_peft_model(\n",
    "                self.text_model,\n",
    "                LoraConfig(\n",
    "                    r=getattr(cfg_loaded, \"lora_r_text\", 8),\n",
    "                    lora_alpha=getattr(cfg_loaded, \"lora_alpha_text\", 16),\n",
    "                    lora_dropout=getattr(cfg_loaded, \"lora_dropout_text\", 0.1),\n",
    "                    target_modules=targets,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.FEATURE_EXTRACTION\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Vision encoder\n",
    "        if use_clip:\n",
    "            self.vision = CLIPVisionModel.from_pretrained(clip_model_name or \"openai/clip-vit-base-patch32\")\n",
    "            idim = self.vision.config.hidden_size\n",
    "            if getattr(cfg_loaded, \"freeze_vision_encoder\", True):\n",
    "                for p in self.vision.parameters(): p.requires_grad = False\n",
    "            if getattr(cfg_loaded, \"use_lora_vision\", False):\n",
    "                vtargets = getattr(cfg_loaded, \"lora_target_vision\", None) or [\"q_proj\",\"v_proj\"]\n",
    "                self.vision = get_peft_model(\n",
    "                    self.vision,\n",
    "                    LoraConfig(\n",
    "                        r=getattr(cfg_loaded, \"lora_r_vision\", 4),\n",
    "                        lora_alpha=getattr(cfg_loaded, \"lora_alpha_vision\", 8),\n",
    "                        lora_dropout=getattr(cfg_loaded, \"lora_dropout_vision\", 0.05),\n",
    "                        target_modules=vtargets,\n",
    "                        bias=\"none\",\n",
    "                        task_type=TaskType.FEATURE_EXTRACTION\n",
    "                    )\n",
    "                )\n",
    "            self.use_clip = True\n",
    "        else:\n",
    "            from torchvision import models\n",
    "            if fallback_backbone.lower() == \"mobilenet_v2\":\n",
    "                try:\n",
    "                    from torchvision.models import MobileNet_V2_Weights\n",
    "                    im = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "                except Exception:\n",
    "                    im = models.mobilenet_v2(pretrained=True)\n",
    "                idim = 1280; im.classifier = nn.Identity()\n",
    "            else:\n",
    "                try:\n",
    "                    from torchvision.models import ResNet18_Weights\n",
    "                    backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "                except Exception:\n",
    "                    backbone = models.resnet18(pretrained=True)\n",
    "                idim = 512; im = nn.Sequential(*list(backbone.children())[:-1], nn.Flatten(1))\n",
    "            if getattr(cfg_loaded, \"freeze_vision_encoder\", True):\n",
    "                for p in im.parameters(): p.requires_grad = False\n",
    "            self.vision = im\n",
    "            self.use_clip = False\n",
    "\n",
    "        self.image_feat_dim = idim\n",
    "\n",
    "        # Projections (align scales)\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(tdim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(getattr(cfg_loaded, \"dropout\", 0.25)),\n",
    "        )\n",
    "        self.i_proj = nn.Sequential(\n",
    "            nn.Linear(idim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(getattr(cfg_loaded, \"dropout\", 0.25)),\n",
    "        )\n",
    "\n",
    "        # Backbones\n",
    "        fusion_in = proj_dim + proj_dim + (getattr(cfg_loaded, \"add_meta_dim\", 2) if getattr(cfg_loaded, \"add_meta_dim\", 2)>0 else 0)\n",
    "        text_in   = proj_dim + (getattr(cfg_loaded, \"add_meta_dim\", 2) if getattr(cfg_loaded, \"add_meta_dim\", 2)>0 else 0)\n",
    "\n",
    "        hidden = getattr(cfg_loaded, \"hidden\", 256)\n",
    "        dropout = getattr(cfg_loaded, \"dropout\", 0.25)\n",
    "\n",
    "        self.fusion_backbone = nn.Sequential(\n",
    "            nn.Linear(fusion_in, hidden), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.GELU(), nn.Dropout(dropout),\n",
    "        )\n",
    "        self.text_backbone = nn.Sequential(\n",
    "            nn.Linear(text_in, hidden), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2), nn.GELU(), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Gate over both modalities (+meta)\n",
    "        self.gate_proj = nn.Sequential(\n",
    "            nn.Linear(fusion_in, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Heads\n",
    "        self.hs3_fusion_head = nn.Linear(hidden//2, 3)\n",
    "        self.hs3_text_head   = nn.Linear(hidden//2, 3)\n",
    "        self.ab2_fusion_head = nn.Linear(hidden//2, 2)\n",
    "        self.ab2_text_head   = nn.Linear(hidden//2, 2)\n",
    "\n",
    "    def _mean_pool(self, last_hidden_state, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "        return summed / denom\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images, has_image_mask, meta=None):\n",
    "        # Text\n",
    "        out_text = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        t_last = out_text.last_hidden_state\n",
    "        pooling = getattr(self.cfg_loaded, \"text_pooling\", \"mean\")\n",
    "        t_feat = self._mean_pool(t_last, attention_mask) if pooling == \"mean\" else t_last[:,0,:]\n",
    "        t_proj = self.t_proj(t_feat)\n",
    "\n",
    "        if (meta is not None) and (meta.dtype != t_proj.dtype):\n",
    "            meta = meta.to(t_proj.dtype)\n",
    "\n",
    "        # Vision (dtype-safe; only compute for has_image samples)\n",
    "        B = images.size(0)\n",
    "        img_mask_flat = has_image_mask.view(-1)\n",
    "        if img_mask_flat.any():\n",
    "            idx = torch.nonzero(img_mask_flat, as_tuple=False).squeeze(1)\n",
    "            images_sub = images[idx].to(memory_format=torch.channels_last)\n",
    "            if self.use_clip:\n",
    "                v_out = self.vision(pixel_values=images_sub)\n",
    "                i_sub = v_out.pooler_output\n",
    "            else:\n",
    "                i_sub = self.vision(images_sub)\n",
    "            i_sub = i_sub.to(images_sub.dtype)\n",
    "            i_feat = images.new_zeros((B, self.image_feat_dim), dtype=images_sub.dtype)\n",
    "            i_feat[idx] = i_sub\n",
    "        else:\n",
    "            i_feat = images.new_zeros((B, self.image_feat_dim), dtype=images.dtype)\n",
    "        i_proj = self.i_proj(i_feat)\n",
    "\n",
    "        # Inputs\n",
    "        add_meta_dim = getattr(self.cfg_loaded, \"add_meta_dim\", 2)\n",
    "        if add_meta_dim and meta is not None:\n",
    "            t_in = torch.cat([t_proj, meta], dim=1)\n",
    "            f_in = torch.cat([t_proj, i_proj, meta], dim=1)\n",
    "        else:\n",
    "            t_in = t_proj\n",
    "            f_in = torch.cat([t_proj, i_proj], dim=1)\n",
    "\n",
    "        # Backbones\n",
    "        t_repr = self.text_backbone(t_in)\n",
    "        f_repr = self.fusion_backbone(f_in)\n",
    "\n",
    "        # Gate (mask to zero if no image)\n",
    "        gate_weight = self.gate_proj(f_in) * img_mask_flat.float().unsqueeze(1)  # [B,1]\n",
    "\n",
    "        # Heads\n",
    "        hs3_f = self.hs3_fusion_head(f_repr)\n",
    "        hs3_t = self.hs3_text_head(t_repr)\n",
    "        hs3_logits = gate_weight * hs3_f + (1 - gate_weight) * hs3_t\n",
    "\n",
    "        ab2_f = self.ab2_fusion_head(f_repr)\n",
    "        ab2_t = self.ab2_text_head(t_repr)\n",
    "        ab2_logits = gate_weight * ab2_f + (1 - gate_weight) * ab2_t\n",
    "\n",
    "        return {\n",
    "            \"hs3_logits\": hs3_logits,\n",
    "            \"ab2_logits\": ab2_logits,\n",
    "            \"gate_weight\": gate_weight.detach(),\n",
    "            \"hs3_f_only\": hs3_f,\n",
    "            \"ab2_f_only\": ab2_f,\n",
    "            \"hs3_t_only\": hs3_t,\n",
    "            \"ab2_t_only\": ab2_t,\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# Loader + predictor helpers\n",
    "# ======================\n",
    "def _clean_keys(sd):\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        if k.startswith(\"_orig_mod.\"): k = k[10:]\n",
    "        if k.startswith(\"module.\"):    k = k[7:]\n",
    "        out[k] = v\n",
    "    return out\n",
    "\n",
    "def load_model_and_processors_lora(ckpt_path, device=\"cpu\"):\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    assert \"cfg\" in ckpt, \"Checkpoint missing cfg.\"\n",
    "\n",
    "    class C: pass\n",
    "    cfg_loaded = C()\n",
    "    for k, v in ckpt[\"cfg\"].items():\n",
    "        setattr(cfg_loaded, k, v)\n",
    "    if not hasattr(cfg_loaded, \"text_pooling\"):\n",
    "        setattr(cfg_loaded, \"text_pooling\", \"mean\")\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(cfg_loaded.text_model_name, use_fast=True)\n",
    "    clip_proc = CLIPImageProcessor.from_pretrained(cfg_loaded.clip_vision_model) if getattr(cfg_loaded, \"use_clip_vision\", True) else None\n",
    "\n",
    "    model = OptimizedMultiModalGated(\n",
    "        text_model_name=cfg_loaded.text_model_name,\n",
    "        freeze_text=getattr(cfg_loaded, \"freeze_text_encoder\", True),\n",
    "        use_clip=getattr(cfg_loaded, \"use_clip_vision\", True),\n",
    "        clip_model_name=getattr(cfg_loaded, \"clip_vision_model\", \"openai/clip-vit-base-patch32\"),\n",
    "        fallback_backbone=getattr(cfg_loaded, \"image_backbone_fallback\", \"mobilenet_v2\"),\n",
    "        freeze_vision=getattr(cfg_loaded, \"freeze_vision_encoder\", True),\n",
    "        use_multi_task=getattr(cfg_loaded, \"use_multi_task\", True),\n",
    "        ab2_mode=getattr(cfg_loaded, \"ab2_mode\", \"ce\"),\n",
    "        hidden=getattr(cfg_loaded, \"hidden\", 256),\n",
    "        dropout=getattr(cfg_loaded, \"dropout\", 0.25),\n",
    "        add_meta_dim=getattr(cfg_loaded, \"add_meta_dim\", 2),\n",
    "        gradient_checkpointing=False,\n",
    "        cfg_loaded=cfg_loaded\n",
    "    ).to(device)\n",
    "\n",
    "    sd = _clean_keys(ckpt[\"model\"])\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    if device == \"cuda\":\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "    model.eval()\n",
    "    return model, tok, clip_proc, cfg_loaded\n",
    "\n",
    "def get_image_tensor(image, clip_proc, image_size=224):\n",
    "    # image can be numpy array, PIL.Image, file path, or None\n",
    "    pil = None\n",
    "    if image is None:\n",
    "        pil = None\n",
    "    elif isinstance(image, str):\n",
    "        if os.path.exists(image):\n",
    "            pil = Image.open(image).convert(\"RGB\")\n",
    "    elif isinstance(image, Image.Image):\n",
    "        pil = image.convert(\"RGB\")\n",
    "    else:\n",
    "        try:\n",
    "            pil = Image.fromarray(image).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            pil = None\n",
    "\n",
    "    if pil is None:\n",
    "        img_tensor = torch.zeros(3, image_size, image_size, dtype=torch.float32)\n",
    "        return img_tensor, None\n",
    "\n",
    "    if clip_proc is not None:\n",
    "        img_tensor = clip_proc(images=pil, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "    else:\n",
    "        from torchvision import transforms\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "        img_tensor = tfm(pil)\n",
    "    return img_tensor, pil\n",
    "\n",
    "def softmax_temp(logits, T=1.0):\n",
    "    if T is None or T <= 0: T = 1.0\n",
    "    return torch.softmax(logits / float(T), dim=-1)\n",
    "\n",
    "# ======================\n",
    "# OCR helpers + normalization (EasyOCR + PaddleOCR, safe fallback)\n",
    "# ======================\n",
    "_OCR_READER = None\n",
    "_OCR_BACKEND = None\n",
    "_ZWS_RE = re.compile(r'[\\u200B-\\u200D\\uFEFF]')  # zero-width chars\n",
    "\n",
    "# --- Aggressive OCR cleanup helpers (drop-in) ---\n",
    "# Collapse long character repeats: \"sooooo\" -> \"soo\"\n",
    "_REPEAT_RE = re.compile(r\"(.)\\1{2,}\")\n",
    "\n",
    "# Simple leetspeak deobfuscation map (used inside tokens that mix letters+digits)\n",
    "_LEET_MAP = {\n",
    "    \"0\": \"o\",\n",
    "    \"1\": \"i\",\n",
    "    \"3\": \"e\",\n",
    "    \"4\": \"a\",\n",
    "    \"5\": \"s\",\n",
    "    \"7\": \"t\",\n",
    "    \"8\": \"ate\",  # helps \"h8\" -> \"hate\" and \"l8r\" -> \"later\"\n",
    "}\n",
    "\n",
    "# Common texting slang/abbreviations to standard English (lowercase keys)\n",
    "_SLANG_MAP = {\n",
    "    # single words\n",
    "    \"u\": \"you\", \"ur\": \"your\", \"r\": \"are\", \"ya\": \"you\", \"yall\": \"you all\",\n",
    "    \"im\": \"i am\", \"ima\": \"i am\", \"ama\": \"ask me anything\",\n",
    "    \"idk\": \"i do not know\", \"imo\": \"in my opinion\", \"imho\": \"in my humble opinion\",\n",
    "    \"btw\": \"by the way\", \"brb\": \"be right back\", \"gtg\": \"got to go\", \"ttyl\": \"talk to you later\",\n",
    "    \"tbh\": \"to be honest\", \"smh\": \"shaking my head\", \"rn\": \"right now\",\n",
    "    \"omg\": \"oh my god\", \"irl\": \"in real life\",\n",
    "    \"pls\": \"please\", \"plz\": \"please\",\n",
    "    \"thx\": \"thanks\", \"thanx\": \"thanks\", \"tnx\": \"thanks\", \"ty\": \"thank you\", \"tysm\": \"thank you so much\",\n",
    "    \"bc\": \"because\", \"cuz\": \"because\", \"coz\": \"because\",\n",
    "    \"wanna\": \"want to\", \"gonna\": \"going to\", \"gotta\": \"got to\",\n",
    "    \"kinda\": \"kind of\", \"sorta\": \"sort of\", \"lemme\": \"let me\", \"gimme\": \"give me\",\n",
    "    \"aint\": \"is not\", \"cant\": \"cannot\", \"wont\": \"will not\",\n",
    "    \"shouldnt\": \"should not\", \"couldnt\": \"could not\", \"wouldnt\": \"would not\",\n",
    "    \"dont\": \"do not\", \"doesnt\": \"does not\", \"didnt\": \"did not\",\n",
    "    \"isnt\": \"is not\", \"arent\": \"are not\", \"wasnt\": \"was not\", \"werent\": \"were not\",\n",
    "    \"havent\": \"have not\", \"hasnt\": \"has not\", \"hadnt\": \"had not\", \"mustnt\": \"must not\", \"neednt\": \"need not\",\n",
    "    \"ok\": \"okay\", \"kk\": \"okay\", \"k\": \"okay\",\n",
    "    \"lol\": \"laughing\", \"lmao\": \"laughing\", \"lmfao\": \"laughing\", \"rofl\": \"laughing\", \"xd\": \"laughing\",\n",
    "    \"nah\": \"no\", \"yup\": \"yes\", \"yeah\": \"yes\", \"nope\": \"no\", \"bruh\": \"bro\",\n",
    "    \"w\": \"with\",  # common \"w\" -> \"with\"\n",
    "    \"luv\": \"love\",\n",
    "    # numeric slang\n",
    "    \"b4\": \"before\", \"l8r\": \"later\", \"gr8\": \"great\",\n",
    "    \"2day\": \"today\", \"2moro\": \"tomorrow\", \"2mrw\": \"tomorrow\", \"tmrw\": \"tomorrow\", \"tmr\": \"tomorrow\",\n",
    "    \"4u\": \"for you\", \"4ya\": \"for you\", \"4you\": \"for you\",\n",
    "    # common obfuscations of profanity (normalize to base form)\n",
    "    \"fuk\": \"fuck\", \"fck\": \"fuck\", \"fcuk\": \"fuck\", \"phuck\": \"fuck\", \"fucc\": \"fuck\",\n",
    "    \"wtf\": \"what the fuck\", \"wth\": \"what the hell\",\n",
    "}\n",
    "\n",
    "def _strip_punct_and_symbols(s: str) -> str:\n",
    "    # Remove all Unicode punctuation (P*) and symbol (S*) categories -> replace with space\n",
    "    return \"\".join(ch if not unicodedata.category(ch).startswith((\"P\", \"S\")) else \" \" for ch in s)\n",
    "\n",
    "def _reduce_repeats(s: str) -> str:\n",
    "    # Cap character repeats to max 2 (e.g., \"loooove\" -> \"loove\")\n",
    "    return _REPEAT_RE.sub(r\"\\1\\1\", s)\n",
    "\n",
    "def _deobfuscate_leet_token(tok: str) -> str:\n",
    "    # Only apply if token mixes letters and digits; keep pure numbers intact\n",
    "    if tok.isdigit() or not any(c.isdigit() for c in tok):\n",
    "        return tok\n",
    "    out = []\n",
    "    for ch in tok:\n",
    "        if ch.isdigit():\n",
    "            out.append(_LEET_MAP.get(ch, ch))\n",
    "        else:\n",
    "            out.append(ch)\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _expand_slang(s: str) -> str:\n",
    "    # Token-level expansion using _SLANG_MAP and leet deobfuscation\n",
    "    toks = s.split()\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        # Direct slang\n",
    "        if t in _SLANG_MAP:\n",
    "            repl = _SLANG_MAP[t]\n",
    "            out.extend(repl.split())\n",
    "            continue\n",
    "        # Leet deobfuscation\n",
    "        t2 = _deobfuscate_leet_token(t)\n",
    "        # Try slang after leet (e.g., \"h8\" -> \"hate\" or via map)\n",
    "        if t2 in _SLANG_MAP:\n",
    "            repl = _SLANG_MAP[t2]\n",
    "            out.extend(repl.split())\n",
    "        else:\n",
    "            out.append(t2)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def _install_paddle_cpu():\n",
    "    try:\n",
    "        import paddle  # noqa\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"Installing paddlepaddle (CPU) ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"paddlepaddle==2.6.1\"])\n",
    "        import paddle  # noqa\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"PaddlePaddle install failed:\", e)\n",
    "        return False\n",
    "\n",
    "def ensure_ocr(engine=\"easyocr\"):\n",
    "    \"\"\"\n",
    "    Returns (reader, backend). If Paddle fails, falls back to EasyOCR.\n",
    "    Paddle is forced to CPU to avoid CUDA mismatches.\n",
    "    \"\"\"\n",
    "    global _OCR_READER, _OCR_BACKEND\n",
    "    if _OCR_READER is not None and _OCR_BACKEND == engine:\n",
    "        return _OCR_READER, _OCR_BACKEND\n",
    "\n",
    "    if engine == \"paddle\":\n",
    "        ok = _install_paddle_cpu()\n",
    "        if not ok:\n",
    "            print(\"Falling back to EasyOCR due to Paddle install failure.\")\n",
    "            engine = \"easyocr\"\n",
    "\n",
    "    if engine == \"paddle\":\n",
    "        try:\n",
    "            import paddle\n",
    "            paddle.device.set_device(\"cpu\")\n",
    "            from paddleocr import PaddleOCR\n",
    "            _OCR_READER = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)\n",
    "            _OCR_BACKEND = \"paddle\"\n",
    "            return _OCR_READER, _OCR_BACKEND\n",
    "        except Exception as e:\n",
    "            print(\"PaddleOCR init failed, falling back to EasyOCR:\", e)\n",
    "            engine = \"easyocr\"\n",
    "\n",
    "    try:\n",
    "        import easyocr\n",
    "    except ImportError:\n",
    "        print(\"Installing easyocr...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"easyocr\"])\n",
    "        import easyocr\n",
    "    _OCR_READER = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "    _OCR_BACKEND = \"easyocr\"\n",
    "    return _OCR_READER, _OCR_BACKEND\n",
    "\n",
    "def normalize_text(s: str, lower: bool = OCR_LOWER, max_chars: int = OCR_MAX_CHARS) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Unicode normalize + strip invisibles and standardize punctuation\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"\\u00A0\", \" \")  # NBSP -> space\n",
    "    s = _ZWS_RE.sub(\"\", s)        # remove zero-width chars\n",
    "    s = (s.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "           .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "           .replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"‐\", \"-\")\n",
    "           .replace(\"…\", \"...\"))\n",
    "\n",
    "    # 2) Early whitespace squeeze (helps contractions lib)\n",
    "    s = \" \".join(s.split())\n",
    "\n",
    "    # 3) Expand English contractions before stripping punctuation\n",
    "    try:\n",
    "        s = contractions.fix(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) Lowercase for consistent matching\n",
    "    if lower:\n",
    "        s = s.lower()\n",
    "\n",
    "    # 5) Aggressive: remove punctuation and symbols\n",
    "    s = _strip_punct_and_symbols(s)\n",
    "\n",
    "    # 6) Reduce character elongations (heeellooo -> heelloo)\n",
    "    s = _reduce_repeats(s)\n",
    "\n",
    "    # 7) Expand common texting slang + simple leetspeak\n",
    "    s = _expand_slang(s)\n",
    "\n",
    "    # 8) Final whitespace squeeze\n",
    "    s = \" \".join(s.split())\n",
    "\n",
    "    # 9) Truncate to max chars\n",
    "    if len(s) > max_chars:\n",
    "        s = s[:max_chars]\n",
    "\n",
    "    return s\n",
    "\n",
    "def _parse_paddle_result(res, min_conf=0.0):\n",
    "    \"\"\"\n",
    "    Robustly parse PaddleOCR results into (text, conf) list.\n",
    "    Supports both:\n",
    "      [[ [points], (text, conf) ], ...] or [ [ [ [points], (text, conf) ], ... ] ]\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    if not isinstance(res, list) or len(res) == 0:\n",
    "        return items\n",
    "    # If single-image list wrapper, unwrap\n",
    "    if len(res) == 1 and isinstance(res[0], list) and len(res[0]) > 0 and isinstance(res[0][0], list):\n",
    "        res = res[0]\n",
    "    for entry in res:\n",
    "        try:\n",
    "            if not isinstance(entry, (list, tuple)) or len(entry) < 2:\n",
    "                continue\n",
    "            info = entry[1]\n",
    "            if isinstance(info, (list, tuple)) and len(info) >= 2:\n",
    "                txt, conf = info[0], float(info[1])\n",
    "                if txt and str(txt).strip() and conf >= min_conf:\n",
    "                    items.append((str(txt).strip(), conf))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return items\n",
    "\n",
    "def ocr_extract_text(pil_image, engine=OCR_ENGINE, min_conf=OCR_MIN_CONF) -> str:\n",
    "    if pil_image is None:\n",
    "        return \"\"\n",
    "    reader, backend = ensure_ocr(engine)\n",
    "    text_pieces = []\n",
    "    try:\n",
    "        if backend == \"paddle\":\n",
    "            from paddleocr import PaddleOCR  # noqa: F401\n",
    "            res = reader.ocr(np.array(pil_image), cls=True)\n",
    "            pairs = _parse_paddle_result(res, min_conf=min_conf)\n",
    "            text_pieces = [t for t, c in pairs]\n",
    "        else:\n",
    "            import easyocr  # noqa: F401\n",
    "            res = reader.readtext(np.array(pil_image))\n",
    "            for _, txt, conf in res:\n",
    "                if float(conf) >= min_conf and txt and txt.strip():\n",
    "                    text_pieces.append(txt.strip())\n",
    "    except Exception as e:\n",
    "        print(\"OCR failed:\", e)\n",
    "        return \"\"\n",
    "    return \" \".join(text_pieces)\n",
    "\n",
    "# ======================\n",
    "# Load model once\n",
    "# ======================\n",
    "if any(n not in globals() for n in [\"model\", \"tok\", \"clip_proc\", \"cfg_loaded\"]):\n",
    "    model, tok, clip_proc, cfg_loaded = load_model_and_processors_lora(ckpt_path, device=device)\n",
    "    print(\"Loaded model\")\n",
    "\n",
    "# ======================\n",
    "# Inference\n",
    "# ======================\n",
    "@torch.inference_mode()\n",
    "def user_predict(\n",
    "    text: str,\n",
    "    image=None,\n",
    "    sarcasm=False,\n",
    "    humour=False,\n",
    "    temperature: float = None,\n",
    "    use_ocr: bool = True,\n",
    "    ocr_engine: str = OCR_ENGINE,\n",
    "    ocr_min_conf: float = OCR_MIN_CONF,\n",
    "    ignore_image_when_ocr: bool = FORCE_TEXT_ONLY_WHEN_OCR\n",
    "):\n",
    "    temperature = float(TEMP if temperature is None else temperature)\n",
    "\n",
    "    # Prepare image tensor and OCR text\n",
    "    img_tensor, pil = get_image_tensor(image, clip_proc, image_size=getattr(cfg_loaded, \"image_size\", 224))\n",
    "    if torch.cuda.is_available(): img_tensor = img_tensor.half()\n",
    "    imgs = img_tensor.unsqueeze(0).to(device)\n",
    "    has_img = torch.tensor([img_tensor.abs().sum().item() > 0], dtype=torch.bool, device=device)\n",
    "\n",
    "    raw_ocr_text = \"\"\n",
    "    if use_ocr and pil is not None:\n",
    "        raw_ocr_text = ocr_extract_text(pil, engine=ocr_engine, min_conf=ocr_min_conf)\n",
    "    ocr_text = normalize_text(raw_ocr_text, lower=OCR_LOWER, max_chars=OCR_MAX_CHARS)\n",
    "\n",
    "    # If OCR produced text, feed it directly; else use user text\n",
    "    user_text = (text or \"\").strip()\n",
    "    final_text = ocr_text if ocr_text.strip() else user_text\n",
    "\n",
    "    # Optionally ignore image when OCR text is used (avoid gate overshadow)\n",
    "    if ignore_image_when_ocr and ocr_text.strip():\n",
    "        has_img = torch.tensor([False], dtype=torch.bool, device=device)\n",
    "        # imgs tensor can remain unchanged; it won't be used when has_img=False\n",
    "\n",
    "    # Tokenize (slightly larger max_len at inference)\n",
    "    used_max_len = max(getattr(cfg_loaded, \"max_len\", 96), INFER_MAX_LEN)\n",
    "    enc = tok(final_text, truncation=True, max_length=used_max_len, padding=True, return_tensors=\"pt\")\n",
    "    ids = enc[\"input_ids\"].to(device); attn = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    meta_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    meta = torch.tensor([[int(bool(sarcasm)), int(bool(humour))]], dtype=meta_dtype, device=device)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.float16):\n",
    "        out = model(ids, attn, imgs, has_img, meta)\n",
    "        probs_t = softmax_temp(out[\"hs3_logits\"], T=temperature).detach().cpu()[0]\n",
    "        gate = float(out[\"gate_weight\"].mean().detach().cpu().item() if out[\"gate_weight\"].numel() else 0.0)\n",
    "\n",
    "    probs = probs_t.numpy()\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    pred = CLASS_NAMES[pred_idx]\n",
    "\n",
    "    # Abusive probability (binary): P(Abusive or Offensive)\n",
    "    p_abusive = float(probs[0] + probs[1])\n",
    "\n",
    "    return {\n",
    "        \"final_label\": pred,\n",
    "        \"final_probs\": {k: float(v) for k, v in zip(CLASS_NAMES, probs)},\n",
    "        \"abusive_probability\": p_abusive,\n",
    "        \"gate_weight_image\": gate,  # ~0 = text-dominant, ~1 = image-dominant\n",
    "        \"used_image\": bool(has_img.item()),\n",
    "        \"temperature\": temperature,\n",
    "        \"ocr_text_raw\": raw_ocr_text,\n",
    "        \"ocr_text_norm\": ocr_text,\n",
    "        \"final_text\": final_text,\n",
    "        \"ocr_engine\": _OCR_BACKEND or ocr_engine,\n",
    "        \"ocr_min_conf\": ocr_min_conf,\n",
    "        \"ignored_image_due_to_ocr\": bool(ignore_image_when_ocr and ocr_text.strip() != \"\")\n",
    "    }\n",
    "\n",
    "# Quick CLI-like test\n",
    "try:\n",
    "    print(user_predict(\"I hate you\", image=None, sarcasm=False, humour=False))\n",
    "except Exception as e:\n",
    "    print(\"Quick test failed:\", e)\n",
    "\n",
    "# ======================\n",
    "# Batch CSV inference (optional)\n",
    "# CSV columns supported: text, image_path, sarcasm, humour\n",
    "# ======================\n",
    "def predict_csv(csv_path, out_path=None, use_ocr=True, ocr_engine=OCR_ENGINE, ocr_min_conf=OCR_MIN_CONF, ignore_image_when_ocr=FORCE_TEXT_ONLY_WHEN_OCR):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)\n",
    "    outs = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = str(row.get(\"text\", \"\") or \"\")\n",
    "        img_p = row.get(\"image_path\", None)\n",
    "        img_p = str(img_p) if (img_p is not None and str(img_p).strip() != \"\" and os.path.exists(str(img_p))) else None\n",
    "        sarcasm = bool(row.get(\"sarcasm\", False))\n",
    "        humour  = bool(row.get(\"humour\", False))\n",
    "        res = user_predict(text, image=img_p, sarcasm=sarcasm, humour=humour,\n",
    "                           use_ocr=use_ocr, ocr_engine=ocr_engine, ocr_min_conf=ocr_min_conf,\n",
    "                           ignore_image_when_ocr=ignore_image_when_ocr)\n",
    "        outs.append({\n",
    "            \"text\": text,\n",
    "            \"image_path\": img_p,\n",
    "            \"sarcasm\": sarcasm,\n",
    "            \"humour\": humour,\n",
    "            \"pred_label\": res[\"final_label\"],\n",
    "            \"p_abusive\": res[\"abusive_probability\"],\n",
    "            \"p_Abusive\": res[\"final_probs\"][\"Abusive\"],\n",
    "            \"p_Offensive\": res[\"final_probs\"][\"Offensive\"],\n",
    "            \"p_Non_abusive\": res[\"final_probs\"][\"Non-abusive\"],\n",
    "            \"gate_weight_image\": res[\"gate_weight_image\"],\n",
    "            \"used_image\": res[\"used_image\"],\n",
    "            \"ocr_text_raw\": res[\"ocr_text_raw\"],\n",
    "            \"ocr_text_norm\": res[\"ocr_text_norm\"],\n",
    "            \"final_text\": res[\"final_text\"],\n",
    "            \"ignored_image_due_to_ocr\": res[\"ignored_image_due_to_ocr\"],\n",
    "        })\n",
    "    out_df = pd.DataFrame(outs)\n",
    "    if out_path:\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        print(f\"Saved predictions to {out_path}\")\n",
    "    return out_df\n",
    "\n",
    "# ======================\n",
    "# Gradio UI (optional)\n",
    "# ======================\n",
    "try:\n",
    "    import gradio as gr\n",
    "    def gr_fn(text, image, sarcasm, humour, temperature, use_ocr, ocr_min_conf, ocr_engine, ignore_image_when_ocr):\n",
    "        res = user_predict(\n",
    "            text, image=image, sarcasm=sarcasm, humour=humour, temperature=temperature,\n",
    "            use_ocr=use_ocr, ocr_engine=ocr_engine, ocr_min_conf=ocr_min_conf,\n",
    "            ignore_image_when_ocr=ignore_image_when_ocr\n",
    "        )\n",
    "        probs_display = {\n",
    "            \"Abusive\": res[\"final_probs\"][\"Abusive\"],\n",
    "            \"Offensive\": res[\"final_probs\"][\"Offensive\"],\n",
    "            \"Non-abusive\": res[\"final_probs\"][\"Non-abusive\"],\n",
    "            \"Abusive (binary)\": res[\"abusive_probability\"],\n",
    "        }\n",
    "        extra = (\n",
    "            f\"Gate(image)={res['gate_weight_image']:.2f} | Used image={res['used_image']} \"\n",
    "            f\"| Temp={res['temperature']} | OCR={res['ocr_engine']}(min_conf={res['ocr_min_conf']}) \"\n",
    "            f\"| Ignored image due to OCR={res['ignored_image_due_to_ocr']}\"\n",
    "        )\n",
    "        return probs_display, res[\"final_label\"], res[\"ocr_text_raw\"], res[\"ocr_text_norm\"], res[\"final_text\"], extra\n",
    "\n",
    "    demo = gr.Interface(\n",
    "        fn=gr_fn,\n",
    "        inputs=[\n",
    "            gr.Textbox(label=\"Text\", lines=3, placeholder=\"Type a comment...\"),\n",
    "            gr.Image(label=\"Optional image\", type=\"numpy\"),\n",
    "            gr.Checkbox(label=\"Sarcasm flag\", value=False),\n",
    "            gr.Checkbox(label=\"Humour flag\", value=False),\n",
    "            gr.Slider(0.5, 2.0, value=TEMP, step=0.1, label=\"Temperature\"),\n",
    "            gr.Checkbox(label=\"Use OCR\", value=True),\n",
    "            gr.Slider(0.0, 1.0, value=OCR_MIN_CONF, step=0.05, label=\"OCR min confidence\"),\n",
    "            gr.Radio(choices=[\"easyocr\",\"paddle\"], value=\"easyocr\", label=\"OCR engine\"),\n",
    "            gr.Checkbox(label=\"Ignore image when OCR text is used\", value=True),\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Label(num_top_classes=4, label=\"Probabilities\"),\n",
    "            gr.Textbox(label=\"Final label\"),\n",
    "            gr.Textbox(label=\"OCR text (raw)\"),\n",
    "            gr.Textbox(label=\"OCR text (normalized)\"),\n",
    "            gr.Textbox(label=\"Final text sent to model\"),\n",
    "            gr.Markdown(label=\"Details\"),\n",
    "        ],\n",
    "        title=\"Abusive/Offensive Detection (LoRA, direct 3-class) + OCR (Paddle-safe + Easy fallback)\"\n",
    "    )\n",
    "    # Uncomment to launch with a public link:\n",
    "    demo.launch(share=True)\n",
    "    print(\"Gradio app ready. Uncomment demo.launch(share=True) to run.\")\n",
    "except Exception as e:\n",
    "    print(\"Gradio not available:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89726776-8659-4f92-9845-4b7806934dbc",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "INTERPRETATION OF DEPLOYMENT PIPELINE — ROBUST MULTIMODAL HATE DETECTION WITH OCR\n",
    "\n",
    "The user inference pipeline demonstrates a production-ready system for multimodal\n",
    "hate detection that handles real-world complexities through sophisticated OCR\n",
    "integration and robust text normalization.\n",
    "\n",
    "Key deployment interpretations:\n",
    "\n",
    "- **OCR-Driven Modality Selection**: Intelligently prioritizes OCR-extracted text\n",
    "  over user-provided text when available, recognizing that meme text often\n",
    "  contains the primary offensive content. The system can optionally ignore\n",
    "  visual features when OCR text is present to prevent gate overshadowing.\n",
    "\n",
    "- **Aggressive Text Normalization Pipeline**: Implements multi-stage text cleaning\n",
    "  that handles common obfuscation techniques used in hateful content:\n",
    "  - Unicode normalization and zero-width character removal\n",
    "  - Contraction expansion and punctuation standardization  \n",
    "  - Character repeat reduction (\"sooooo\" → \"soo\")\n",
    "  - Leetspeak deobfuscation (\"h8\" → \"hate\", \"l8r\" → \"later\")\n",
    "  - Texting slang expansion (\"u\" → \"you\", \"ur\" → \"your\")\n",
    "  This normalization is crucial for detecting coded hate speech and evasive language.\n",
    "\n",
    "- **Dual OCR Engine Support**: Provides fallback between PaddleOCR (higher accuracy)\n",
    "  and EasyOCR (easier installation), with automatic CPU fallback to handle\n",
    "  deployment environment constraints and CUDA compatibility issues.\n",
    "\n",
    "- **Modality Gate Transparency**: Exposes the gate weight that shows how much\n",
    "  the model relies on visual vs. textual features, providing interpretability\n",
    "  for why particular predictions were made.\n",
    "\n",
    "- **Temperature-Controlled Confidence**: Allows adjustment of prediction confidence\n",
    "  through temperature scaling, enabling calibration for different risk tolerance\n",
    "  levels in production deployment.\n",
    "\n",
    "- **Batch Processing Capability**: Supports CSV-based batch inference for\n",
    "  large-scale content moderation workflows, with comprehensive output including\n",
    "  OCR results and modality usage.\n",
    "\n",
    "Overall interpretation:\n",
    "This deployment system addresses the practical challenges of real-world hate\n",
    "detection: evasive text patterns, multimodal content, and deployment constraints.\n",
    "The sophisticated OCR pipeline ensures that visual text is properly captured\n",
    "and normalized, while the flexible inference options support both interactive\n",
    "moderation interfaces and automated batch processing. The system's transparency\n",
    "about modality usage and confidence calibration makes it suitable for\n",
    "high-stakes content moderation applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35610d0-ce72-41bd-867d-5dac9ed03e3b",
   "metadata": {
    "id": "a35610d0-ce72-41bd-867d-5dac9ed03e3b",
    "outputId": "e2c9fd84-f440-48d9-e4ae-fb16eeebd616"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment wordcloud xgboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec30bae-9850-41b4-94dc-e4955df8d117",
   "metadata": {
    "id": "aec30bae-9850-41b4-94dc-e4955df8d117",
    "outputId": "e448efd5-4d1b-4b32-cf57-e44235fe7841"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder structure\n",
    "base_folder = \"datasets/processed_data/splits\"\n",
    "subfolders = [\"train\", \"test\", \"val\"]\n",
    "file_names = {\n",
    "    \"train\": \"text_train.csv\",\n",
    "    \"test\": \"text_test.csv\",\n",
    "    \"val\": \"text_val.csv\"\n",
    "}\n",
    "\n",
    "# List to store all dataframes\n",
    "dataframes = []\n",
    "\n",
    "for folder in subfolders:\n",
    "    file_path = os.path.join(base_folder, folder, file_names[folder])\n",
    "\n",
    "    # Check if file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Prune columns - keep only 'text' and 'label'\n",
    "        columns_to_keep = [col for col in ['text', 'label'] if col in df.columns]\n",
    "        df_pruned = df[columns_to_keep]\n",
    "\n",
    "        # Add source column to track which split it came from\n",
    "        df_pruned['source'] = folder\n",
    "\n",
    "        dataframes.append(df_pruned)\n",
    "        print(f\"Processed {file_path}: {len(df_pruned)} rows\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found - {file_path}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display info about the combined dataset\n",
    "    print(f\"\\nCombined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"\\nRows from each source:\")\n",
    "    print(combined_df['source'].value_counts())\n",
    "\n",
    "    # Save the combined dataframe to CSV\n",
    "    combined_df.to_csv('combined_text_data.csv', index=False)\n",
    "    print(f\"\\nCombined CSV saved as 'combined_text_data.csv'\")\n",
    "\n",
    "    # Display first few rows\n",
    "    print(f\"\\nFirst few rows of combined data:\")\n",
    "    print(combined_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"No data was processed. Please check your file paths.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c63a4d-86ec-4358-bc42-7d55efd35049",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "INTERPRETATION OF DATA CONSOLIDATION PIPELINE — DATASET AGGREGATION AND STRUCTURAL ANALYSIS\n",
    "\n",
    "The data consolidation script performs systematic aggregation of training splits\n",
    "while maintaining provenance tracking and implementing selective column retention.\n",
    "\n",
    "Key structural interpretations:\n",
    "\n",
    "- **Multi-Split Integration**: Combines train, validation, and test partitions\n",
    "  into a unified dataset, enabling holistic analysis of data distribution\n",
    "  across the entire experimental pipeline. This reveals potential data leakage\n",
    "  or split imbalances that could affect model generalization.\n",
    "\n",
    "- **Selective Feature Retention**: Prunes all columns except 'text' and 'label',\n",
    "  focusing the dataset on core predictive features while eliminating metadata\n",
    "  and auxiliary variables that might complicate analysis or introduce bias.\n",
    "\n",
    "- **Provenance Tracking**: Adds a 'source' column to preserve split membership,\n",
    "  allowing researchers to trace predictions back to original data partitions\n",
    "  and analyze performance variations across training/validation/test sets.\n",
    "\n",
    "- **Data Integrity Verification**: Implements existence checks for file paths\n",
    "  and provides detailed logging of processing outcomes, ensuring transparent\n",
    "  handling of missing or corrupted data sources.\n",
    "\n",
    "- **Unified Analysis Foundation**: Creates a consolidated dataset suitable for:\n",
    "  - Exploratory data analysis across all splits\n",
    "  - Label distribution comparison between partitions\n",
    "  - Text quality assessment and preprocessing validation\n",
    "  - Cross-split model error analysis\n",
    "\n",
    "Overall interpretation:\n",
    "This consolidation represents a crucial preprocessing step that transforms\n",
    "disjointed experimental splits into an analytically coherent dataset. The\n",
    "deliberate column pruning and source tracking balance analytical utility\n",
    "with data integrity, creating a foundation for robust model evaluation\n",
    "and dataset quality assessment across the entire machine learning pipeline.\n",
    "The approach demonstrates thoughtful data management practices that support\n",
    "reproducible research and comprehensive model diagnostics.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce84f3-4f5d-4f52-a311-401f7763f1fe",
   "metadata": {
    "id": "c0ce84f3-4f5d-4f52-a311-401f7763f1fe",
    "outputId": "a7dd0dd6-074c-40d9-ba65-4515f12e4711"
   },
   "outputs": [],
   "source": [
    "import os, re, gc, string, warnings, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # lighter backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score, balanced_accuracy_score,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "from sklearn.calibration import calibration_curve # Moved from sklearn.metrics\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "\n",
    "# Import optional\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    VADER_AVAILABLE = True\n",
    "except Exception:\n",
    "    VADER_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    WORDCLOUD_AVAILABLE = True\n",
    "except Exception:\n",
    "    WORDCLOUD_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG — T4 RAM OPTIMIZED\n",
    "# ----------------------------\n",
    "BINARY_PALETTE = [\"#1a9850\", \"#d73027\"]  # [Non-Abusive, Abusive]\n",
    "sns.set_theme(style=\"whitegrid\", palette=BINARY_PALETTE)\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 10,\n",
    "    \"axes.prop_cycle\": plt.cycler(color=BINARY_PALETTE),\n",
    "})\n",
    "\n",
    "CFG = {\n",
    "    \"CSV_PATH\": \"combined_text_data.csv\",\n",
    "    \"TEXT_COL\": \"text\",\n",
    "    \"LABEL_COL\": \"label\",\n",
    "    \"OUTPUT_DIR\": \"figures_full\",\n",
    "    \"SVD_COMPONENTS\": 200,\n",
    "    \"MIN_DF\": 50,\n",
    "    \"MAX_DF\": 0.85,\n",
    "    \"SENTIMENT_SAMPLE\": 80_000,\n",
    "    \"WORDCLOUD_SAMPLE\": 150_000,\n",
    "    \"MASK_TERMS\": True,\n",
    "    \"MASK_KEEP\": 1,\n",
    "    \"ALPHA0\": 10.0,\n",
    "    \"RANDOM_STATE\": 42\n",
    "}\n",
    "\n",
    "os.makedirs(CFG[\"OUTPUT_DIR\"], exist_ok=True)\n",
    "np.random.seed(CFG[\"RANDOM_STATE\"])\n",
    "\n",
    "# ----------------------------\n",
    "# UTILS\n",
    "# ----------------------------\n",
    "def normalize_label_series(s):\n",
    "    vals = s.astype(\"string\").str.strip().str.lower()\n",
    "    mapping = {\n",
    "        \"abusive\":1,\"toxic\":1,\"hate\":1,\"hateful\":1,\"offensive\":1,\"insult\":1,\n",
    "        \"non-abusive\":0,\"non_abusive\":0,\"non abusive\":0,\"neutral\":0,\"benign\":0,\"clean\":0\n",
    "    }\n",
    "    if pd.api.types.is_numeric_dtype(s) and set(pd.unique(pd.to_numeric(s, errors=\"coerce\").dropna())).issubset({0,1}):\n",
    "        return s.astype(\"int8\")\n",
    "    mapped = vals.map(mapping)\n",
    "    if mapped.isna().any():\n",
    "        cats = sorted(vals.dropna().unique())\n",
    "        if len(cats)==2: mapped = vals.map({cats[0]:0, cats[1]:1})\n",
    "    return mapped.fillna(0).astype(\"int8\")\n",
    "\n",
    "def mask_token(tok, keep=1):\n",
    "    if len(tok) <= 2*keep: return \"*\" * len(tok)\n",
    "    return tok[:keep] + (\"*\" * (len(tok) - 2*keep)) + tok[-keep:]\n",
    "\n",
    "def maybe_mask_terms(terms, mask=True, keep=1):\n",
    "    if not mask: return terms\n",
    "    out = []\n",
    "    for t in terms:\n",
    "        pieces = t.split() if \" \" in t else [t]\n",
    "        masked = []\n",
    "        for p in pieces:\n",
    "            if len(p) > 3 and any(c.isalpha() for c in p):\n",
    "                masked.append(mask_token(p, keep))\n",
    "            else:\n",
    "                masked.append(p)\n",
    "        out.append(\" \".join(masked))\n",
    "    return out\n",
    "\n",
    "EMOJI_RE = re.compile(\"[\" \"\\U0001F1E6-\\U0001F1FF\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F600-\\U0001F64F\"\n",
    "                      \"\\U0001F680-\\U0001F6FF\" \"\\U0001F700-\\U0001F77F\" \"\\U0001F780-\\U0001F7FF\"\n",
    "                      \"\\U0001F800-\\U0001F8FF\" \"\\U0001F900-\\U0001F9FF\" \"\\U0001FA00-\\U0001FAFF\"\n",
    "                      \"\\u2600-\\u26FF\" \"\\u2700-\\u27BF\" \"]+\")\n",
    "\n",
    "STOPWORDS = list(set(ENGLISH_STOP_WORDS) - {\"no\", \"nor\", \"not\", \"never\"})\n",
    "TOK_RE = re.compile(r\"#\\w+|@\\w+|[A-Za-z]+(?:'[A-Za-z]+)?|[A-Za-z]+\")\n",
    "\n",
    "def custom_tokenizer(text): return TOK_RE.findall(text)\n",
    "\n",
    "def clean_for_vectorizer(txt):\n",
    "    x = str(txt)\n",
    "    x = re.sub(r\"http\\S+\", \" URL \", x)\n",
    "    x = re.sub(r\"@\\w+\", \" @user \", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip().lower()\n",
    "    return x\n",
    "\n",
    "def stratified_sample(df, label_col=\"label\", per_class=50_000, seed=42):\n",
    "    parts = []\n",
    "    for v, g in df.groupby(label_col):\n",
    "        n = min(per_class, len(g))\n",
    "        parts.append(g.sample(n, random_state=seed))\n",
    "    return pd.concat(parts).sample(frac=1, random_state=seed).reset_index(drop=True) if parts else df\n",
    "\n",
    "def timer(msg):\n",
    "    print(f\"\\n--- {msg} ---\")\n",
    "    gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD DATA — ALL ROWS\n",
    "# ----------------------------\n",
    "print(\"Loading FULL dataset...\")\n",
    "df = pd.read_csv(CFG[\"CSV_PATH\"], usecols=[CFG[\"TEXT_COL\"], CFG[\"LABEL_COL\"]])\n",
    "df[CFG[\"TEXT_COL\"]] = df[CFG[\"TEXT_COL\"]].astype(\"string\").fillna(\"\").str.strip()\n",
    "df[\"label\"] = normalize_label_series(df[CFG[\"LABEL_COL\"]])\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "\n",
    "# ----------------------------\n",
    "# BASIC METRICS\n",
    "# ----------------------------\n",
    "df[\"char_count\"] = df[CFG[\"TEXT_COL\"]].str.len().astype(np.int32)\n",
    "df[\"word_count\"] = df[CFG[\"TEXT_COL\"]].str.count(r\"\\b\\w+\\b\").astype(np.int32)\n",
    "\n",
    "label_counts = df[\"label\"].value_counts().sort_index()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sample_plot = stratified_sample(df, \"label\", per_class=min(100_000, df[\"label\"].value_counts().min()))\n",
    "sns.countplot(data=sample_plot, x=\"label\", ax=axes[0])\n",
    "axes[0].set_xticklabels([\"Non-Abusive\", \"Abusive\"])\n",
    "axes[1].pie(label_counts.values, labels=[\"Non-Abusive\", \"Abusive\"], colors=BINARY_PALETTE, autopct='%1.1f%%')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CFG['OUTPUT_DIR']}/01_label_distribution.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "sample_plot = stratified_sample(df[[\"char_count\",\"word_count\",\"label\"]], \"label\", per_class=50_000)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for col, ax in zip([\"char_count\", \"word_count\"], axes):\n",
    "    sns.histplot(data=sample_plot, x=col, hue=\"label\", kde=True, ax=ax, alpha=0.6)\n",
    "    ax.set_title(col.replace(\"_\",\" \").title())\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CFG['OUTPUT_DIR']}/02_text_length.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# AGGRESSION FEATURES\n",
    "# ----------------------------\n",
    "timer(\"Aggression cues\")\n",
    "t = df[CFG[\"TEXT_COL\"]]\n",
    "df[\"exclamation_count\"] = t.str.count(\"!\").astype(np.int16)\n",
    "df[\"upper_letter_count\"] = t.str.count(r\"[A-Z]\").astype(np.int32)\n",
    "df[\"total_letter_count\"] = t.str.count(r\"[A-Za-z]\").astype(np.int32)\n",
    "df[\"upper_letter_ratio\"] = (df[\"upper_letter_count\"] / df[\"total_letter_count\"].clip(lower=1)).fillna(0).astype(np.float32)\n",
    "df[\"elongation_count\"] = t.str.count(r\"(.)\\1{2,}\").astype(np.int16)\n",
    "df[\"all_caps_token_ratio\"] = t.str.count(r\"\\b[A-Z]{2,}\\b\").div(t.str.count(r\"\\b\\w+\\b\").clip(lower=1)).fillna(0.0).astype(np.float32)\n",
    "df[\"punct_ratio\"] = t.str.count(rf\"[{re.escape(string.punctuation)}]\").div(t.str.len().clip(lower=1)).fillna(0.0).astype(np.float32)\n",
    "\n",
    "if len(df) <= 1_500_000:\n",
    "    df[\"emoji_count\"] = t.apply(lambda x: len(EMOJI_RE.findall(str(x)))).astype(np.int16)\n",
    "else:\n",
    "    emoji_sample = df.sample(500_000, random_state=CFG[\"RANDOM_STATE\"])\n",
    "    emoji_means = emoji_sample.groupby(\"label\")[CFG[\"TEXT_COL\"]].apply(\n",
    "        lambda s: np.mean([len(EMOJI_RE.findall(str(x))) for x in s])\n",
    "    ).to_dict()\n",
    "    df[\"emoji_count\"] = df[\"label\"].map(emoji_means).fillna(0).astype(np.float32)\n",
    "\n",
    "plot_df = stratified_sample(\n",
    "    df[[\"upper_letter_ratio\",\"exclamation_count\",\"elongation_count\",\"emoji_count\",\n",
    "        \"all_caps_token_ratio\",\"punct_ratio\",\"label\"]],\n",
    "    \"label\", per_class=50_000\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "cols = [\"upper_letter_ratio\",\"exclamation_count\",\"elongation_count\",\"emoji_count\",\"all_caps_token_ratio\",\"punct_ratio\"]\n",
    "for i, col in enumerate(cols):\n",
    "    ax = axes[i//3, i%3]\n",
    "    sns.boxplot(data=plot_df, x=\"label\", y=col, ax=ax)\n",
    "    ax.set_title(col.replace(\"_\",\" \").title())\n",
    "    ax.set_xticklabels([\"Non-Abusive\",\"Abusive\"])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CFG['OUTPUT_DIR']}/03_aggression_cues.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# N-GRAMS — FULL DATA\n",
    "# ----------------------------\n",
    "timer(\"N-gram modeling (FULL data, RAM-optimized)\")\n",
    "ngram_df = df.copy()\n",
    "ngram_df[\"clean\"] = ngram_df[CFG[\"TEXT_COL\"]].apply(clean_for_vectorizer)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer, preprocessor=lambda x: x, lowercase=True,\n",
    "    stop_words=STOPWORDS, ngram_range=(1,2), min_df=CFG[\"MIN_DF\"], max_df=CFG[\"MAX_DF\"], strip_accents=\"unicode\"\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(ngram_df[\"clean\"])\n",
    "feats = np.array(tfidf.get_feature_names_out())\n",
    "y = ngram_df[\"label\"].values\n",
    "\n",
    "m1 = np.asarray(X_tfidf[y==1].mean(axis=0)).ravel()\n",
    "m0 = np.asarray(X_tfidf[y==0].mean(axis=0)).ravel()\n",
    "\n",
    "cv = CountVectorizer(vocabulary=feats, tokenizer=custom_tokenizer, preprocessor=lambda x: x)\n",
    "Xc = cv.transform(ngram_df[\"clean\"])\n",
    "c1 = np.asarray(Xc[y==1].sum(axis=0)).ravel()\n",
    "c0 = np.asarray(Xc[y==0].sum(axis=0)).ravel()\n",
    "\n",
    "n1, n0 = c1.sum(), c0.sum()\n",
    "bg = c1 + c0\n",
    "alpha = CFG[\"ALPHA0\"] * (bg / max(1, bg.sum()))\n",
    "\n",
    "def log_odds_z(ca, na, cb, nb, a):\n",
    "    la = np.log((ca + a) / (na + a.sum() - (ca + a) + 1e-12))\n",
    "    lb = np.log((cb + a) / (nb + a.sum() - (cb + a) + 1e-12))\n",
    "    delta = la - lb\n",
    "    var = 1.0/(ca + a + 1e-12) + 1.0/(cb + a + 1e-12)\n",
    "    return delta / np.sqrt(var)\n",
    "\n",
    "z = log_odds_z(c1, n1, c0, n0, alpha)\n",
    "\n",
    "df1 = (Xc[y==1] > 0).sum(axis=0).A1\n",
    "df0 = (Xc[y==0] > 0).sum(axis=0).A1\n",
    "pur_ab = df1 / np.maximum(1, df1 + df0)\n",
    "\n",
    "def top_terms(direction=\"abusive\", k=20, purity_thresh=0.65):\n",
    "    if direction == \"abusive\":\n",
    "        mask = (pur_ab >= purity_thresh) & (m1 > m0)\n",
    "        idx = np.argsort(z[mask])[-k:][::-1]\n",
    "        terms = feats[mask][idx]\n",
    "    else:\n",
    "        mask = (1 - pur_ab >= purity_thresh) & (m0 > m1)\n",
    "        idx = np.argsort(-z[mask])[-k:][::-1]\n",
    "        terms = feats[mask][idx]\n",
    "    return terms, idx\n",
    "\n",
    "top_ab_terms, _ = top_terms(\"abusive\", k=20)\n",
    "top_na_terms, _ = top_terms(\"non\", k=20)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 9))\n",
    "for ax, terms, title, color in zip(\n",
    "    axes,\n",
    "    [top_ab_terms, top_na_terms],\n",
    "    [\"Abusive discriminators\", \"Non-Abusive discriminators\"],\n",
    "    [BINARY_PALETTE[1], BINARY_PALETTE[0]]\n",
    "):\n",
    "    z_vals = [z[np.where(feats == t)[0][0]] for t in terms]\n",
    "    if \"Non-Abusive\" in title:\n",
    "        z_vals = [-v for v in z_vals]\n",
    "    ax.barh(range(len(terms)), z_vals, color=color)\n",
    "    ax.set_yticks(range(len(terms)))\n",
    "    ax.set_yticklabels(maybe_mask_terms(terms, CFG[\"MASK_TERMS\"], CFG[\"MASK_KEEP\"]))\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(title + \" (log-odds z)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CFG['OUTPUT_DIR']}/05_discriminative_terms_logodds.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Bigram novelty\n",
    "cv2 = CountVectorizer(ngram_range=(2,2), tokenizer=custom_tokenizer, preprocessor=lambda x: x,\n",
    "                      stop_words=STOPWORDS, min_df=30, max_df=0.9, strip_accents=\"unicode\", binary=True)\n",
    "X2 = cv2.fit_transform(ngram_df[\"clean\"])\n",
    "v2 = np.array(cv2.get_feature_names_out())\n",
    "seen_ab = np.asarray(X2[y==1].sum(axis=0)).ravel() > 0\n",
    "seen_na = np.asarray(X2[y==0].sum(axis=0)).ravel() > 0\n",
    "novel_ab_rate = (seen_ab & ~seen_na).sum() / max(1, seen_ab.sum())\n",
    "print(f\"Bigram novelty rate: {novel_ab_rate:.2%}\")\n",
    "\n",
    "# ----------------------------\n",
    "# SENTIMENT (sampled)\n",
    "# ----------------------------\n",
    "if VADER_AVAILABLE:\n",
    "    timer(\"VADER sentiment\")\n",
    "    sent_sample = stratified_sample(df[[CFG[\"TEXT_COL\"], \"label\"]], \"label\", per_class=CFG[\"SENTIMENT_SAMPLE\"]//2)\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sent_sample[\"vader_compound\"] = sent_sample[CFG[\"TEXT_COL\"]].apply(lambda x: analyzer.polarity_scores(str(x))[\"compound\"])\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(data=sent_sample, x=\"label\", y=\"vader_compound\")\n",
    "    plt.xticks([0,1], [\"Non-Abusive\",\"Abusive\"])\n",
    "    plt.title(\"VADER Compound Sentiment\")\n",
    "    plt.savefig(f\"{CFG['OUTPUT_DIR']}/04_sentiment.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# CORRELATION\n",
    "# ----------------------------\n",
    "corr_cols = [\"char_count\",\"word_count\",\"upper_letter_ratio\",\"exclamation_count\",\"elongation_count\",\"all_caps_token_ratio\",\"punct_ratio\"]\n",
    "if \"emoji_count\" in df.columns:\n",
    "    corr_cols.append(\"emoji_count\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[corr_cols].corr(), annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Matrix (FULL)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CFG['OUTPUT_DIR']}/07_correlation.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# WORDCLOUD\n",
    "# ----------------------------\n",
    "if WORDCLOUD_AVAILABLE:\n",
    "    try:\n",
    "        wc_df = ngram_df.sample(min(CFG[\"WORDCLOUD_SAMPLE\"], len(ngram_df)), random_state=CFG[\"RANDOM_STATE\"])\n",
    "        cv_wc = CountVectorizer(tokenizer=custom_tokenizer, preprocessor=lambda x: x,\n",
    "                                stop_words=STOPWORDS, max_features=2000, min_df=10, strip_accents=\"unicode\")\n",
    "        Xc_wc = cv_wc.fit_transform(wc_df[\"clean\"])\n",
    "        vocab_wc = cv_wc.get_feature_names_out()\n",
    "        freqs = np.asarray(Xc_wc.sum(axis=0)).ravel()\n",
    "        word_freq = dict(zip(vocab_wc, freqs))\n",
    "        wc = WordCloud(width=800, height=400, background_color=\"white\", max_words=200, colormap=\"RdYlGn_r\")\n",
    "        wc.generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(12, 6)); plt.imshow(wc, interpolation=\"bilinear\"); plt.axis(\"off\"); plt.title(\"Word Cloud (Sampled)\")\n",
    "        plt.savefig(f\"{CFG['OUTPUT_DIR']}/08_wordcloud.png\", bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"WordCloud skipped: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# MODELING — FULL DATA\n",
    "# ----------------------------\n",
    "timer(\"Modeling (FULL data)\")\n",
    "model_df = df.copy()\n",
    "model_df[\"clean\"] = model_df[CFG[\"TEXT_COL\"]].apply(clean_for_vectorizer)\n",
    "\n",
    "X_text = tfidf.transform(model_df[\"clean\"])\n",
    "del ngram_df, Xc, cv, tfidf\n",
    "gc.collect()\n",
    "\n",
    "svd = TruncatedSVD(n_components=CFG[\"SVD_COMPONENTS\"], random_state=CFG[\"RANDOM_STATE\"])\n",
    "X_text_svd = svd.fit_transform(X_text)\n",
    "scaler = StandardScaler()\n",
    "X_text_svd = scaler.fit_transform(X_text_svd)\n",
    "del X_text, svd, scaler\n",
    "gc.collect()\n",
    "\n",
    "num_cols = [\"upper_letter_ratio\", \"exclamation_count\", \"elongation_count\", \"all_caps_token_ratio\", \"punct_ratio\"]\n",
    "if \"emoji_count\" in df.columns:\n",
    "    num_cols.append(\"emoji_count\")\n",
    "X_num = model_df[num_cols].fillna(0).to_numpy().astype(np.float32)\n",
    "\n",
    "X_all = np.hstack([X_text_svd, X_num])\n",
    "y_all = model_df[\"label\"].to_numpy()\n",
    "del X_text_svd, X_num, model_df\n",
    "gc.collect()\n",
    "\n",
    "groups = df[CFG[\"TEXT_COL\"]].str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=CFG[\"RANDOM_STATE\"])\n",
    "train_idx, test_idx = next(gss.split(X_all, y_all, groups=groups))\n",
    "X_tr, X_te = X_all[train_idx], X_all[test_idx]\n",
    "y_tr, y_te = y_all[train_idx], y_all[test_idx]\n",
    "del X_all, y_all, groups\n",
    "gc.collect()\n",
    "\n",
    "def report_model(name, y_true, y_prob, y_pred):\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    pr = average_precision_score(y_true, y_prob)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    ba = balanced_accuracy_score(y_true, y_pred)\n",
    "    print(f\"{name}: ROC-AUC={roc:.3f} | PR-AUC={pr:.3f} | F1={f1:.3f} | BalAcc={ba:.3f}\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"liblinear\", random_state=CFG[\"RANDOM_STATE\"])\n",
    "lr.fit(X_tr, y_tr)\n",
    "lr_prob = lr.predict_proba(X_te)[:,1]\n",
    "lr_pred = (lr_prob >= 0.5).astype(int)\n",
    "report_model(\"LogReg\", y_te, lr_prob, lr_pred)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_leaf=3, n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\", random_state=CFG[\"RANDOM_STATE\"]\n",
    ")\n",
    "rf.fit(X_tr, y_tr)\n",
    "rf_prob = rf.predict_proba(X_te)[:,1]\n",
    "rf_pred = (rf_prob >= 0.5).astype(int)\n",
    "report_model(\"RandomForest\", y_te, rf_prob, rf_pred)\n",
    "\n",
    "# XGBoost\n",
    "if XGB_AVAILABLE:\n",
    "    pos, neg = (y_tr == 1).sum(), (y_tr == 0).sum()\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=500, max_depth=7, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        tree_method=\"hist\", objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "        scale_pos_weight=neg / pos, random_state=CFG[\"RANDOM_STATE\"], n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr, eval_set=[(X_te, y_te)], verbose=0)\n",
    "    xgb_prob = xgb.predict_proba(X_te)[:,1]\n",
    "    xgb_pred = (xgb_prob >= 0.5).astype(int)\n",
    "    report_model(\"XGBoost\", y_te, xgb_prob, xgb_pred)\n",
    "\n",
    "# ----------------------------\n",
    "# EXTRA MODEL PLOTS\n",
    "# ----------------------------\n",
    "def plot_roc_pr(y_true, y_proba, model_name, save_path):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    axes[0].plot(fpr, tpr, color=BINARY_PALETTE[1], lw=2, label=f'ROC-AUC = {roc_auc_score(y_true, y_proba):.3f}')\n",
    "    axes[0].plot([0,1],[0,1], '--', color='gray')\n",
    "    axes[0].set(xlabel='FPR', ylabel='TPR', title=f'{model_name} - ROC')\n",
    "    axes[0].legend()\n",
    "\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
    "    axes[1].plot(rec, prec, color=BINARY_PALETTE[0], lw=2, label=f'PR-AUC = {average_precision_score(y_true, y_proba):.3f}')\n",
    "    axes[1].set(xlabel='Recall', ylabel='Precision', title=f'{model_name} - PR')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_calibration(y_true, y_proba, model_name, save_path):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(prob_pred, prob_true, 'o-', color=BINARY_PALETTE[1])\n",
    "    plt.plot([0,1],[0,1], '--', color='gray')\n",
    "    plt.xlabel(\"Mean Predicted Prob\"); plt.ylabel(\"Fraction Positive\")\n",
    "    plt.title(f\"{model_name} - Calibration\")\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, save_path):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"RdYlGn_r\",\n",
    "                xticklabels=[\"Non-Abusive\", \"Abusive\"],\n",
    "                yticklabels=[\"Non-Abusive\", \"Abusive\"])\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.ylabel(\"True\"); plt.xlabel(\"Predicted\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name, save_path, top_k=20):\n",
    "    if hasattr(model, 'coef_'):\n",
    "        imp = np.abs(model.coef_[0])\n",
    "        top_idx = np.argsort(imp)[-top_k:][::-1]\n",
    "        color = BINARY_PALETTE[1]\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        imp = model.feature_importances_\n",
    "        top_idx = np.argsort(imp)[-top_k:][::-1]\n",
    "        color = BINARY_PALETTE[0]\n",
    "    else:\n",
    "        return\n",
    "    names = [feature_names[i] for i in top_idx]\n",
    "    masked = [maybe_mask_terms([n], CFG[\"MASK_TERMS\"], CFG[\"MASK_KEEP\"])[0] if any(c.isalpha() for c in n) else n for n in names]\n",
    "    plt.figure(figsize=(8, top_k*0.35))\n",
    "    plt.barh(range(len(imp[top_idx])), imp[top_idx], color=color)\n",
    "    plt.yticks(range(len(masked)), masked)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Importance\" if \"Forest\" in model_name or \"XGBoost\" in model_name else \"|Coefficient|\")\n",
    "    plt.title(f\"{model_name} - Top {top_k} Features\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Feature names for importance\n",
    "svd_names = [f\"SVD_{i}\" for i in range(CFG[\"SVD_COMPONENTS\"])]\n",
    "full_feature_names = svd_names + num_cols\n",
    "\n",
    "# LogReg\n",
    "plot_roc_pr(y_te, lr_prob, \"Logistic Regression\", f\"{CFG['OUTPUT_DIR']}/10_lr_roc_pr.png\")\n",
    "plot_calibration(y_te, lr_prob, \"Logistic Regression\", f\"{CFG['OUTPUT_DIR']}/11_lr_calibration.png\")\n",
    "plot_confusion_matrix(y_te, lr_pred, \"Logistic Regression\", f\"{CFG['OUTPUT_DIR']}/12_lr_confusion.png\")\n",
    "plot_feature_importance(lr, full_feature_names, \"Logistic Regression\", f\"{CFG['OUTPUT_DIR']}/13_lr_feature_importance.png\")\n",
    "\n",
    "# RandomForest\n",
    "plot_roc_pr(y_te, rf_prob, \"Random Forest\", f\"{CFG['OUTPUT_DIR']}/14_rf_roc_pr.png\")\n",
    "plot_calibration(y_te, rf_prob, \"Random Forest\", f\"{CFG['OUTPUT_DIR']}/15_rf_calibration.png\")\n",
    "plot_confusion_matrix(y_te, rf_pred, \"Random Forest\", f\"{CFG['OUTPUT_DIR']}/16_rf_confusion.png\")\n",
    "plot_feature_importance(rf, full_feature_names, \"Random Forest\", f\"{CFG['OUTPUT_DIR']}/17_rf_feature_importance.png\")\n",
    "\n",
    "# XGBoost\n",
    "if XGB_AVAILABLE:\n",
    "    plot_roc_pr(y_te, xgb_prob, \"XGBoost\", f\"{CFG['OUTPUT_DIR']}/18_xgb_roc_pr.png\")\n",
    "    plot_calibration(y_te, xgb_prob, \"XGBoost\", f\"{CFG['OUTPUT_DIR']}/19_xgb_calibration.png\")\n",
    "    plot_confusion_matrix(y_te, xgb_pred, \"XGBoost\", f\"{CFG['OUTPUT_DIR']}/20_xgb_confusion.png\")\n",
    "    plot_feature_importance(xgb, full_feature_names, \"XGBoost\", f\"{CFG['OUTPUT_DIR']}/21_xgb_feature_importance.png\")\n",
    "\n",
    "print(\"All model plots saved!\")\n",
    "\n",
    "# ----------------------------\n",
    "# FINAL SUMMARY\n",
    "# ----------------------------\n",
    "summary = {\n",
    "    \"total_samples\": len(df),\n",
    "    \"abusive_ratio\": float((df[\"label\"] == 1).mean()),\n",
    "    \"bigram_novelty_rate\": float(novel_ab_rate),\n",
    "    \"mean_upper_ratio_abusive\": float(df[df[\"label\"]==1][\"upper_letter_ratio\"].mean()),\n",
    "    \"mean_upper_ratio_non\": float(df[df[\"label\"]==0][\"upper_letter_ratio\"].mean()),\n",
    "    \"top_abusive_terms_masked\": maybe_mask_terms(list(top_ab_terms[:10]), CFG[\"MASK_TERMS\"], CFG[\"MASK_KEEP\"]),\n",
    "    \"top_non_abusive_terms_masked\": maybe_mask_terms(list(top_na_terms[:10]), CFG[\"MASK_TERMS\"], CFG[\"MASK_KEEP\"])\n",
    "}\n",
    "with open(f\"{CFG['OUTPUT_DIR']}/eda_summary_full.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL ANALYSIS COMPLETE — All rows used, all plots saved\")\n",
    "print(f\"Output: {CFG['OUTPUT_DIR']}/\")\n",
    "print(\"Non-Abusive = #1a9850 | Abusive = #d73027\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a4062-4429-4ff9-a042-bde6155a611c",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "INTERPRETATION OF COMPREHENSIVE EDA PIPELINE — FULL-SCALE TEXT ANALYSIS FRAMEWORK\n",
    "\n",
    "The exploratory data analysis pipeline implements a systematic, production-grade\n",
    "framework for understanding abusive content patterns through multi-modal\n",
    "feature engineering and statistical modeling.\n",
    "\n",
    "Key analytical interpretations:\n",
    "\n",
    "- **Multi-Dimensional Feature Engineering**: Extracts linguistic, structural,\n",
    "  and behavioral cues including capitalization patterns, punctuation intensity,\n",
    "  character elongation, and emoji usage. These features capture both explicit\n",
    "  and subtle aggression markers that distinguish abusive from non-abusive content.\n",
    "\n",
    "- **Statistical Discriminative Analysis**: Employs log-odds ratio with z-scoring\n",
    "  to identify terms with the strongest class separation power, revealing both\n",
    "  overt hate speech vocabulary and more nuanced offensive language patterns.\n",
    "\n",
    "- **RAM-Optimized Scalability**: Implements strategic sampling, sparse matrix\n",
    "  operations, and memory-efficient data types to handle massive text corpora\n",
    "  while maintaining analytical rigor across the entire dataset.\n",
    "\n",
    "- **Multi-Model Benchmarking**: Compares Logistic Regression, Random Forest,\n",
    "  and XGBoost to evaluate different learning paradigms' effectiveness for\n",
    "  abusive content detection, providing insights into feature interactions\n",
    "  and classification boundaries.\n",
    "\n",
    "- **Comprehensive Model Diagnostics**: Extends beyond basic accuracy metrics\n",
    "  to include ROC/PR curves, calibration analysis, confusion matrices, and\n",
    "  feature importance, offering a complete picture of model behavior and\n",
    "  deployment readiness.\n",
    "\n",
    "- **Privacy-Preserving Visualization**: Implements term masking for sensitive\n",
    "  content while maintaining analytical transparency, balancing research\n",
    "  utility with ethical considerations.\n",
    "\n",
    "Overall interpretation:\n",
    "This EDA framework represents a sophisticated approach to understanding\n",
    "abusive content that moves beyond simple keyword counting to capture\n",
    "the complex linguistic and behavioral patterns characteristic of online\n",
    "harm. The systematic feature engineering and multi-model evaluation\n",
    "provide both immediate insights into dataset characteristics and\n",
    "foundational understanding for developing robust detection systems.\n",
    "The implementation demonstrates careful consideration of computational\n",
    "constraints while maintaining analytical depth, making it suitable for\n",
    "both research and production applications in content moderation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906065b8-e362-4fb1-9e47-f649cac32f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f157e1f-7ef8-4db1-af89-dc88bd9c008e",
   "metadata": {
    "id": "5f157e1f-7ef8-4db1-af89-dc88bd9c008e",
    "outputId": "79a22453-daa0-47ff-b98f-ebd87d081078"
   },
   "outputs": [],
   "source": [
    "# Memotion Dataset 7k — Local/Jupyter-Compatible EDA\n",
    "RUN_TEXT_TSNE = False\n",
    "RUN_IMAGE_EMBEDDINGS = False\n",
    "SAMPLE_IMAGE_EMB_COUNT = 500\n",
    "TFIDF_MAX_FEATURES = 5000\n",
    "\n",
    "# Install dependencies (safe in Jupyter)\n",
    "try:\n",
    "    import wordcloud\n",
    "except ImportError:\n",
    "    !pip install -q wordcloud\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define local paths (you can update DATA_DIR to your actual folder)\n",
    "DATA_DIR = Path(\"datasets/memotion_dataset_7k\")  # CHANGE THIS IF NEEDED\n",
    "assert DATA_DIR.exists(), f\"Dataset folder '{DATA_DIR}' not found. Please set DATA_DIR correctly.\"\n",
    "\n",
    "# Use consistent output directory\n",
    "OUT_DIR = Path(\"figure_multimodal\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Outputs will be saved to:\", OUT_DIR.resolve())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ------------------------------------------------------------------\n",
    "csvs = {p.name: p for p in DATA_DIR.glob('*.csv')}\n",
    "print(\"Found CSVs:\", list(csvs.keys()))\n",
    "\n",
    "# Merge reference.csv and labels.csv if both exist\n",
    "if 'reference.csv' in csvs and 'labels.csv' in csvs:\n",
    "    ref = pd.read_csv(csvs['reference.csv'], encoding='utf-8', low_memory=False)\n",
    "    lab = pd.read_csv(csvs['labels.csv'], encoding='utf-8', low_memory=False)\n",
    "    print(\"reference.csv shape:\", ref.shape)\n",
    "    print(\"labels.csv shape:\", lab.shape)\n",
    "\n",
    "    common_cols = set(ref.columns).intersection(set(lab.columns))\n",
    "    print(\"Common columns:\", common_cols)\n",
    "\n",
    "    if 'image_name' in ref.columns and 'image_name' in lab.columns:\n",
    "        df_all = ref.merge(lab, on='image_name', how='outer', suffixes=('_ref', '_lab'))\n",
    "    else:\n",
    "        # Try other common keys\n",
    "        possible_keys = ['image_url', 'imageid', 'image_id', 'img', 'img_name']\n",
    "        key = None\n",
    "        for k in possible_keys:\n",
    "            if k in ref.columns and k in lab.columns:\n",
    "                key = k\n",
    "                break\n",
    "        if key:\n",
    "            df_all = ref.merge(lab, on=key, how='outer', suffixes=('_ref', '_lab'))\n",
    "        else:\n",
    "            print(\"No common key found. Concatenating (may cause duplication).\")\n",
    "            df_all = pd.concat([ref, lab], ignore_index=True)\n",
    "else:\n",
    "    # Load all CSVs and concatenate\n",
    "    dfs = []\n",
    "    for name, p in csvs.items():\n",
    "        try:\n",
    "            dfs.append(pd.read_csv(p, encoding='utf-8', low_memory=False))\n",
    "        except Exception:\n",
    "            dfs.append(pd.read_csv(p, encoding='latin1', low_memory=False))\n",
    "    df_all = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]\n",
    "\n",
    "print(\"Combined dataframe shape:\", df_all.shape)\n",
    "display(df_all.head(3))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLUMN MAPPING & BASIC STATS\n",
    "# ------------------------------------------------------------------\n",
    "def find_col_by_keywords(keywords, available_cols):\n",
    "    lower_map = {c.lower(): c for c in available_cols}\n",
    "    for k in keywords:\n",
    "        if k in lower_map:\n",
    "            return lower_map[k]\n",
    "    for c in available_cols:\n",
    "        for k in keywords:\n",
    "            if k in c.lower():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "cols = df_all.columns.tolist()\n",
    "col_map = {\n",
    "    'image_name': find_col_by_keywords(['image_name','imagename','image','imageurl','image_url'], cols),\n",
    "    'text_ocr': find_col_by_keywords(['text_ocr','text','text_corrected','ocr','caption'], cols),\n",
    "    'text_corrected': find_col_by_keywords(['text_corrected','textclean','clean_text'], cols),\n",
    "    'sentiment': find_col_by_keywords(['overall_sentiment','sentiment','label','labels'], cols),\n",
    "    'humour': find_col_by_keywords(['humour','humor'], cols),\n",
    "    'sarcasm': find_col_by_keywords(['sarcasm'], cols),\n",
    "    'offensive': find_col_by_keywords(['offensive'], cols),\n",
    "    'motivational': find_col_by_keywords(['motivational','motiv'], cols),\n",
    "}\n",
    "\n",
    "print(\"\\nGuessed column mapping:\")\n",
    "for k, v in col_map.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# LABEL DISTRIBUTIONS\n",
    "# ------------------------------------------------------------------\n",
    "if col_map['sentiment'] and col_map['sentiment'] in df_all.columns:\n",
    "    s = col_map['sentiment']\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    display(df_all[s].value_counts(dropna=False))\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    order = df_all[s].value_counts().index\n",
    "    sns.countplot(y=df_all[s], order=order)\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"01_sentiment_dist.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "for e in ['humour', 'sarcasm', 'offensive', 'motivational']:\n",
    "    c = col_map.get(e)\n",
    "    if c and c in df_all.columns:\n",
    "        print(f\"\\n{e.capitalize()} distribution:\")\n",
    "        display(df_all[c].value_counts(dropna=False))\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        sns.countplot(y=df_all[c], order=df_all[c].value_counts().index)\n",
    "        plt.title(f\"{e.capitalize()} Distribution\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / f\"02_{e}_dist.png\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# TEXT PROCESSING\n",
    "# ------------------------------------------------------------------\n",
    "FALLBACK_STOPWORDS = {\n",
    "    'a','an','the','and','or','is','are','to','of','in','on','for','with','that','this','it','as','at','by','be',\n",
    "    'from','was','were','has','have','i','you','we','they','he','she','not','but','so','if','then'\n",
    "}\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    STOPWORDS = set(ENGLISH_STOP_WORDS).union(FALLBACK_STOPWORDS)\n",
    "except Exception:\n",
    "    STOPWORDS = FALLBACK_STOPWORDS\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    s = text.lower()\n",
    "    s = re.sub(r'http\\S+', ' ', s)\n",
    "    s = re.sub(r\"[^a-z0-9']\", ' ', s)\n",
    "    tokens = [t for t in re.split(r'\\s+', s) if t and t not in STOPWORDS and len(t) > 1]\n",
    "    return tokens\n",
    "\n",
    "text_col = col_map.get('text_ocr') or col_map.get('text_corrected')\n",
    "if text_col and text_col in df_all.columns:\n",
    "    if col_map.get('text_corrected') and col_map['text_corrected'] in df_all.columns:\n",
    "        df_all['clean_text'] = df_all[col_map['text_corrected']].fillna('').astype(str)\n",
    "    else:\n",
    "        df_all['clean_text'] = df_all[text_col].fillna('').astype(str)\n",
    "\n",
    "    df_all['clean_text'] = df_all['clean_text'].apply(lambda s: re.sub(r'\\s+', ' ', s.strip().lower()))\n",
    "\n",
    "    # Token stats\n",
    "    tokens = []\n",
    "    for t in df_all['clean_text']:\n",
    "        tokens.extend(simple_tokenize(t))\n",
    "    top_tokens = Counter(tokens).most_common(60)\n",
    "    print(\"\\nTop tokens:\", top_tokens[:20])\n",
    "\n",
    "    # Bigrams\n",
    "    bigrams = Counter()\n",
    "    for t in df_all['clean_text']:\n",
    "        toks = simple_tokenize(t)\n",
    "        for i in range(len(toks) - 1):\n",
    "            bigrams[(toks[i], toks[i+1])] += 1\n",
    "    top_bigrams = [(\" \".join(k), v) for k, v in bigrams.most_common(30)]\n",
    "    print(\"\\nTop bigrams:\", top_bigrams[:20])\n",
    "\n",
    "    # WordCloud\n",
    "    all_text = \" \".join([t for t in df_all['clean_text'] if isinstance(t, str) and t.strip()])\n",
    "    if all_text:\n",
    "        wc = WordCloud(width=900, height=400, background_color='white', collocations=False).generate(all_text)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('WordCloud — All Meme Texts')\n",
    "        plt.savefig(OUT_DIR / \"03_wordcloud.png\", bbox_inches=\"tight\", dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    # Text length features\n",
    "    df_all['text_len_chars'] = df_all['clean_text'].str.len()\n",
    "    df_all['text_len_words'] = df_all['clean_text'].apply(lambda s: len(simple_tokenize(s)))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.histplot(df_all['text_len_chars'].dropna(), bins=50)\n",
    "    plt.title('Text Length (Characters)')\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.histplot(df_all['text_len_words'].dropna(), bins=30)\n",
    "    plt.title('Text Length (Tokens)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"04_text_length.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No usable text column found.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# IMAGE ANALYSIS\n",
    "# ------------------------------------------------------------------\n",
    "# Guess image directory\n",
    "image_dir = None\n",
    "for candidate in [DATA_DIR / 'images', DATA_DIR]:\n",
    "    if candidate.is_dir():\n",
    "        n_imgs = len(list(candidate.glob('*.jpg'))) + len(list(candidate.glob('*.png')))\n",
    "        if n_imgs > 10:\n",
    "            image_dir = candidate\n",
    "            break\n",
    "\n",
    "print(\"Image directory:\", image_dir)\n",
    "\n",
    "if image_dir:\n",
    "    img_files = [p for p in image_dir.rglob('*') if p.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "    print(f\"Found {len(img_files)} image files.\")\n",
    "\n",
    "    # Sample images\n",
    "    if col_map['image_name'] and col_map['image_name'] in df_all.columns:\n",
    "        sample_fns = df_all[col_map['image_name']].dropna().sample(min(12, len(df_all))).tolist()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for i, fn in enumerate(sample_fns):\n",
    "            plt.subplot(3, 4, i + 1)\n",
    "            try:\n",
    "                p = image_dir / fn\n",
    "                if not p.exists():\n",
    "                    matches = list(image_dir.rglob(fn))\n",
    "                    p = matches[0] if matches else p\n",
    "                img = Image.open(p).convert('RGB')\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(str(fn)[:30], fontsize=8)\n",
    "            except Exception:\n",
    "                plt.text(0.5, 0.5, \"missing\", ha='center', va='center')\n",
    "                plt.axis('off')\n",
    "                plt.title(str(fn)[:30], fontsize=8)\n",
    "        plt.suptitle('Sample Memes')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"05_sample_images.png\", bbox_inches=\"tight\", dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    # Image size stats (sample 1000)\n",
    "    sizes = []\n",
    "    for p in img_files[:1000]:\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                sizes.append(im.size)\n",
    "        except:\n",
    "            continue\n",
    "    if sizes:\n",
    "        widths = [s[0] for s in sizes]\n",
    "        heights = [s[1] for s in sizes]\n",
    "        ratios = [w / h for w, h in sizes if h > 0]\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.histplot(widths, bins=30)\n",
    "        plt.title('Width')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.histplot(heights, bins=30)\n",
    "        plt.title('Height')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.histplot(ratios, bins=30)\n",
    "        plt.title('Aspect Ratio (W/H)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"06_image_stats.png\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No image directory found.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CORRELATIONS\n",
    "# ------------------------------------------------------------------\n",
    "if 'text_len_words' in df_all.columns and col_map['sentiment'] in df_all.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=df_all[col_map['sentiment']], y=df_all['text_len_words'])\n",
    "    plt.title('Text Length vs Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"07_text_len_vs_sentiment.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "if image_dir and col_map['image_name'] in df_all.columns:\n",
    "    fn_to_area = {}\n",
    "    for p in img_files[:2000]:\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                fn_to_area[p.name] = im.width * im.height\n",
    "        except:\n",
    "            continue\n",
    "    if fn_to_area:\n",
    "        df_all['img_area'] = df_all[col_map['image_name']].map(fn_to_area).fillna(0)\n",
    "        if col_map['sentiment'] in df_all.columns:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.boxplot(x=df_all[col_map['sentiment']], y=df_all['img_area'])\n",
    "            plt.yscale('log')\n",
    "            plt.title('Image Area vs Sentiment (log scale)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(OUT_DIR / \"08_img_area_vs_sentiment.png\", bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# OPTIONAL: TEXT t-SNE (disabled by default)\n",
    "# ------------------------------------------------------------------\n",
    "if RUN_TEXT_TSNE and 'clean_text' in df_all.columns:\n",
    "    print(\"Running t-SNE (may take several minutes)...\")\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    vect = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=(1,2), stop_words='english')\n",
    "    X_tfidf = vect.fit_transform(df_all['clean_text'].fillna(''))\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    X_svd = svd.fit_transform(X_tfidf)\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "    X_tsne = tsne.fit_transform(X_svd)\n",
    "    df_all['text_tsne_x'] = X_tsne[:,0]\n",
    "    df_all['text_tsne_y'] = X_tsne[:,1]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    if col_map['sentiment'] in df_all.columns:\n",
    "        sns.scatterplot(data=df_all, x='text_tsne_x', y='text_tsne_y', hue=col_map['sentiment'], alpha=0.6)\n",
    "    else:\n",
    "        plt.scatter(df_all['text_tsne_x'], df_all['text_tsne_y'], s=6)\n",
    "    plt.title('t-SNE of Meme Texts')\n",
    "    plt.savefig(OUT_DIR / \"09_text_tsne.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# OPTIONAL: IMAGE EMBEDDINGS (disabled by default)\n",
    "# ------------------------------------------------------------------\n",
    "if RUN_IMAGE_EMBEDDINGS and image_dir:\n",
    "    print(\"Image embeddings require PyTorch and GPU. Skipping unless enabled and configured.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SAVE OUTPUTS\n",
    "# ------------------------------------------------------------------\n",
    "df_all.head(2000).to_csv(OUT_DIR / 'df_sample_2000.csv', index=False)\n",
    "\n",
    "try:\n",
    "    with open(OUT_DIR / 'top_tokens.json', 'w') as f:\n",
    "        json.dump({'top_tokens': top_tokens[:200]}, f, indent=2)\n",
    "except Exception as e:\n",
    "    print(\"Could not save tokens:\", e)\n",
    "\n",
    "print(f\"\\nEDA complete. All outputs saved to: {OUT_DIR.resolve()}\")\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf69dc1-89ea-4022-a81e-a35be1016b0b",
   "metadata": {
    "id": "d6516f47-e4fa-4a92-b21b-2bc28e8dbae7"
   },
   "source": [
    "\"\"\"\n",
    "MEMOTION DATASET 7K — EXPLORATORY DATA ANALYSIS (EDA)\n",
    "-----------------------------------------------------\n",
    "\n",
    "DATA LOADING & PREPARATION:\n",
    "- Merges multiple CSV files (reference.csv + labels.csv) using image_name as key\n",
    "- Automatically maps columns using keyword matching for text, labels, and metadata\n",
    "- Handles encoding issues and provides comprehensive data shape reporting\n",
    "\n",
    "TEXT ANALYSIS:\n",
    "- Implements smart tokenization with URL removal and stopword filtering\n",
    "- Generates word frequency distributions and bigram analysis\n",
    "- Creates word clouds to visualize prominent text themes\n",
    "- Analyzes text length distributions in characters and tokens\n",
    "\n",
    "VISUAL CONTENT ANALYSIS:\n",
    "- Locates image directories and validates file accessibility\n",
    "- Displays sample memes in grid layout for qualitative inspection\n",
    "- Computes image dimensions and aspect ratio statistics\n",
    "- Correlates visual features with textual sentiment labels\n",
    "\n",
    "MULTIMODAL CORRELATIONS:\n",
    "- Examines relationships between text length and sentiment categories\n",
    "- Analyzes image area vs sentiment patterns using logarithmic scaling\n",
    "- Provides optional t-SNE visualization for text embedding clustering\n",
    "- Supports image embedding analysis (disabled by default for performance)\n",
    "\n",
    "OUTPUT & REPORTING:\n",
    "- Generates comprehensive visualization suite (9+ plot types)\n",
    "- Saves processed data samples for further analysis\n",
    "- Exports token frequency data as JSON for external use\n",
    "- Creates self-contained output directory with all results\n",
    "\n",
    "OVERALL INTERPRETATION:\n",
    "This EDA pipeline provides a complete multimodal analysis of the Memotion 7K dataset,\n",
    "revealing patterns in text characteristics, visual properties, and their relationships\n",
    "with sentiment labels. The systematic approach enables data quality validation, feature\n",
    "understanding, and informs subsequent model design decisions for meme classification tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e3095-fb47-4084-ab56-165c2bf687e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (mmtox)",
   "language": "python",
   "name": "mmtox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
